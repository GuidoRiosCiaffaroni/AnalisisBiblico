# -*- coding: utf-8 -*-
"""Control2_05.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1faqVs9Nou6E_tDy0ndAiQgVqZDEE36JN

# Control 2

#### Guido Rios Ciaffaroni
#### Eduardo Opazo Diaz

## 1. Estructura general del código

Este archivo es, en esencia, el mismo pipeline NLP + estadística descriptiva que el anterior:

1. Descarga y carga del texto bíblico (formato BCVS).
2. Limpieza inicial (text_clean, eliminación de <I>, ¶).
3. Limpieza profunda / eliminación de ruido (text_no_noise).
4. Normalización:
* minúsculas
* word_tokenize
* eliminación de stopwords (tokens_no_stop)
* text_normalized
5. Métricas básicas:
* nº de versículos
* nº total de palabras
* nº de palabras únicas / vocabulario
* nº de libros y capítulos
6. Análisis de longitudes:
* palabras por versículo (media, mediana, moda, varianza, desviación estándar)
* longitud máxima y mínima de versículos
* distribución de longitudes (histogramas, KDE, boxplots)
7. Análisis de palabras:
* top N palabras globales
* top N por libro y por capítulo
* palabras clave (dios, hombre, tierra) global y por libro
* comparación con y sin stopwords
8. Métricas por libro y capítulo:
* palabras totales por libro y capítulo
* versículo más largo / más corto por libro
* libro más extenso y más breve
9. Métricas de densidad y riqueza léxica:
* TTR global
* TTR por libro
* hapax legomena / dislegomena
* palabras raras (freq ≤ 3)
* palabras comunes (percentil 95)
* diversidad léxica (tokens vs tipos)
10. NLP avanzado:
* N-grams (bi, tri, 4-grams)
* spaCy: lematización + POS tagging
* conteo de verbos, sustantivos, adjetivos
* vocabulario con ID, frecuencia, exportado a CSV
* TF-IDF por libro (palabras características)
* TF-IDF por personaje (dios, adan, noe, abraham, jacob)

https://www.biblesupersearch.com/unbound-downloads/
"""

!wget -O spanish_sagradas_escrituras_1569.txt \
https://raw.githubusercontent.com/GuidoRiosCiaffaroni/AnalisisBiblico/refs/heads/main/Libros/spanish_sagradas_escrituras_1569_utf8_mapped_to_NRSVA.txt

"""## Descripción técnica del bloque: Setup, Importación y Preprocesamiento del Texto

Este bloque inicial implementa la configuración del entorno y el pipeline base de preprocesamiento para análisis textual. Primero se importan los módulos esenciales para manipulación estructurada de datos (pandas, numpy), visualización (matplotlib, seaborn) y procesamiento de lenguaje natural (nltk, re, Counter). Además, se inicializan los recursos fundamentales de NLTK (stopwords, punkt) asegurando que el entorno disponga de tokenizadores y listas de palabras vacías.

A continuación, se define la ruta del archivo de entrada (FILE_PATH) y se especifica la estructura de columnas del formato BCVS. El archivo de texto es cargado utilizando pd.read_csv() con separador por tabulación, encabezados personalizados, ignorando líneas de comentario y normalizando espacios iniciales.

El preprocesamiento se divide en dos etapas:

1. Limpieza superficial
* Eliminación de etiquetas de formato (<I>...</I>).
* Sustitución del símbolo ¶.

Normalización de valores vacíos y eliminación de registros sin contenido.

2. Limpieza profunda para NLP

Se implementa la función limpiar_texto(), la cual:

* Convierte a minúsculas.
* Filtra caracteres no alfabéticos mediante expresiones regulares.
* Normaliza secuencias de espacios.
* Retorna texto depurado apto para tokenización y modelado.


"""

# =================================================================
# SETUP E IMPORTACIÓN DE DATOS
# =================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# (BLOQUE PREVIO – YA EJECUTADO)
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# import nltk
# import re
# from collections import Counter
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize



# Descargar recursos de NLTK (solo necesario la primera vez)
try:
    nltk.download('stopwords')
    nltk.download('punkt')
except:
    pass  # Ya descargado

# Ruta del archivo subido al entorno (proveído por ChatGPT)
FILE_PATH = '/content/spanish_sagradas_escrituras_1569.txt'

# Columnas del formato BCVS (The Unbound Bible)
COLUMNS = [
    'nrsva_book_index', 'nrsva_chapter', 'nrsva_verse',
    'orig_book_index', 'orig_chapter', 'orig_verse',
    'orig_subverse', 'order_by', 'text'
]

# =================================================================
# CARGA DEL ARCHIVO
# =================================================================
df = pd.read_csv(
    FILE_PATH,
    sep='\t',
    names=COLUMNS,
    comment='#',
    encoding='utf-8',
    skipinitialspace=True
)

# =================================================================
# LIMPIEZA INICIAL DEL TEXTO
# =================================================================

# Eliminar etiquetas <I> ... </I> y el símbolo ¶
df['text_clean'] = df['text'].astype(str)
df['text_clean'] = df['text_clean'].apply(lambda x: re.sub(r'<I>.*?</I>', '', x))
df['text_clean'] = df['text_clean'].apply(lambda x: x.replace('¶', ' ').strip())

# Reemplazar cadenas vacías por NaN y eliminar filas vacías
df['text_clean'] = df['text_clean'].replace('', np.nan)
df = df.dropna(subset=['text_clean']).reset_index(drop=True)

# =================================================================
# LIMPIEZA PROFUNDA DEL TEXTO (para NLP)
# =================================================================

def limpiar_texto(t):
    t = t.lower()  # Minúsculas
    t = re.sub(r'[^a-záéíóúñü ]', ' ', t)  # Solo letras
    t = re.sub(r'\s+', ' ', t)  # Un solo espacio
    return t.strip()

df['text_clean_deep'] = df['text_clean'].apply(limpiar_texto)

print(" DataFrame cargado y limpiado correctamente")

print("\nPrimeras 5 filas del DataFrame limpio:")
print(df[['nrsva_book_index', 'nrsva_chapter', 'nrsva_verse', 'text_clean_deep']].head())

print("\nInformación del DataFrame:")
print(df.info())

"""## Ruta y configuración

Posteriormente se ejecuta la instrucción pd.read_csv() para cargar el archivo con los siguientes parámetros técnicos:

* sep='\t': Indica que el archivo está estructurado mediante tabulaciones.
* names=COLUMNS: Asigna las columnas manualmente, dado que el archivo no contiene encabezados.
* comment='#': Ignora líneas marcadas como comentarios dentro del archivo.
* encoding='utf-8': Asegura compatibilidad con caracteres acentuados del español.
* skipinitialspace=True: Normaliza espacios posteriores al separador.

El resultado es un DataFrame estructurado que preserva la indexación secuencial del corpus. Finalmente, se imprime su dimensión utilizando df.shape para validar que la carga inicial del dataset se completó correctamente.
"""

# =================================================================
# RUTA
# =================================================================

FILE_PATH = '/content/spanish_sagradas_escrituras_1569.txt'

COLUMNS = [
    'nrsva_book_index', 'nrsva_chapter', 'nrsva_verse',
    'orig_book_index', 'orig_chapter', 'orig_verse',
    'orig_subverse', 'order_by', 'text'
]

df = pd.read_csv(
    FILE_PATH,
    sep='\t',
    names=COLUMNS,
    comment='#',
    encoding='utf-8',
    skipinitialspace=True
)

print("DataFrame original cargado:", df.shape)

"""# Manejo de faltantes y duplicados

1. Revisión de valores faltantes
2. Normalización y eliminación de textos vacíos
3. Eliminación de duplicados
4. Validación posterior a la limpieza
"""

# =================================================================
# MANEJO DE FALTANTES Y DUPLICADOS
# =================================================================

# Copia de trabajo para no modificar el original
df_clean = df.copy()

# 1.1 Revisión de valores faltantes
print("Valores faltantes por columna antes de limpiar:")
print(df_clean.isna().sum())

# 1.2 Eliminar filas donde 'text' esté vacío o sea NaN
df_clean['text'] = df_clean['text'].astype(str)
df_clean['text'] = df_clean['text'].replace(['', ' ', 'nan'], np.nan)
df_clean = df_clean.dropna(subset=['text'])

# 1.3 Eliminación de filas duplicadas exactas en todas las columnas
df_clean = df_clean.drop_duplicates()

# (Opcional) Eliminación de duplicados solo a nivel de texto
# df_clean = df_clean.drop_duplicates(subset=['text'])

df_clean = df_clean.reset_index(drop=True)

print("\n Después de limpiar faltantes y duplicados:")
print("Shape:", df_clean.shape)
print("Valores faltantes por columna:")
print(df_clean.isna().sum())

"""# BLOQUE 2 – Eliminación de ruido (etiquetas, símbolos, puntuación, números…)

Este bloque implementa un proceso avanzado de normalización destinado a depurar el contenido textual eliminando elementos considerados “ruido” para tareas posteriores de procesamiento de lenguaje natural (NLP). Para preservar la consistencia del dataset previamente limpiado, se genera una copia operativa (df_noise = df_clean.copy()).

1. Definición de la función eliminar_ruido()
2. Generación de la columna depurada
3. Validación de la depuración
"""

# =================================================================
# ELIMINACIÓN DE RUIDO
# =================================================================

import re

# Partimos de df_clean generado en el bloque anterior
df_noise = df_clean.copy()

def eliminar_ruido(texto):
    # Aseguramos que sea string
    t = str(texto)

    # 2.1 Eliminar etiquetas HTML tipo <I>...</I> y similares
    t = re.sub(r'<.*?>', ' ', t)

    # 2.2 Eliminar símbolo ¶
    t = t.replace('¶', ' ')

    # 2.3 Eliminar números
    t = re.sub(r'\d+', ' ', t)

    # 2.4 Eliminar signos de puntuación y caracteres no alfabéticos
    t = re.sub(r"[^a-zA-ZáéíóúÁÉÍÓÚñÑüÜ\s]", " ", t)

    # 2.5 Reemplazar múltiples espacios por uno solo
    t = re.sub(r'\s+', ' ', t)

    # 2.6 Quitar espacios al inicio y al final
    t = t.strip()

    return t

# Nueva columna con texto sin ruido
df_noise['text_no_noise'] = df_noise['text'].apply(eliminar_ruido)

print(" Ruido eliminado. Ejemplo de antes / después:\n")
print("ANTES :", df_noise['text'].iloc[0])
print("DESPUÉS:", df_noise['text_no_noise'].iloc[0])

"""# Normalización (minusculización, stopwords, tokenización, etc.)

Este bloque implementa la fase de normalización lingüística del corpus, sobre la base del texto ya depurado de ruido (df_noise). El objetivo es obtener una representación estandarizada del texto apta para tareas de análisis léxico, estadístico y de NLP.

1. Configuración de recursos NLTK
2. Copia de trabajo y conversión a minúsculas
3. Tokenización
4. Definición y aplicación de stopwords en español
5. Reconstrucción del texto normalizado
6. Validación cualitativa
"""

# =================================================================
# NORMALIZACIÓN DEL TEXTO (CORREGIDO y VALIDADO)
# =================================================================

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Descargar recursos necesarios
try:
    nltk.download('stopwords')
    nltk.download('punkt')
    nltk.download('punkt_tab')  # NECESARIO EN GOOGLE COLAB
except:
    pass

df_norm = df_noise.copy()

# 3.1 Convertir a minúsculas
df_norm['text_lower'] = df_norm['text_no_noise'].str.lower()

# 3.2 Tokenización
df_norm['tokens'] = df_norm['text_lower'].apply(word_tokenize)

# 3.3 Stopwords en español
stopwords_es = set(stopwords.words('spanish'))

def quitar_stopwords(tokens):
    return [tok for tok in tokens if tok not in stopwords_es and len(tok) > 1]

df_norm['tokens_no_stop'] = df_norm['tokens'].apply(quitar_stopwords)

# 3.4 Reconstruir texto normalizado
df_norm['text_normalized'] = df_norm['tokens_no_stop'].apply(lambda toks: ' '.join(toks))

print(" Normalización completada correctamente.")

# Mostrar ejemplo
print("\nEjemplo:\n")
print("Original :", df_norm['text_no_noise'].iloc[0])
print("Tokens   :", df_norm['tokens'].iloc[0][:20])
print("Sin stop :", df_norm['tokens_no_stop'].iloc[0][:20])
print("Texto normalizado :", df_norm['text_normalized'].iloc[0][:200])

"""# Estadistica Basica

## Número total de versículos

Este bloque implementa la visualización descriptiva del corpus bíblico agrupado por libro, utilizando como referencia el índice NRSVA presente en el dataset. Para ello:

1. Agrupación del corpus
2. Construcción del gráfico de barras
3. Etiquetas y estilo del gráfico

### Interpretación del gráfico

El gráfico evidencia de forma inmediata la variabilidad significativa en la extensión de los libros bíblicos, medida en cantidad de versículos. Se observan tendencias clave:

Libros particularmente extensos:
Destaca un pico extremadamente elevado en el libro correspondiente al índice con mayor barra (aproximadamente 2.450 versículos), lo que sugiere que pertenece a uno de los libros más largos del Antiguo Testamento, típicamente Salmos, Jeremías o Ezequiel, dependiendo del índice utilizado por la edición NRSVA.

Distribución heterogénea:
Hay libros con más de 1.000 versículos, mientras otros apenas superan los 50. Esto refleja la diversidad literaria y teológica del corpus: desde narraciones largas y proféticas hasta libros cortos de carácter epistolar o sapiencial.

Patrón bimodal aproximado:
Se aprecia un conjunto de libros de mayor extensión al inicio del eje (representando el Pentateuco y libros históricos) y otro grupo elevado en la segunda mitad (probablemente evangelios y otros libros del Nuevo Testamento).
Entre ambos grupos, una zona de libros medianos y cortos provee una transición.

Libros muy breves:
Se observan varios índices con valores inferiores a 50 versículos, típicamente correspondientes a Obdías, Judas, Filemón, 2 Juan, 3 Juan, entre otros.

En conjunto, este gráfico permite visualizar la estructura general del corpus y facilita decisiones posteriores para análisis de frecuencia, segmentación, modelado o comparaciones entre libros.
"""

# =================================================================
# Cálculo del número total de versículos
# =================================================================

total_versiculos = len(df_norm)

print(" Número total de versículos en la base de datos:", total_versiculos)

# =================================================================
# Gráfico de número de versículos por libro
# =================================================================

import matplotlib.pyplot as plt

# Contar versículos por libro usando el índice de libro original
versiculos_por_libro = df_norm.groupby('nrsva_book_index').size()

# Crear el gráfico
plt.figure(figsize=(14, 6))
versiculos_por_libro.plot(kind='bar')

plt.title('Número de Versículos por Libro')
plt.xlabel('Libro (Índice NRSVA)')
plt.ylabel('Cantidad de Versículos')

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Número total de palabras

1. Validación estructural
Antes de procesar, se comprueba que exista la columna tokens_no_stop en df_norm.

* Si no existe, se lanza una excepción (ValueError) indicando que la etapa de normalización (tokenización y eliminación de stopwords) no ha sido ejecutada.

2. Cálculo del total de palabras “limpias”

* df_norm['tokens_no_stop'].apply(len) calcula, para cada versículo, el número de tokens restantes tras la limpieza y eliminación de stopwords.

* sum() acumula todos esos conteos a nivel de corpus, obteniendo el número total de palabras significativas presentes en la versión normalizada del texto.

3. Salida informativa
Se imprime el total de palabras, dejando explícito que se trata de palabras limpias y sin stopwords, es decir, un vocabulario más cercano al contenido semántico relevante.
"""

# =================================================================
# Cálculo del número total de palabras
# =================================================================

# Asegurarse de que la columna existe
if 'tokens_no_stop' not in df_norm.columns:
    raise ValueError("⚠ La columna 'tokens_no_stop' no existe. Asegúrate de haber ejecutado la normalización.")

# Contar palabras
total_palabras = df_norm['tokens_no_stop'].apply(len).sum()

print(" Número total de palabras (limpias y sin stopwords):", total_palabras)

"""1. Cálculo del tamaño por versículo
2. Construcción del gráfico de línea
3. Rotulación y estilo

El gráfico muestra, versículo a versículo, cómo varía la longitud de los versículos en palabras significativas a lo largo de toda la Biblia (según el orden de order_by):

* Rango de variación
La mayoría de los versículos parecen situarse aproximadamente entre 10 y 25 palabras, con picos que alcanzan alrededor de 30–35. Esto sugiere una longitud media relativamente estable, pero con variabilidad suficiente como para evidenciar versículos muy breves y otros bastante más extensos.

* Patrones y bloques
Se observan segmentos del gráfico con mayor densidad de valores altos y otros con valores más bajos y compactos.
* * Los tramos con mayor concentración de palabras podrían corresponder a libros narrativos o proféticos, donde las oraciones tienden a ser más desarrolladas.
* * Los tramos con valores más bajos pueden asociarse a libros con estilo proverbio, sentencias o versículos muy cortos.

* Estabilidad global con fluctuaciones locales
Aunque el gráfico presenta muchas oscilaciones punto a punto (propias de un versículo más largo seguido de uno más corto, etc.), la “banda” general de valores es relativamente estable: no hay secciones del corpus dominadas por versículos extremadamente largos o extremadamente cortos de forma continua.

* Utilidad analítica
Esta visualización permite:

* * Detectar outliers (versículos inusualmente largos o cortos).
* * Estudiar diferencias estilísticas entre secciones del canon.
* * Servir como base para análisis posteriores, por ejemplo, relacionar longitud de versículo con tipo de libro, temática o testamento (Antiguo / Nuevo).
"""

# =================================================================
# Gráfico de número de palabras por versículo
# =================================================================

import matplotlib.pyplot as plt

# Crear una columna con el conteo de palabras por versículo
df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

plt.figure(figsize=(14, 5))
plt.plot(df_norm['word_count'], alpha=0.7)

plt.title("Número de Palabras por Versículo (Texto Normalizado)")
plt.xlabel("Versículo (ordenado por 'order_by')")
plt.ylabel("Cantidad de Palabras")

plt.grid(True, linestyle='--', alpha=0.5)

plt.show()

"""# Promedio de palabras por libro

Este bloque calcula y visualiza la longitud promedio de los versículos en cada libro del corpus, utilizando el conteo de palabras ya procesado en df_norm.

El gráfico revela diferencias claras en la estructura lingüística de los libros bíblicos:

1. Variabilidad entre libros

Los promedios varían aproximadamente entre 7 y 15 palabras por versículo, lo que indica:

Libros más “explícitos” o narrativos tienden a tener versículos más largos.

Libros más poéticos, sapienciales o epistolares tienden a tener versículos más breves.

2. Libros con versículos significativamente más largos

Se observan picos destacados en ciertos índices, donde los promedios superan las 14–15 palabras por versículo.
Estos suelen corresponder a:

* Libros proféticos extensos
* Libros narrativos del AT con frases largas
* Evangelios con estructura discursiva más fluida

La presencia de estos picos sugiere un estilo más elaborado y denso en su sintaxis.

3. Libros con versículos más cortos

En varios índices aparecen valores cercanos a 7–8 palabras, consistentes con:

* Libros muy breves
* Narraciones concisas
* Partes del NT donde predominan exhortaciones breves o fórmulas litúrgicas

Estos libros presentan un estilo más directo y compacto.

4. Perfil general del corpus

A nivel global:

* La mayoría de los libros se sitúan entre 10 y 12 palabras por versículo, indicando una estructura relativamente estable en promedio.
* No existe una distribución uniforme: hay picos y depresiones que reflejan diversidad en géneros literarios (narración, poesía, profecía, cartas, proverbios).
"""

# =================================================================
# Promedio de palabras por libro
# =================================================================

promedio_palabras_libro = df_norm.groupby('nrsva_book_index')['word_count'].mean()

plt.figure(figsize=(14, 6))
promedio_palabras_libro.plot(kind='bar')

plt.title("Promedio de Palabras por Versículo en Cada Libro")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("Promedio de Palabras")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Número total de palabras únicas ("vocabulario")

Este bloque calcula el tamaño del vocabulario global del texto bíblico normalizado

1. Aplanamiento de listas de tokens
2. Eliminación de duplicados y cálculo del vocabulario
"""

# =================================================================
# Cálculo del número de palabras únicas (vocabulario)
# =================================================================

# Aplanar lista de listas
todas_las_palabras = [pal for lista in df_norm['tokens_no_stop'] for pal in lista]

# Convertir a conjunto (elimina duplicados)
vocabulario = set(todas_las_palabras)

num_palabras_unicas = len(vocabulario)

print(" Número total de palabras únicas en el texto:", num_palabras_unicas)

"""Este bloque desagrega el análisis de vocabulario a nivel de cada libro:

1. Función vocabulario_por_libro(df)
2. Conversión a Series
3. Construcción del gráfico de barras

El gráfico muestra cómo varía la riqueza léxica de cada libro bíblico, medida como el número de palabras distintas usadas:

1. Libros con vocabulario especialmente amplio
* Se observan barras muy altas (por sobre las 4.000–4.500 palabras únicas) en algunos índices del Antiguo Testamento.
* Esto suele corresponder a libros largos y teológicamente complejos (por ejemplo, Salmos, Jeremías, Ezequiel, etc.), donde la variedad temática y poética incrementa la diversidad de términos.

2. Libros con vocabulario moderado
* Un número importante de libros se concentra entre ~1.500 y ~3.000 palabras únicas.
* Estos libros mantienen una buena riqueza léxica, pero con menor extensión o repetición de estructuras, típico de libros históricos, narrativos o cartas extensas.

3. Libros con vocabulario reducido
* En la zona final del gráfico aparecen libros con menos de 500, 200 o incluso cerca de 100 palabras únicas.
* Esto es coherente con libros muy breves (por ejemplo, Filemón, 2 Juan, 3 Juan, Judas, Abdías, etc.), donde la extensión del texto limita naturalmente el tamaño del vocabulario.

4. Relación entre longitud y vocabulario
* Aunque hay una correlación intuitiva (libros más largos tienden a tener más vocabulario), no es lineal:
* * Algunos libros relativamente extensos pueden mostrar vocabulario menos variado (uso reiterado de fórmulas o estructuras).
* * Otros, aun sin ser los más largos, presentan un vocabulario muy rico, posiblemente por su carácter poético o sapiencial.
"""

# =================================================================
# Tamaño del vocabulario por libro
# =================================================================

import matplotlib.pyplot as plt

# Función para obtener vocabulario de un libro
def vocabulario_por_libro(df):
    resultados = {}
    for libro, grupo in df.groupby('nrsva_book_index'):
        palabras = [pal for lista in grupo['tokens_no_stop'] for pal in lista]
        resultados[libro] = len(set(palabras))
    return resultados

vocab_libros = vocabulario_por_libro(df_norm)

# Convertir a Series para facilitar el gráfico
import pandas as pd
vocab_series = pd.Series(vocab_libros).sort_index()

# Gráfico
plt.figure(figsize=(14, 6))
vocab_series.plot(kind='bar')

plt.title("Tamaño del Vocabulario por Libro")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("Número de palabras únicas")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Longitud promedio del texto por versículo

1. Cálculo de la longitud por versículo
2. Cálculo del promedio global de longitud
3. Cálculo del promedio por libro
4. Construcción y visualización del gráfico de longitud promedio
"""

# =================================================================
# Longitud promedio del texto por versículo
# =================================================================

# Crear columna de longitud por versículo
df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

# Longitud promedio
longitud_promedio = df_norm['word_count'].mean()

print(" Longitud promedio del texto por versículo:", longitud_promedio)

"""El gráfico refleja cómo varía la longitud promedio de los versículos —medida en cantidad de palabras significativas tras la normalización del texto— en cada libro del corpus bíblico. El análisis muestra varios patrones relevantes:

1. Variabilidad significativa entre libros

Los promedios oscilan aproximadamente entre 7 y 15 palabras por versículo, lo que evidencia que cada libro posee una estructura sintáctica propia. Esta variación refleja diferencias en el género literario:

* Los libros narrativos y proféticos suelen tener versículos más extensos.
* Los libros sapienciales, epistolares o de estructura sentenciosa tienden a tener versículos más breves.

2. Libros con versículos especialmente largos

Algunos libros presentan promedios superiores a las 14–15 palabras, señalando un estilo más complejo:

* Escritura más discursiva.
* Frases largas con abundantes complementos.
* Narraciones detalladas o profecías extensas.

Estos patrones se asocian habitualmente con libros del Antiguo Testamento de alta densidad teológica y narrativa.

3. Libros con versículos especialmente cortos

En el extremo inferior del gráfico se observan libros con promedios en torno a 7–8 palabras por versículo, característicos de:

* Epístolas breves del Nuevo Testamento.
* Libros proféticos menores.
* Literatura sapiencial con frases concentradas y directas.

Este estilo muestra textos más concisos y de estructura simple.

4. Distribución global relativamente estable

A pesar de la variación entre libros, la mayoría se agrupa en una banda entre 10 y 12 palabras por versículo, lo que indica:

* una relativa homogeneidad del corpus en términos de densidad textual,
* un equilibrio en la estructura de los versículos,
* pero con oscilaciones asociadas a los distintos géneros literarios presentes en la Biblia.
"""

# =================================================================
# Gráfico: Longitud promedio por libro
# =================================================================

import matplotlib.pyplot as plt

# Calcular promedio por libro
promedio_por_libro = df_norm.groupby('nrsva_book_index')['word_count'].mean()

plt.figure(figsize=(14, 6))
promedio_por_libro.plot(kind='bar')

plt.title("Longitud Promedio del Texto por Versículo en Cada Libro")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("Número Promedio de Palabras por Versículo")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""1. Obtención de la longitud por versículo
2. Construcción de la distribución global de longitudes
3. Visualización de la distribución
4. Análisis descriptivo de la variabilidad del corpus

### Grafico
1. Amplia variabilidad en las longitudes de los versículos

La distribución muestra que los versículos presentan longitudes muy diversas:

* Algunos son extremadamente breves (3–5 palabras).
* La mayoría se desplaza en un rango de 10–20 palabras.
* Existen versículos más extensos que superan las 30 palabras, aunque son menos frecuentes.

Esto evidencia que el corpus combina géneros literarios heterogéneos, desde expresiones concisas hasta discursos desarrollados.

2. Alta concentración en torno a una longitud central

El grueso de la distribución se concentra alrededor de 10 a 15 palabras por versículo, lo cual coincide con los promedios observados por libro. Esto sugiere:

* un patrón sintáctico relativamente estable,
* versículos que tienden a tener un tamaño “modal” común,
* una estructura que facilita la memorización, recitación y lectura tradicional.

3. Presencia de colas hacia valores altos

Aunque la densidad principal se ubica en la zona media, existe una cola que se extiende hacia versículos largos:

* Estos corresponden a pasajes más narrativos, proféticos o discursivos,
* donde las ideas se desarrollan en oraciones más complejas.

Su presencia indica que, aunque minoritarios, los versículos largos aportan diversidad estructural y semántica al corpus.

4. Evidencia de heterogeneidad literaria

La forma general de la distribución refleja:

* Libros poéticos o sapienciales: tienden a aportar versículos más cortos y directos.
* Libros narrativos o proféticos: aportan versículos más largos y variables.
* Epístolas: combinan concisión con algunas frases extendidas.

Esto confirma que el análisis de longitud es un buen indicador de género y estilo literario.

5. Utilidad analítica de la distribución global

La distribución de longitudes permite:

* detectar patrones estilométricos generales del corpus,
* identificar outliers (versículos excepcionalmente largos o cortos),
* definir umbrales para estudios posteriores (e.g., segmentaciones basadas en longitud),
* comprender diferencias sintácticas entre secciones sin necesidad de leer texto.
"""

# =================================================================
# Distribución general de longitudes de versículos
# =================================================================

plt.figure(figsize=(12, 5))
plt.hist(df_norm['word_count'], bins=40, alpha=0.7)

plt.title("Distribución de Longitudes de Versículos")
plt.xlabel("Número de Palabras por Versículo")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""### Calcular longitud máxima y mínima del versículo

1. Identificación de la longitud mínima del versículo
2. Identificación de la longitud máxima del versículo
3. Obtención de los versículos asociados a esos extremos
4. Análisis comparativo entre versículos extremadamente cortos y largos
"""

# =================================================================
# Longitud máxima y mínima del versículo
# =================================================================

# Asegurar columna de conteo de palabras
df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

longitud_maxima = df_norm['word_count'].max()
longitud_minima = df_norm['word_count'].min()

# Versículos específicos (opcional)
versiculo_max = df_norm[df_norm['word_count'] == longitud_maxima].iloc[0]
versiculo_min = df_norm[df_norm['word_count'] == longitud_minima].iloc[0]

print(" Longitud máxima de un versículo:", longitud_maxima, "palabras")
print(" Versículo más largo:", versiculo_max['text_no_noise'])
print()
print(" Longitud mínima de un versículo:", longitud_minima, "palabras")
print(" Versículo más corto:", versiculo_min['text_no_noise'])

"""1. Cálculo de los valores extremos de longitud (mínimo y máximo)
2. Cálculo de la longitud promedio del corpus
3. Integración de los valores en una estructura comparativa
4. Construcción del gráfico con valores máximo, mínimo y promedio

1. Longitud mínima: evidencia de versículos extremadamente breves

La barra correspondiente a la longitud mínima muestra un valor muy bajo, en torno a 0–1 palabras. Este fenómeno puede explicarse por:

* versículos originalmente muy breves en el texto,
* efectos de normalización que eliminan stopwords y reducen el contenido a una o ninguna palabra relevante,
* estructuras propias de algunos libros: expresiones cortas, títulos internos, frases unidad de un solo término.

Estos valores demuestran que existen pasajes con síntesis extrema, característicos de géneros como poesía sapiencial o epístolas breves.

2. Longitud promedio: estructura central del corpus

El promedio se sitúa alrededor de 10–11 palabras por versículo, lo cual coincide con los análisis anteriores. Este valor representa:

* la “longitud típica” del versículo bíblico,
* una medida estable que refleja equilibrio entre concisión y complejidad,
* una estructura sintáctica recurrente en la mayoría de los libros.

Este promedio sirve como punto de referencia para evaluar desviaciones extremas y comparar estilos entre libros.

3. Longitud máxima: presencia de versículos muy extensos

La barra correspondiente a la longitud máxima (cercana a 35 palabras) indica versículos notablemente largos, asociados a:

* descripciones detalladas,
* estructuras con varias cláusulas,
* discursos proféticos o narrativos,
* oraciones complejas que condensan grandes cantidades de contenido en un solo versículo.

Estos versículos representan el extremo opuesto del corpus, mostrando alta densidad textual.

4. La diferencia entre mínimo, promedio y máximo revela una fuerte heterogeneidad

La distancia entre los tres valores pone en evidencia:

* que el corpus contiene una gran diversidad sintáctica,
* la coexistencia de versículos muy breves y otros muy extensos,
* un amplio rango estilístico entre géneros literarios (narrativo, poético, profético, epistolar).

Este contraste reafirma que la longitud del versículo es una métrica reveladora de la diversidad literaria bíblica.
"""

# =================================================================
# Gráfico: Longitud máxima, mínima y promedio
# =================================================================

import matplotlib.pyplot as plt

longitud_promedio = df_norm['word_count'].mean()

metricas = {
    "Mínima": longitud_minima,
    "Promedio": longitud_promedio,
    "Máxima": longitud_maxima
}

plt.figure(figsize=(8, 5))
plt.bar(metricas.keys(), metricas.values(), color=['green', 'blue', 'red'])

plt.title("Longitud Mínima, Promedio y Máxima de los Versículos")
plt.ylabel("Número de Palabras")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""1. Obtención de la longitud del versículo a partir del texto normalizado
2. Construcción de la distribución de frecuencias
3. Generación del histograma de longitudes
4. Evaluación visual de la dispersión y forma de la distribución

Distribución de Longitudes de Versículos

El gráfico representa un histograma que muestra cómo se distribuyen las longitudes de los versículos —medidas en número de palabras significativas normalizadas— en todo el corpus bíblico. La distribución revela múltiples características estilísticas y estructurales del texto.

1. La mayor parte de los versículos tiene entre 6 y 12 palabras

El histograma muestra un claro núcleo central:

* La mayor frecuencia se concentra aproximadamente entre 7 y 12 palabras,
* Con picos alrededor de los 8–9 y 10–11.

Esto confirma que la Biblia, en su conjunto, utiliza una estructura versicular relativamente estable: versículos de longitud media, ni demasiado cortos ni excesivamente largos.

Este comportamiento coincide con los promedios por libro analizados previamente.

2. La distribución tiene forma asimétrica (sesgo positivo)

La cola derecha se extiende hacia longitudes mayores:

* Existen versículos que alcanzan entre 20 y 35 palabras,
* Sin embargo, su frecuencia es mucho menor.

Esto produce una distribución asimétrica a la derecha, lo cual indica:

* que los versículos largos son minoritarios,
* pero suficientemente presentes como para generar una cola extensa,
* reflejando la existencia de discursos, descripciones o estructuras sintácticas complejas en ciertas secciones del corpus.

3. Presencia de versículos ultracortos

A la izquierda, el histograma muestra frecuencias bajas pero presentes para valores entre 0 y 3 palabras.
Esto sugiere:

* versículos muy breves,
* títulos internos,
* frases aisladas,
* o efectos de la normalización (pérdida de stopwords en versículos muy cortos).

Estos elementos refuerzan la idea de una diversidad extrema en la construcción versicular.

4. Alta densidad de valores medios: estabilidad estilística

La mayoría del corpus se concentra en un rango estrecho:

* alrededor de 7–12 palabras,
* lo que implica una gran consistencia estructural en términos de longitud,
* especialmente en libros narrativos y en epístolas largas.

Este patrón indica que la Biblia, como corpus total, tiene una tendencia hacia versículos de densidad media, lo cual facilita:

* la lectura,
* la memorización,
* y el análisis sintáctico sistemático.

5. La cola derecha revela complejidad sintáctica localizada

Los versículos más largos (20–35 palabras) representan:

* discursos teológicos amplios,
* descripciones detalladas,
* profecías extensas,
* oraciones subordinadas múltiples.

Aunque son pocos, su presencia es esencial para:

* la diversidad semántica del corpus,
* el enriquecimiento del vocabulario,
* la variabilidad estilométrica.
"""

# =================================================================
# Distribución de longitudes de versículos
# =================================================================

plt.figure(figsize=(12, 5))
plt.hist(df_norm['word_count'], bins=40, alpha=0.7, color='purple')

plt.title("Distribución de Longitudes de Versículos")
plt.xlabel("Número de Palabras")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""### Cálculo de la distribución de longitudes

1. Cálculo estadístico de medidas descriptivas de longitud
2. Identificación de valores centrales (media, mediana, moda)
3. Estimación de medidas de dispersión (rango, varianza, desviación estándar)
4. Detección de outliers a partir de métricas de distribución
5. Representación descriptiva para caracterizar la forma general de la distribución
"""

# =================================================================
# Distribución descriptiva de longitudes de versículos
# =================================================================

# Asegurar que existe la columna de conteo
df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

descripcion = df_norm['word_count'].describe()

print(" Distribución descriptiva de longitudes:")
print(descripcion)

"""1. Obtención de la longitud del versículo desde el texto normalizado
2. Generación del conjunto de longitudes para todo el corpus
3. Construcción del histograma de frecuencias
4. Análisis visual de la forma, dispersión y concentración de la distribución

1. El núcleo principal de la distribución se concentra entre 6 y 12 palabras por versículo

El histograma muestra una gran acumulación de valores en este rango, lo que indica que:

* la mayoría de los versículos son de longitud media,
* este intervalo constituye la “zona típica” o estándar del corpus,
* es el estilo predominante tanto en libros narrativos como en epístolas o textos poéticos.

Esto coincide con los promedios calculados previamente (aprox. 10–11 palabras).

2. La distribución presenta asimetría positiva (cola hacia la derecha)

Se observa una forma claramente sesgada hacia valores altos:

* La cola derecha se extiende hasta alrededor de 35 palabras,
* Lo cual evidencia la existencia de versículos largos, aunque poco frecuentes.

Esta asimetría sugiere la coexistencia de:

* versículos concisos y muy breves,
* versículos densos y extensos, típicos de discursos o descripciones más complejas.

3. Los versículos ultralargos son pocos pero significativos

Aunque la mayoría se concentra en la zona media, el histograma muestra:

* valores entre 20 y 35 palabras,
* representados por barras inferiores pero claramente presentes.

Estos versículos:

* pertenecen a libros donde el estilo literario permite mayor expansión sintáctica,
* tienden a ser narrativos o proféticos,

* aportan complejidad y riqueza semántica al corpus.

4. También hay versículos extremadamente cortos

En el extremo izquierdo aparecen valores entre 1 y 4 palabras, aunque con frecuencia menor:

* suelen ser afirmaciones breves,
* expresiones independientes,
* o versículos que pierden stopwords al normalizarse.

Estos valores indican la existencia de estructuras altamente concisas.

5. La forma general refleja una estructura sintáctica consistente pero heterogénea

El histograma muestra claramente:

* un centro dominante, que da estabilidad al corpus;
* una variabilidad moderada, que refleja distintos géneros literarios;
* una cola larga, que evidencia diversidad estilística.

Esto confirma que el texto bíblico presenta un equilibrio entre repetición estructural y complejidad sintáctica localizada.
"""

# =================================================================
# Histograma de la distribución de longitudes
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.hist(df_norm['word_count'], bins=40, alpha=0.7, color='blue')

plt.title("Distribución de Longitudes de Versículos")
plt.xlabel("Número de Palabras por Versículo")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""1. Obtención de la longitud de cada versículo
2. Construcción del conjunto de longitudes para el análisis estadístico
3. Generación del boxplot para identificar distribución y valores extremos
4. Interpretación visual de mediana, cuartiles y outliers

1. La mediana se ubica alrededor de 10 palabras

La línea central del boxplot indica que la longitud mediana del versículo es de aproximadamente 10 palabras, lo cual coincide con:

* los promedios globales,
* el núcleo del histograma,
* y la estructura sintáctica típica del corpus.

Esto confirma que los versículos del texto bíblico tienen una tendencia central clara y estable.

2. La “caja” muestra cuartiles que van aproximadamente entre 7 y 13 palabras

Los límites del boxplot indican:

* Q1 (cuartil inferior) ≈ 7 palabras,
* Q3 (cuartil superior) ≈ 13 palabras.

Esto significa que el 50% de todos los versículos se encuentran dentro de este rango, mostrando:

* una variabilidad moderada,
* un patrón sintáctico consistente,
* y ausencia de dispersión caótica en la mayor parte del corpus.

3. Los “bigotes” se extienden hasta cerca de 20–22 palabras

Los bigotes representan la longitud dentro de la cual los datos aún son considerados “normales” según la regla IQR. Esto indica que:

* versículos entre ~14 y ~22 palabras son relativamente menos comunes,
* pero siguen siendo parte del comportamiento esperado del corpus,
* y no se consideran outliers.

La presencia de bigotes largos muestra que existen pasajes extensos, pero no anómalos.

4. Los puntos individuales representan outliers (versículos atípicamente largos)

El boxplot muestra numerosos puntos aislados en el rango de:

* 23 a 35 palabras, aproximadamente.

Estos puntos representan versículos sustancialmente más largos que el resto. Su presencia sugiere:

* discursos, oraciones o descripciones muy desarrolladas,
* estructuras sintácticas complejas,
* géneros literarios con mayor densidad (por ejemplo, profecías extensas o narraciones elaboradas).

Aunque son minoritarios, aportan diversidad estilística y semántica al corpus.

5. La ausencia de outliers en el extremo corto confirma una longitud mínima estable

No se observan outliers extremadamente bajos (por debajo del bigote inferior), lo que indica:

* versículos cortos, pero dentro de límites esperables,
* normalización del texto que elimina stopwords sin generar casos extremos anómalos.
"""

# =================================================================
# Boxplot de longitudes de versículos
# =================================================================

plt.figure(figsize=(8, 4))
plt.boxplot(df_norm['word_count'], vert=False)

plt.title("Boxplot de Longitudes de Versículos")
plt.xlabel("Número de Palabras")

plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.show()

"""1. Obtención del conjunto de longitudes de versículos
2. Aplicación de un estimador de densidad por kernel (KDE)
3. Generación de la curva suavizada de densidad
4. Análisis visual de la forma continua de la distribución

1. La mayor densidad se concentra entre 6 y 10 palabras

La curva presenta un máximo claro (pico) alrededor de este rango, lo que indica que:

* la gran mayoría de los versículos tiene longitudes en este tramo,
* este rango constituye la “zona típica” del corpus,
* coincide con la mediana, la moda y las zonas más pobladas del histograma.

Este comportamiento refleja un patrón sintáctico muy estable en los versículos bíblicos.

2. Forma de la distribución: asimetría positiva o sesgo a la derecha

La curva asciende abruptamente desde valores bajos, alcanza su máximo entre 6 y 10 palabras, y luego desciende lentamente hacia la derecha.

Esto sugiere:

* una alta frecuencia de versículos cortos y medianos,
* una proporción menor de versículos largos,
* pero suficiente presencia de versículos extensos para generar una cola prolongada.

La asymetría positiva indica diversidad estructural en longitudes grandes.

3. La cola derecha refleja versículos largos (15–35 palabras)

Aunque la densidad se vuelve baja más allá de las 15–20 palabras, la curva se mantiene positiva hasta aproximadamente 35 palabras.

Esto muestra que:

* los versículos muy largos son excepcionales pero reales,
* pertenecen a libros con estilo discursivo, narrativo o profético,
* estos versículos aportan complejidad sintáctica al corpus.

La cola extendida es típica de textos con géneros variados.

4. La curva confirma la coherencia entre métricas descriptivas previas

El KDE coincide con:

* el núcleo del histograma,
* la mediana (~10 palabras),
* la moda (~8–9 palabras),
* los cuartiles del boxplot,
* y los valores extremos identificados.

Esto refuerza la validez del análisis y demuestra que la distribución tiene un comportamiento estable y consistente.

5. El KDE revela suavemente transiciones y tendencias que el histograma oculta

La ventaja del KDE es mostrar:

* cambios graduales en densidad,
* la forma global de la distribución,
* la estructura continua del conjunto de longitudes,
* zonas de transición entre tramos de baja y alta frecuencia.

Esto proporciona un entendimiento más fino del comportamiento del corpus.
"""

# =================================================================
# KDE (curva suavizada de la distribución)
# =================================================================

import seaborn as sns

plt.figure(figsize=(12, 5))
sns.kdeplot(df_norm['word_count'], fill=True)

plt.title("Distribución KDE de Longitudes de Versículos")
plt.xlabel("Número de Palabras")
plt.ylabel("Densidad")
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

"""### Calcular métricas estadísticas de longitud

1. Cálculo de la media de la longitud de los versículos
2. Cálculo de la mediana como medida de tendencia central robusta
3. Identificación de la moda como longitud más frecuente
4. Cálculo de la varianza como indicador de dispersión
5. Cálculo de la desviación estándar para medir la variabilidad estructural
6. Integración de las métricas en un resumen comparativo de la distribución
"""

# =================================================================
# Métricas estadísticas: media, mediana, moda, varianza,
# desviación estándar de la longitud de versículos
# =================================================================

import numpy as np
from scipy import stats

# Asegurar columna de conteo
df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

# Media
media = df_norm['word_count'].mean()

# Mediana
mediana = df_norm['word_count'].median()

# Moda (puede devolver varias; tomamos la primera)
moda = stats.mode(df_norm['word_count'], keepdims=True)[0][0]

# Varianza
varianza = df_norm['word_count'].var()

# Desviación estándar
desviacion = df_norm['word_count'].std()

print(" MÉTRICAS ESTADÍSTICAS DE LONGITUD DE VERSÍCULOS")
print("-----------------------------------------------")
print(f"Media: {media:.2f} palabras")
print(f"Mediana: {mediana:.2f} palabras")
print(f"Moda: {moda} palabras")
print(f"Varianza: {varianza:.2f}")
print(f"Desviación Estándar: {desviacion:.2f}")

"""1. Las medidas de tendencia central (media, mediana, moda) muestran gran coherencia

En el gráfico, estos valores suelen aparecer cercanos entre sí:

* Moda ≈ 8–9 palabras
* Mediana ≈ 10 palabras
* Media ≈ 10–11 palabras

Esta cercanía indica:

* una distribución unimodal,
* ausencia de anomalías fuertes en el centro,
* un corpus con estructura estable en términos de longitud.

La moda ligeramente más baja sugiere un sesgo positivo, lo cual se confirma en otros gráficos (histograma, KDE).

2. La diferencia entre media y moda evidencia una cola a la derecha

El hecho de que la media > moda implica:

* la presencia de versículos largos que empujan el promedio hacia arriba,
* confirmación de la asimetría positiva observada en la distribución de densidad,
* impacto real de los versículos extensos aunque sean minoritarios.

Este comportamiento es típico de distribuciones lingüísticas naturales.

3. La varianza y la desviación estándar muestran una dispersión moderada

En el gráfico, estas métricas aparecen visiblemente separadas de las medidas de tendencia central:

* Desviación estándar ≈ 4–5 palabras
* Varianza (valor mayor, pero proporcional al cuadrado de la desviación)

Esto indica:

* variabilidad estructural significativa,
* coexistencia de versículos muy cortos y muy largos,
* pero con un núcleo estable que mantiene la distribución compacta.

La presencia de dispersión moderada confirma lo observado en el boxplot.

4. La comparación visual permite ver equilibrio entre estabilidad y diversidad

El gráfico muestra claramente que:

* las métricas centrales son cercanas → estabilidad textual,
* las métricas de dispersión son notables → diversidad estilística,
* existe una separación natural entre “centro” y “variabilidad”.

Este equilibrio es propio de corpus complejos donde conviven géneros diversos (narrativo, poético, legal, epistolar).
"""

# =================================================================
# Gráfico comparativo de métricas estadísticas
# =================================================================

import matplotlib.pyplot as plt

metricas = {
    "Media": media,
    "Mediana": mediana,
    "Moda": moda,
    "Varianza": varianza,
    "Desviación Estándar": desviacion
}

plt.figure(figsize=(10, 5))
plt.bar(metricas.keys(), metricas.values(), color=['blue', 'green', 'orange', 'purple', 'red'])

plt.title("Métricas Estadísticas de Longitud de Versículos")
plt.ylabel("Valor")
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

# =================================================================
# Tabla resumen de métricas estadísticas
# =================================================================

import pandas as pd

tabla = pd.DataFrame({
    "Métrica": ["Media", "Mediana", "Moda", "Varianza", "Desviación Estándar"],
    "Valor": [media, mediana, moda, varianza, desviacion]
})

display(tabla)

"""### Cálculo de la cantidad de letras promedio por palabra"""

# =================================================================
# Cantidad de letras promedio por palabra
# =================================================================

# Aplanar lista de palabras
todas_las_palabras = [palabra for lista in df_norm['tokens_no_stop'] for palabra in lista]

# Calcular longitud de cada palabra
longitudes = [len(palabra) for palabra in todas_las_palabras]

# Promedio de letras
promedio_letras = sum(longitudes) / len(longitudes)

print(" Cantidad de letras promedio por palabra:", round(promedio_letras, 2))

"""1. Extracción de palabras normalizadas del corpus
2. Cálculo de la longitud en letras para cada palabra
3. Construcción del histograma de frecuencias de longitudes
4. Evaluación visual del patrón de distribución léxica

1. La mayor parte de las palabras tiene entre 4 y 7 letras

El gráfico muestra un claro núcleo central:

* El rango 4–7 letras concentra la mayor frecuencia absoluta.

* La longitud más común está alrededor de 4, 5 y 6 letras, evidenciado por los picos más altos.

* Este patrón coincide con el comportamiento típico de la lengua española, donde muchas palabras funcionales y léxicas tienen estas longitudes.

Este rango constituye la zona de máxima densidad léxica del corpus.

2. La distribución es unimodal con cola larga hacia la derecha

La forma del histograma indica:

* Una sola cúspide principal (unimodalidad),

* Aumento rápido desde longitudes cortas,

* Descenso gradual hacia longitudes superiores a 8 letras.

La cola hacia la derecha sugiere que existen palabras largas, pero su frecuencia es baja, una característica común en textos naturales.

3. Palabras cortas (2–3 letras) son frecuentes, pero menos que las medianas

Las barras correspondientes a longitudes de 2–3 letras muestran:

* una cantidad significativa de palabras breves (artículos, preposiciones, conjunciones),

* pero todavía menos que las longitudes óptimas de 4–6 letras,

* lo cual indica que el corpus contiene abundante vocabulario contenido y semánticamente significativo.

4. Las palabras largas (10–18 letras) son minoritarias

El extremo derecho del histograma muestra:

* presencia real de palabras de 10 a 18 letras,

* aunque en cantidades muy inferiores respecto al centro del gráfico.

Estas palabras suelen corresponder a:

* conceptos teológicos,

* términos compuestos,

* construcciones morfológicas complejas.

Su menor frecuencia es esperable y aporta diversidad semántica al corpus.

5. La forma global refleja la estructura morfológica típica del español

La distribución:

* coincide con patrones fonotácticos y morfológicos comunes de las lenguas románicas,

* respeta las proporciones habituales entre palabras cortas, medias y largas,

* confirma que el vocabulario bíblico combina palabras funcionales, nombres propios, verbos conjugados y términos específicos.
"""

# =================================================================
# Histograma de longitudes de palabras
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.hist(longitudes, bins=15, alpha=0.7, color='teal')

plt.title("Distribución de Longitudes de Palabras")
plt.xlabel("Número de Letras")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""1. Obtención de la longitud de cada palabra (número de letras)
2. Preparación del conjunto de longitudes para análisis descriptivo
3. Construcción del boxplot para evaluar dispersión y valores extremos
4. Identificación de mediana, cuartiles, bigotes y outliers

1. Mediana alrededor de 6 letras: muestra la longitud típica de las palabras

La línea central dentro de la caja indica que la longitud mediana de las palabras es aproximadamente:

6 letras,

lo cual coincide con el comportamiento esperado del español, donde las palabras léxicas más comunes (verbos, sustantivos, adjetivos) suelen tener entre 5 y 7 letras.

Esta mediana indica:

una estructura léxica estable,

predominancia de palabras de longitud media,

consistencia con el histograma de longitudes.

2. El rango intercuartílico (Q1–Q3) está entre 4 y 7 letras

La “caja” del boxplot abarca:

Q1 ≈ 4 letras

Q3 ≈ 7 letras

Esto significa que el 50% de todas las palabras se encuentra en este rango.
Este rango confirma:

concentración léxica en longitudes medias,

escasa dispersión en el núcleo del vocabulario,

alta representatividad lingüística del español moderno y clásico.

3. Los bigotes se extienden hasta ~10 letras: palabras largas pero no extremas

Los bigotes del boxplot alcanzan aproximadamente:

10 letras como límite de longitud “normal”.

Esto indica que palabras largas (como términos teológicos, nombres propios o verbos compuestos) son menos comunes pero aún dentro de la variabilidad natural.

Su presencia muestra:

complejidad moderada en el léxico,

coexistencia de palabras simples y complejas.

4. Los puntos aislados representan outliers: palabras muy largas (11–18 letras)

Los círculos individuales a partir de 11 letras representan palabras atípicamente largas.

Estas suelen corresponder a:

términos teológicos específicos,

nombres propios extensos,

compuestos morfológicos,

derivaciones complejas.

Aunque son poco frecuentes, su presencia añade:

diversidad semántica,

riqueza morfológica,

variación estilística.

El rango de estos outliers (11–18 letras) se alinea con el histograma previo.

5. Ausencia de outliers por debajo del valor mínimo

No se observan puntos aislados a la izquierda del bigote inferior.
Esto indica:

que las palabras cortas (por ejemplo, de 2 letras) no son anómalas,

que su frecuencia está dentro de los límites esperados,

y que su variación es menor que la de las palabras largas.

6. Interpretación global: un vocabulario equilibrado con variedad morfológica

El boxplot revela:

un núcleo léxico muy homogéneo (4–7 letras),

un mediana estrechamente alineada con el centro de la caja,

dispersión moderada hacia palabras largas,

y outliers que representan complejidad y riqueza terminológica.

Este patrón confirma la naturaleza del español bíblico:
una base léxica simple y funcional, enriquecida por términos extensos que aportan profundidad semántica.
"""

# =================================================================
# Boxplot de longitudes de palabras
# =================================================================

plt.figure(figsize=(8, 4))
plt.boxplot(longitudes, vert=False)

plt.title("Boxplot de Longitudes de Palabras")
plt.xlabel("Número de Letras")
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.show()

"""### Calcular porcentaje de stopwords vs. palabras útiles"""

# =================================================================
# Porcentaje de stopwords vs. palabras útiles
# =================================================================

from nltk.corpus import stopwords
stopwords_es = set(stopwords.words('spanish'))

# Aplanar listas
todas_las_palabras = [pal for lista in df_norm['tokens'] for pal in lista]

# Identificar stopwords
stopwords_en_texto = [pal for pal in todas_las_palabras if pal.lower() in stopwords_es]

# Palabras útiles (no stopwords)
palabras_utiles = [pal for pal in todas_las_palabras if pal.lower() not in stopwords_es]

# Totales
total = len(todas_las_palabras)
total_stop = len(stopwords_en_texto)
total_utiles = len(palabras_utiles)

# Porcentajes
porcentaje_stop = (total_stop / total) * 100
porcentaje_utiles = (total_utiles / total) * 100

print(" PORCENTAJE DE STOPWORDS VS. PALABRAS ÚTILES")
print("------------------------------------------------")
print(f"Total de palabras: {total}")
print(f"Stopwords: {total_stop} ({porcentaje_stop:.2f}%)")
print(f"Palabras útiles: {total_utiles} ({porcentaje_utiles:.2f}%)")

# =================================================================
# Gráfico: Stopwords vs palabras útiles (Pie Chart)
# =================================================================

import matplotlib.pyplot as plt

labels = ['Stopwords', 'Palabras útiles']
sizes = [porcentaje_stop, porcentaje_utiles]
colors = ['red', 'green']

plt.figure(figsize=(8, 8))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)
plt.title("Porcentaje de Stopwords vs. Palabras Útiles")
plt.axis('equal')  # Mantener forma circular
plt.show()

# =================================================================
# Gráfico de barras: Stopwords vs palabras útiles
# =================================================================

plt.figure(figsize=(8, 5))
plt.bar(['Stopwords', 'Palabras útiles'], [total_stop, total_utiles], color=['red', 'green'])

plt.title("Cantidad de Stopwords vs. Palabras Útiles")
plt.ylabel("Número de Palabras")
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Número de libros distintos"""

# =================================================================
# Número de libros distintos
# =================================================================

# Contar cuántos libros diferentes hay en el índice NRSVA
num_libros_distintos = df_norm['nrsva_book_index'].nunique()

print(" Número de libros distintos en el corpus:", num_libros_distintos)

# =================================================================
# Listado de libros distintos
# =================================================================

libros_unicos = df_norm['nrsva_book_index'].unique()
libros_unicos.sort()

print(" Libros presentes (índices NRSVA):")
print(libros_unicos)

# =================================================================
# Gráfico: Número de versículos por libro
# =================================================================

import matplotlib.pyplot as plt

versiculos_por_libro = df_norm.groupby('nrsva_book_index').size()

plt.figure(figsize=(14, 6))
versiculos_por_libro.plot(kind='bar')

plt.title("Cantidad de Versículos por Libro")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("Número de Versículos")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Número de capítulos por libro"""

# =================================================================
# Número de capítulos por libro
# =================================================================

# Contar capítulos únicos dentro de cada libro
capitulos_por_libro = df_norm.groupby('nrsva_book_index')['nrsva_chapter'].nunique()

print("📘 Número de capítulos por libro:")
print(capitulos_por_libro)

# =================================================================
# Gráfico: Número de capítulos por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
capitulos_por_libro.plot(kind='bar', color='skyblue')

plt.title("Número de Capítulos por Libro")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("Cantidad de Capítulos")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

# =================================================================
# Tabla ordenada de capítulos por libro
# =================================================================

import pandas as pd

tabla_capitulos = capitulos_por_libro.sort_index().reset_index()
tabla_capitulos.columns = ['Libro (NRSVA)', 'Capítulos']

display(tabla_capitulos)

"""### Calcular las Top N palabras más frecuentes"""

# =================================================================
# Top N palabras más frecuentes
# =================================================================

from collections import Counter

N = 50  # Cambia este valor para obtener más o menos palabras

# Aplanar lista de palabras útiles
todas_las_palabras_utiles = [pal for lista in df_norm['tokens_no_stop'] for pal in lista]

# Calcular frecuencias
frecuencias = Counter(todas_las_palabras_utiles)

top_n = frecuencias.most_common(N)

print(f"🔝 Top {N} palabras más frecuentes:")
for palabra, freq in top_n:
    print(f"{palabra}: {freq}")

# =================================================================
# Tabla con las Top N palabras más frecuentes
# =================================================================

import pandas as pd

df_top_n = pd.DataFrame(top_n, columns=['Palabra', 'Frecuencia'])
display(df_top_n)

# =================================================================
# Gráfico de barras: Top N palabras más frecuentes
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.bar(df_top_n['Palabra'], df_top_n['Frecuencia'], color='teal')

plt.title(f"Top {N} Palabras Más Frecuentes (Limpias y Sin Stopwords)")
plt.xlabel("Palabra")
plt.ylabel("Frecuencia")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Frecuencia de palabras por libro (tabla completa)"""

# =================================================================
# Ver los índices de libros disponibles en df_norm
# =================================================================

libros_disponibles = sorted(df_norm['nrsva_book_index'].unique())

print(" Libros disponibles (índices NRSVA):")
print(libros_disponibles)

# =================================================================
# Frecuencia de palabras por libro
# =================================================================

from collections import Counter

# Diccionario: libro → frecuencia de palabras
frecuencias_por_libro = {}

for libro, grupo in df_norm.groupby('nrsva_book_index'):
    palabras = [pal for lista in grupo['tokens_no_stop'] for pal in lista]
    frecuencias_por_libro[libro] = Counter(palabras)

print(" Frecuencias de palabras por libro generadas correctamente.")

# =================================================================
# Frecuencia de palabras por libro (seguro)
# =================================================================

from collections import Counter

frecuencias_por_libro = {}

for libro, grupo in df_norm.groupby('nrsva_book_index'):
    palabras = [pal for lista in grupo['tokens_no_stop'] for pal in lista]
    frecuencias_por_libro[libro] = Counter(palabras)

print(" Frecuencias por libro generadas correctamente.")
print(" Libros que tienen datos:", list(frecuencias_por_libro.keys()))

# =================================================================
# Selección segura de un libro para mostrar Top N palabras
# =================================================================

N = 20
libro_id = libros_disponibles[0]   # Por ejemplo, elegir el primero automáticamente

print(f" Usando el libro con índice NRSVA: {libro_id}")

top_n_libro = frecuencias_por_libro[libro_id].most_common(N)

print(f"\n Top {N} palabras del libro {libro_id}:")
for palabra, freq in top_n_libro:
    print(f"{palabra}: {freq}")

# =================================================================
# Tabla con las Top N palabras del libro elegido
# =================================================================

import pandas as pd

df_top_n_libro = pd.DataFrame(top_n_libro, columns=["Palabra", "Frecuencia"])
display(df_top_n_libro)

# =================================================================
# Gráfico Top N palabras del libro elegido
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.bar(df_top_n_libro['Palabra'], df_top_n_libro['Frecuencia'], color='darkcyan')

plt.title(f"Top {N} Palabras Más Frecuentes — Libro NRSVA {libro_id}")
plt.xlabel("Palabra")
plt.ylabel("Frecuencia")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Frecuencia de palabras por capítulo"""

# =================================================================
# Frecuencia de palabras por capítulo
# =================================================================

from collections import Counter

frecuencias_por_capitulo = {}

for (libro, cap), grupo in df_norm.groupby(['nrsva_book_index', 'nrsva_chapter']):
    palabras = [pal for lista in grupo['tokens_no_stop'] for pal in lista]
    frecuencias_por_capitulo[(libro, cap)] = Counter(palabras)

print(" Frecuencias generadas correctamente para cada capítulo.")
print("Ejemplo de claves disponibles (libro, capítulo):")
print(list(frecuencias_por_capitulo.keys())[:10])  # Mostrar solo primeros 10

# =================================================================
# Ver capítulos disponibles
# =================================================================

capitulos_disponibles = list(frecuencias_por_capitulo.keys())
capitulos_disponibles[:10]

# =================================================================
# Top N palabras de un capítulo
# =================================================================

N = 20

# Seleccionar un capítulo usando un índice válido
libro_id = capitulos_disponibles[0][0]      # Ej: primer libro disponible
capitulo_id = capitulos_disponibles[0][1]   # Ej: primer capítulo disponible

print(f" Analizando Libro {libro_id}, Capítulo {capitulo_id}")

top_n_capitulo = frecuencias_por_capitulo[(libro_id, capitulo_id)].most_common(N)

print(f" Top {N} palabras del libro {libro_id}, capítulo {capitulo_id}:")
for palabra, freq in top_n_capitulo:
    print(f"{palabra}: {freq}")

# =================================================================
# DataFrame de las Top N palabras del capítulo
# =================================================================

import pandas as pd

df_top_n_cap = pd.DataFrame(top_n_capitulo, columns=["Palabra", "Frecuencia"])
display(df_top_n_cap)

# =================================================================
# Gráfico Top N palabras del capítulo
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.bar(df_top_n_cap['Palabra'], df_top_n_cap['Frecuencia'], color='darkolivegreen')

plt.title(f"Top {N} Palabras — Libro {libro_id}, Capítulo {capitulo_id}")
plt.xlabel("Palabra")
plt.ylabel("Frecuencia")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Histograma de frecuencia de palabras (Top N)"""

# =================================================================
# Histograma de frecuencia de palabras (Top N)
# =================================================================

from collections import Counter
import matplotlib.pyplot as plt

N = 50  # Cambia para mostrar más o menos palabras

# Aplanar palabras
palabras = [p for lista in df_norm['tokens_no_stop'] for p in lista]

# Frecuencias
frecuencias = Counter(palabras)
top_n = frecuencias.most_common(N)

# Solo los valores de frecuencia
valores = [freq for palabra, freq in top_n]

plt.figure(figsize=(12, 6))
plt.hist(valores, bins=10, color='steelblue', alpha=0.8)

plt.title(f"Histograma de Frecuencia — Top {N} Palabras")
plt.xlabel("Frecuencia")
plt.ylabel("Cantidad de Palabras")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Histograma de longitud de palabras
# =================================================================

# Longitud de cada palabra
longitudes_palabras = [len(p) for p in palabras]

plt.figure(figsize=(12, 5))
plt.hist(longitudes_palabras, bins=15, color='teal', alpha=0.8)

plt.title("Histograma de Longitud de Palabras")
plt.xlabel("Número de Letras por Palabra")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Histograma de longitud de versículos
# =================================================================

# Asegurar columna de conteo
df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

plt.figure(figsize=(12, 5))
plt.hist(df_norm['word_count'], bins=40, color='purple', alpha=0.8)

plt.title("Histograma de Longitud de Versículos")
plt.xlabel("Número de Palabras por Versículo")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""# Definir palabras clave y contar sus frecuencias en todo el corpus"""

# =================================================================
# Frecuencia total de palabras clave en todo el corpus
# =================================================================

palabras_clave = ["dios", "hombre", "tierra"]  # Puedes agregar más

# Todas las palabras del corpus
todas = [p.lower() for lista in df_norm['tokens_no_stop'] for p in lista]

frecuencia_global = {p: todas.count(p) for p in palabras_clave}

print(" Frecuencia global de palabras clave:")
for palabra, freq in frecuencia_global.items():
    print(f"{palabra}: {freq}")

# =================================================================
# Gráfico de frecuencia global de palabras clave
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.bar(frecuencia_global.keys(), frecuencia_global.values(), color=['purple', 'green', 'orange'])

plt.title("Frecuencia Global de Palabras Clave")
plt.xlabel("Palabra")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Frecuencia de palabras clave por libro
# =================================================================

frecuencia_por_libro = {}

for libro, grupo in df_norm.groupby('nrsva_book_index'):
    palabras = [p.lower() for lista in grupo['tokens_no_stop'] for p in lista]
    frecuencia_por_libro[libro] = {clave: palabras.count(clave) for clave in palabras_clave}

print(" Frecuencias por libro generadas.")
frecuencia_por_libro

# =================================================================
# Crear DataFrame: filas = libros, columnas = palabras clave
# =================================================================

import pandas as pd

df_frec_libro = pd.DataFrame.from_dict(frecuencia_por_libro, orient='index')
df_frec_libro.index.name = "Libro (NRSVA)"

display(df_frec_libro)

# =================================================================
# Gráfico comparativo por libro
# =================================================================

plt.figure(figsize=(14, 7))
df_frec_libro.plot(kind='bar', figsize=(14, 7))

plt.title("Frecuencia de Palabras Clave por Libro")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("Frecuencia")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Top N palabras con stopwords"""

# =================================================================
# Top N palabras CON stopwords
# =================================================================

from collections import Counter

N = 20  # Ajusta la cantidad de palabras a listar

# Aplanar todas las palabras (incluye stopwords)
palabras_con_stop = [p.lower() for lista in df_norm['tokens'] for p in lista]

frecuencias_con_stop = Counter(palabras_con_stop)
top_n_con_stop = frecuencias_con_stop.most_common(N)

print(f" Top {N} palabras CON stopwords:")
for palabra, freq in top_n_con_stop:
    print(f"{palabra}: {freq}")

# =================================================================
# Top N palabras SIN stopwords
# =================================================================

# Aplanar palabras útiles (sin stopwords)
palabras_sin_stop = [p.lower() for lista in df_norm['tokens_no_stop'] for p in lista]

frecuencias_sin_stop = Counter(palabras_sin_stop)
top_n_sin_stop = frecuencias_sin_stop.most_common(N)

print(f"\n Top {N} palabras SIN stopwords:")
for palabra, freq in top_n_sin_stop:
    print(f"{palabra}: {freq}")

# =================================================================
# Comparación en DataFrame
# =================================================================

import pandas as pd

df_con = pd.DataFrame(top_n_con_stop, columns=["Palabra_con", "Frecuencia_con"])
df_sin = pd.DataFrame(top_n_sin_stop, columns=["Palabra_sin", "Frecuencia_sin"])

display(df_con)
display(df_sin)

# =================================================================
# Gráficos comparativos lado a lado
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(16, 6))

# --- Gráfico 1: CON stopwords
plt.subplot(1, 2, 1)
plt.bar(df_con['Palabra_con'], df_con['Frecuencia_con'], color='crimson')
plt.title(f"Top {N} Palabras CON Stopwords")
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

# --- Gráfico 2: SIN stopwords
plt.subplot(1, 2, 2)
plt.bar(df_sin['Palabra_sin'], df_sin['Frecuencia_sin'], color='teal')
plt.title(f"Top {N} Palabras SIN Stopwords")
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.tight_layout()
plt.show()

"""### Calcular palabras totales por libro"""

# =================================================================
# Palabras totales por libro
# =================================================================

from collections import Counter

# Diccionario: libro → total de palabras
palabras_por_libro = {}

for libro, grupo in df_norm.groupby('nrsva_book_index'):
    palabras = [p for lista in grupo['tokens_no_stop'] for p in lista]
    palabras_por_libro[libro] = len(palabras)

print(" Palabras totales por libro calculadas correctamente.\n")
print(palabras_por_libro)

# =================================================================
# Tabla: palabras totales por libro
# =================================================================

import pandas as pd

df_palabras_por_libro = pd.DataFrame(
    list(palabras_por_libro.items()),
    columns=["Libro (NRSVA)", "Palabras Totales"]
).sort_values("Libro (NRSVA)").reset_index(drop=True)

display(df_palabras_por_libro)

# =================================================================
# Gráfico: Palabras totales por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.bar(df_palabras_por_libro["Libro (NRSVA)"],
        df_palabras_por_libro["Palabras Totales"],
        color='slateblue')

plt.title("Palabras Totales por Libro")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("Cantidad de Palabras")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""### Calcular palabras totales por capítulo"""

# =================================================================
# Palabras totales por capítulo
# =================================================================

palabras_por_capitulo = {}

for (libro, cap), grupo in df_norm.groupby(['nrsva_book_index', 'nrsva_chapter']):
    palabras = [p for lista in grupo['tokens_no_stop'] for p in lista]
    palabras_por_capitulo[(libro, cap)] = len(palabras)

print(" Palabras totales por capítulo calculadas correctamente.")
print("Ejemplo de los primeros elementos:")
list(palabras_por_capitulo.items())[:10]

# =================================================================
# DataFrame: Palabras totales por capítulo
# =================================================================

import pandas as pd

df_palabras_capitulo = pd.DataFrame(
    [(libro, cap, total) for (libro, cap), total in palabras_por_capitulo.items()],
    columns=["Libro (NRSVA)", "Capítulo", "Palabras Totales"]
).sort_values(["Libro (NRSVA)", "Capítulo"]).reset_index(drop=True)

display(df_palabras_capitulo)

# =================================================================
# Gráfico: Palabras totales por capítulo
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 7))
plt.plot(df_palabras_capitulo["Palabras Totales"], marker='o', linestyle='-', alpha=0.7)

plt.title("Palabras Totales por Capítulo (Ordenado por Libro y Capítulo)")
plt.xlabel("Capítulos (secuenciales)")
plt.ylabel("Total de Palabras")

plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

"""### Calcular promedio de palabras por versículo"""

# =================================================================
# Promedio de palabras por versículo
# =================================================================

# Crear la columna word_count (si no existe)
df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

# Calcular el promedio
promedio_palabras_versiculo = df_norm['word_count'].mean()

print(" Promedio de palabras por versículo:", round(promedio_palabras_versiculo, 2))

# =================================================================
# Histograma del número de palabras por versículo
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.hist(df_norm['word_count'], bins=40, color='mediumseagreen', alpha=0.8)

plt.title("Distribución del Número de Palabras por Versículo")
plt.xlabel("Número de Palabras")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Boxplot del número de palabras por versículo
# =================================================================

plt.figure(figsize=(8, 4))
plt.boxplot(df_norm['word_count'], vert=False)

plt.title("Boxplot: Palabras por Versículo")
plt.xlabel("Número de Palabras")
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.show()

"""### Asegurar columna word_count"""

# =================================================================
# Asegurar columna de conteo de palabras
# =================================================================

df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

"""### Obtener el versículo más largo por libro"""

# =================================================================
# Versículo más largo por libro
# =================================================================

versiculo_mas_largo_por_libro = (
    df_norm.loc[df_norm.groupby('nrsva_book_index')['word_count'].idxmax()]
    .sort_values('nrsva_book_index')
    .reset_index(drop=True)
)

display(versiculo_mas_largo_por_libro)

# =================================================================
# Versión resumida del versículo más largo por libro
# =================================================================

resumen = versiculo_mas_largo_por_libro[
    ['nrsva_book_index', 'nrsva_chapter', 'nrsva_verse', 'word_count']
]

display(resumen)

# =================================================================
# Gráfico: longitud del versículo más largo por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.bar(resumen['nrsva_book_index'], resumen['word_count'], color='darkorange')

plt.title("Versículo Más Largo por Libro (NRSVA)")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("Número de Palabras")

plt.xticks(resumen['nrsva_book_index'], rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Asegurar columna word_count"""

# =================================================================
# Asegurar columna de conteo de palabras
# =================================================================

df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

"""### Obtener el versículo más corto por libro"""

# =================================================================
# Versículo más corto por libro
# =================================================================

versiculo_mas_corto_por_libro = (
    df_norm.loc[df_norm.groupby('nrsva_book_index')['word_count'].idxmin()]
    .sort_values('nrsva_book_index')
    .reset_index(drop=True)
)

display(versiculo_mas_corto_por_libro)

# =================================================================
# Resumen del versículo más corto por libro
# =================================================================

resumen_corto = versiculo_mas_corto_por_libro[
    ['nrsva_book_index', 'nrsva_chapter', 'nrsva_verse', 'word_count']
]

display(resumen_corto)

# =================================================================
# Gráfico: Versículo más corto por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.bar(resumen_corto['nrsva_book_index'], resumen_corto['word_count'], color='forestgreen')

plt.title("Versículo Más Corto por Libro (NRSVA)")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("Número de Palabras")

plt.xticks(resumen_corto['nrsva_book_index'], rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Calcular palabras totales por libro"""

# =================================================================
# Palabras totales por libro
# =================================================================

palabras_por_libro = {}

for libro, grupo in df_norm.groupby('nrsva_book_index'):
    palabras = [p for lista in grupo['tokens_no_stop'] for p in lista]
    palabras_por_libro[libro] = len(palabras)

print(" Palabras totales calculadas correctamente.")

# =================================================================
# Libro más extenso y más breve
# =================================================================

# Convertir a DataFrame
import pandas as pd

df_palabras_por_libro = pd.DataFrame(
    list(palabras_por_libro.items()),
    columns=["Libro (NRSVA)", "Palabras Totales"]
)

# Ordenar de mayor a menor
df_ordenado = df_palabras_por_libro.sort_values("Palabras Totales", ascending=False)

libro_mas_extenso = df_ordenado.iloc[0]
libro_mas_breve = df_ordenado.iloc[-1]

print(" Libro más extenso:")
print(libro_mas_extenso)

print("\n Libro más breve:")
print(libro_mas_breve)

"""1. Qué muestra el gráfico

El gráfico ordena los libros bíblicos según su cantidad total de palabras (de mayor a menor). Esto permite identificar rápidamente cuáles son los libros más extensos y cuáles son los más breves dentro del corpus.

2. Libros extensos

Los primeros libros del gráfico superan ampliamente las 15.000 palabras.
Corresponden principalmente a:

* libros narrativos e históricos largos,

* y grandes libros proféticos.

Esto indica una alta densidad narrativa y doctrinal.

3. Libros medianos

El tramo intermedio (6.000–10.000 palabras) agrupa:

* libros de sabiduría,

* profecías medianas,

* narrativas más breves.

Representan el cuerpo central y más equilibrado del canon.

4. Libros breves

Los últimos libros varían entre 300 y 2.000 palabras:

* profetas menores,

* epístolas cortas,

* textos altamente condensados.

Su brevedad refleja un estilo directo y concentrado.

5. Conclusión general

El gráfico evidencia una Biblia heterogénea en extensión, donde:

* unos pocos libros aportan gran parte del volumen total,

* mientras muchos otros son breves pero conceptualmente densos.

Es una visión clara de la distribución macrotextual del corpus.
"""

# =================================================================
# Gráfico: Palabras totales por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.bar(df_ordenado["Libro (NRSVA)"], df_ordenado["Palabras Totales"], color='royalblue')

plt.title("Palabras Totales por Libro — Identificación de Libros Extensos y Breves")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("Palabras Totales")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""### Asegurar columna word_count"""

# =================================================================
# Asegurar columna de conteo de palabras por versículo
# =================================================================

df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

print("✔ Columna word_count creada correctamente.")

# =================================================================
# Densidad promedio de palabras por versículo
# =================================================================

densidad_promedio = df_norm['word_count'].mean()

print(" Densidad promedio de palabras por versículo:", round(densidad_promedio, 2))

# =================================================================
# Estadísticas descriptivas de la densidad
# =================================================================

print(df_norm['word_count'].describe())

# =================================================================
# Histograma de palabras por versículo
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.hist(df_norm['word_count'], bins=40, color='steelblue', alpha=0.8)

plt.title("Distribución de Palabras por Versículo")
plt.xlabel("Palabras por Versículo")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Boxplot de palabras por versículo
# =================================================================

plt.figure(figsize=(10, 4))
plt.boxplot(df_norm['word_count'], vert=False, notch=True)

plt.title("Boxplot — Palabras por Versículo")
plt.xlabel("Número de Palabras")
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.show()

"""### Asegurar columna de longitud de versículo"""

# =================================================================
# Asegurar columna de longitud de versículos
# =================================================================

df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)
print("✔ Columna word_count asegurada.")

"""### Cálculo de longitud promedio por libro"""

# =================================================================
# Longitud promedio de versículos por libro
# =================================================================

import pandas as pd

longitud_promedio_libro = (
    df_norm.groupby('nrsva_book_index')['word_count']
    .mean()
    .reset_index(name='Promedio_Palabras')
    .sort_values('nrsva_book_index')
)

display(longitud_promedio_libro)

"""1. Libros con versículos más largos

Algunos libros muestran medianas altas (≈12–15 palabras por versículo) y cajas más amplias, indicando:

* estilo narrativo o profético más desarrollado,

* versículos descriptivos, extensos y detallados.

Suelen ser libros históricos, proféticos o narrativos mayores.

2. Libros con versículos breves

Otros libros presentan medianas bajas (≈6–8 palabras), lo que refleja:

* estructura concisa,

* estilo directo,

* versículos cortos y repetitivos.

Este patrón es frecuente en libros como profetas menores, poesía breve, epístolas cortas o textos de estructura sentenciosa.

3. Variabilidad entre libros

El gráfico deja ver diferencias notables:

* Algunos libros tienen alta dispersión, lo que indica variación fuerte entre versículos muy breves y otros muy extensos.

* Otros muestran cajas compactas, lo que revela una estructura versicular uniforme y homogénea.

Esto ayuda a identificar estilos literarios: desde narrativa amplia hasta textos formales y estructurados.

4. Conclusión general

El boxplot evidencia que:

* la Biblia posee una diversidad estilística considerable,

* algunos libros mantienen una longitud versicular regular,

* otros alternan entre versos muy cortos y otros muy largos.

Este gráfico es clave para comprender el perfil lingüístico individual de cada libro y compararlos entre sí.
"""

# =================================================================
# Boxplot — Distribución de longitud de versículos por libro
# =================================================================

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(16, 7))
sns.boxplot(
    data=df_norm,
    x='nrsva_book_index',
    y='word_count',
    showfliers=False,     # para evitar ruido visual por outliers
    palette='viridis'
)

plt.title("Distribución de Longitud de Versículos por Libro (sin outliers)")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("Palabras por Versículo")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

# =================================================================
# Gráfico de violín — Longitud por libro
# =================================================================

plt.figure(figsize=(16, 7))
sns.violinplot(
    data=df_norm,
    x='nrsva_book_index',
    y='word_count',
    inner='quartile',
    palette='magma'
)

plt.title("Violín Plot: Distribución de Longitud de Versículos por Libro")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("Palabras por Versículo")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

# =================================================================
# Estadísticas descriptivas de longitud por libro
# =================================================================

estadisticas_libro = (
    df_norm.groupby('nrsva_book_index')['word_count']
    .describe()
    .round(2)
)

display(estadisticas_libro)

"""### Preparar tokens para N-grams"""

# =================================================================
# Preparar tokens para N-grams
# =================================================================

# Obtener todas las palabras útiles
tokens = [palabra for lista in df_norm['tokens_no_stop'] for palabra in lista]

print(f"✔ Total de palabras procesadas: {len(tokens)}")

"""### Función genérica para generar N-grams"""

# =================================================================
# Función para generar N-grams
# =================================================================

from collections import Counter

def generar_ngrams(tokens, n):
    ngrams = zip(*[tokens[i:] for i in range(n)])
    return Counter([" ".join(ngram) for ngram in ngrams])

"""### Bigramas (2-grams)"""

# =================================================================
# Bigramas
# =================================================================

bigrams = generar_ngrams(tokens, 2)
top_bigrams = bigrams.most_common(20)

print(" Top 20 BIGRAMAS:")
for bg, freq in top_bigrams:
    print(f"{bg}: {freq}")

"""### BLOQUE 4 — Trigramas (3-grams)"""

# =================================================================
# Trigramas
# =================================================================

trigrams = generar_ngrams(tokens, 3)
top_trigrams = trigrams.most_common(20)

print(" Top 20 TRIGRAMAS:")
for tg, freq in top_trigrams:
    print(f"{tg}: {freq}")

"""###BLOQUE 5 — 4-grams"""

# =================================================================
# 4-grams
# =================================================================

fourgrams = generar_ngrams(tokens, 4)
top_fourgrams = fourgrams.most_common(20)

print(" Top 20 4-GRAMS:")
for fg, freq in top_fourgrams:
    print(f"{fg}: {freq}")

"""###Convertir resultados a DataFrames"""

# =================================================================
# Convertir N-grams a DataFrames
# =================================================================

import pandas as pd

df_bigrams = pd.DataFrame(top_bigrams, columns=["Bigram", "Frecuencia"])
df_trigrams = pd.DataFrame(top_trigrams, columns=["Trigram", "Frecuencia"])
df_fourgrams = pd.DataFrame(top_fourgrams, columns=["4-gram", "Frecuencia"])

display(df_bigrams)
display(df_trigrams)
display(df_fourgrams)

"""###Instalar spaCy y el modelo en español"""

# =================================================================
# Instalación de spaCy y modelo de español
# =================================================================

!pip install -U spacy

# Descargar modelo pequeño de español
!python -m spacy download es_core_news_sm

"""### Cargar spaCy y el modelo de español"""

# =================================================================
# Carga de spaCy y modelo de español
# =================================================================

import spacy

# Cargar el modelo de español
nlp = spacy.load("es_core_news_sm")

print(" Modelo de spaCy en español cargado correctamente.")

"""### Función de lematización para una lista de tokens"""

# =================================================================
# Función para lematizar una lista de tokens
# =================================================================

def lematizar_lista(tokens):
    """
    Recibe una lista de tokens (palabras) y devuelve una lista de lemas.
    """
    # Unir tokens en un solo texto
    texto = " ".join(tokens)

    # Procesar con spaCy
    doc = nlp(texto)

    # Extraer lemas (evitando espacios y signos)
    lemas = [token.lemma_ for token in doc if token.is_alpha]

    return lemas

"""### Aplicar lematización al DataFrame"""

# =================================================================
# Aplicar lematización a cada versículo
# =================================================================

# Verificación básica
if 'tokens_no_stop' not in df_norm.columns:
    raise ValueError(" La columna 'tokens_no_stop' no existe. Asegúrate de haber ejecutado la normalización antes.")

# Aplicar a todo el DataFrame (puede tardar un poco)
df_norm['lemmas'] = df_norm['tokens_no_stop'].apply(lematizar_lista)

# Reconstruir el texto lematizado
df_norm['text_lemmatized'] = df_norm['lemmas'].apply(lambda lst: " ".join(lst))

print(" Lematización completada.")
print("\nEjemplo de la primera fila:\n")
print("Tokens sin stopwords :", df_norm['tokens_no_stop'].iloc[0][:20])
print("Lemas                 :", df_norm['lemmas'].iloc[0][:20])
print("Texto lematizado      :", df_norm['text_lemmatized'].iloc[0][:200])

"""### Top N lemas más frecuentes"""

# =================================================================
# Top N lemas más frecuentes
# =================================================================

from collections import Counter

N = 30

todos_los_lemas = [lemma for lista in df_norm['lemmas'] for lemma in lista]
frecuencias_lemas = Counter(todos_los_lemas)
top_lemas = frecuencias_lemas.most_common(N)

print(f" Top {N} lemas más frecuentes:")
for lem, freq in top_lemas:
    print(f"{lem}: {freq}")

"""#### Asegurar instalación y carga del modelo spaCy en español"""

# =================================================================
# Instalación de spaCy y modelo de español
# =================================================================

!pip install -U spacy
!python -m spacy download es_core_news_sm

"""### Cargar spaCy con POS-tagging"""

# =================================================================
# Cargar spaCy con modelo de español
# =================================================================

import spacy
nlp = spacy.load("es_core_news_sm")

print(" Modelo spaCy cargado con POS-tagging.")

"""### Función para etiquetar POS en una lista de tokens"""

# =================================================================
# Función que aplica POS-tagging a una lista de tokens
# =================================================================

def pos_tags(tokens):
    texto = " ".join(tokens)
    doc = nlp(texto)
    return [(token.text, token.lemma_, token.pos_) for token in doc if token.is_alpha]

"""###Aplicar POS-tagging a cada versículo"""

# =================================================================
# Aplicar POS-tagging a cada versículo
# =================================================================

df_norm['pos_tags'] = df_norm['tokens_no_stop'].apply(pos_tags)

print("✔ POS-tagging aplicado correctamente.")
print(df_norm['pos_tags'].iloc[0][:20])  # Mostrar ejemplo

"""### Conteo de verbos, sustantivos y adjetivos"""

# =================================================================
# Conteo global de verbos, sustantivos y adjetivos
# =================================================================

from collections import Counter

contador_pos = Counter()

for lista in df_norm['pos_tags']:
    for _, _, pos in lista:
        contador_pos[pos] += 1

verbos = contador_pos['VERB']
sustantivos = contador_pos['NOUN'] + contador_pos['PROPN']   # Sustantivo común + propio
adjetivos = contador_pos['ADJ']

print(" CONTEO GRAMATICAL GLOBAL")
print("-----------------------------")
print(f"Verbos (VERB):        {verbos}")
print(f"Sustantivos (NOUN+PROPN): {sustantivos}")
print(f"Adjetivos (ADJ):      {adjetivos}")

""" #### Tabla comparativa"""

# =================================================================
# Tabla con los conteos gramaticales
# =================================================================

import pandas as pd

df_pos_global = pd.DataFrame({
    "Categoría gramatical": ["Verbos", "Sustantivos", "Adjetivos"],
    "Cantidad": [verbos, sustantivos, adjetivos]
})

display(df_pos_global)

"""###Gráfico comparativo"""

# =================================================================
# Gráfico de barras comparativo POS
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.bar(["Verbos", "Sustantivos", "Adjetivos"],
        [verbos, sustantivos, adjetivos],
        color=['blue', 'green', 'purple'])

plt.title("Distribución de Categorías Gramaticales")
plt.ylabel("Cantidad")
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""### Vocabulario Unico"""

# =================================================================
# Vocabulario único a partir de tokens_no_stop
# =================================================================

# Aplanar todas las palabras útiles
todas_las_palabras = [pal for lista in df_norm['tokens_no_stop'] for pal in lista]

# Construir el vocabulario único
vocabulario_unico = sorted(set(todas_las_palabras))

print(" Tamaño del vocabulario único:", len(vocabulario_unico))
print("\nEjemplo de las primeras 50 palabras del vocabulario:")
print(vocabulario_unico[:50])

# =================================================================
# Diccionarios word2id e id2word
# =================================================================

word2id = {palabra: idx for idx, palabra in enumerate(vocabulario_unico)}
id2word = {idx: palabra for palabra, idx in word2id.items()}

print("✔ Diccionarios de vocabulario construidos.")
print("Ejemplo:")
for i in range(10):
    print(i, "->", id2word[i])

# =================================================================
# Vocabulario con frecuencia
# =================================================================

from collections import Counter
import pandas as pd

frecuencias_vocab = Counter(todas_las_palabras)

df_vocabulario = (
    pd.DataFrame(
        [(pal, freq, word2id[pal]) for pal, freq in frecuencias_vocab.items()],
        columns=["Palabra", "Frecuencia", "ID"]
    )
    .sort_values("Frecuencia", ascending=False)
    .reset_index(drop=True)
)

display(df_vocabulario.head(30))
print("\n✔ Vocabulario con frecuencia construido. Filas totales:", len(df_vocabulario))

# =================================================================
# Guardar vocabulario a CSV
# =================================================================

df_vocabulario.to_csv("vocabulario_unico_con_frecuencia.csv", index=False, encoding="utf-8")
print(" Archivo 'vocabulario_unico_con_frecuencia.csv' guardado en el entorno de trabajo.")

# =================================================================
# Vocabulario único basado en lemas (opcional)
# =================================================================

todas_los_lemas = [lem for lista in df_norm['lemmas'] for lem in lista]
vocabulario_lemas = sorted(set(todas_los_lemas))

print(" Tamaño del vocabulario único (lemas):", len(vocabulario_lemas))
print(vocabulario_lemas[:50])

"""###Construir texto limpio por libro"""

# =================================================================
# Construir corpus por libro
# =================================================================

import pandas as pd

# Convertir tokens sin stopwords a texto
df_norm['text_no_stop'] = df_norm['tokens_no_stop'].apply(lambda x: " ".join(x))

# Agrupar textos por libro
corpus_por_libro = (
    df_norm.groupby('nrsva_book_index')['text_no_stop']
    .apply(lambda textos: " ".join(textos))
)

print(" Total de libros procesados:", len(corpus_por_libro))
corpus_por_libro.head()

"""###Vectorizar usando TF-IDF"""

# =================================================================
# Vectorización TF-IDF por libro
# =================================================================

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    lowercase=True,
    analyzer='word',
    token_pattern=r'\b[a-záéíóúñ]+\b',
    min_df=5,      # ignora palabras que aparecen menos de 5 veces
)

tfidf_matrix = vectorizer.fit_transform(corpus_por_libro)

print(" Matriz TF-IDF creada")
print("Dimensiones:", tfidf_matrix.shape)

"""###Función para obtener Top N palabras características por libro"""

# =================================================================
# Función: Top N palabras características (keyness)
# =================================================================

import numpy as np

feature_names = vectorizer.get_feature_names_out()

def palabras_caracteristicas(libro_id, N=20):
    """
    Obtiene las N palabras con mayor puntuación TF-IDF para un libro.
    """
    idx = list(corpus_por_libro.index).index(libro_id)
    fila = tfidf_matrix[idx].toarray().flatten()
    top_indices = fila.argsort()[-N:][::-1]

    return [(feature_names[i], fila[i]) for i in top_indices]

# =================================================================
# Ejemplo: palabras características del primer libro del corpus
# =================================================================

primer_libro = corpus_por_libro.index[0]

print(f" Palabras más características para el libro {primer_libro}:\n")

for palabra, score in palabras_caracteristicas(primer_libro, N=20):
    print(f"{palabra}: {score:.4f}")

# =================================================================
# Tabla de palabras características para un libro
# =================================================================

libro_id = primer_libro  # cámbialo por cualquier índice NRSVA

data = palabras_caracteristicas(libro_id, N=30)
df_keyness = pd.DataFrame(data, columns=["Palabra", "TF-IDF"])

display(df_keyness)

# =================================================================
# Gráfico TF-IDF para palabras más distintivas
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.bar(df_keyness["Palabra"], df_keyness["TF-IDF"], color='darkred')

plt.title(f"Palabras más características del libro {libro_id}")
plt.xlabel("Palabra")
plt.ylabel("TF-IDF")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""###Preprocesamiento: asegurar columnas"""

# =================================================================
# Asegurar columnas necesarias
# =================================================================

df_norm['text_no_stop'] = df_norm['tokens_no_stop'].apply(lambda x: " ".join(x))

print(" Columnas aseguradas: tokens_no_stop, text_no_stop")

# =================================================================
# Filtrar versículos donde aparece un personaje
# =================================================================

def versiculos_personaje(df, nombre):
    """
    Devuelve todos los versículos donde aparece el 'nombre' como palabra exacta.
    """
    nombre = nombre.lower()
    mask = df['tokens_no_stop'].apply(lambda lista: nombre in [p.lower() for p in lista])
    return df[mask]

# =================================================================
# Construir corpus por personaje
# =================================================================

def corpus_de_personaje(df, nombre):
    df_p = versiculos_personaje(df, nombre)
    texto = " ".join(df_p['text_no_stop'])
    return texto

# =================================================================
# Lista de personajes (puedes agregar los que quieras)
# =================================================================

personajes = ["dios", "adan", "noe", "abraham", "jacob"]

# =================================================================
# Corpus por personaje
# =================================================================

corpus_personajes = {p: corpus_de_personaje(df_norm, p) for p in personajes}

print(" Corpus por personaje creado.")

# =================================================================
# Matriz TF-IDF para personajes
# =================================================================

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer_pers = TfidfVectorizer(
    lowercase=True,
    analyzer="word",
    token_pattern=r'\b[a-záéíóúñ]+\b',
    min_df=2
)

tfidf_pers = vectorizer_pers.fit_transform(corpus_personajes.values())
feature_names_pers = vectorizer_pers.get_feature_names_out()

print(" Matriz TF-IDF de personajes creada.")
print(tfidf_pers.shape)

# =================================================================
# Palabras clave por personaje
# =================================================================

import numpy as np
import pandas as pd

def palabras_caracteristicas_personaje(nombre, N=20):
    """
    Devuelve las N palabras más características del personaje 'nombre'
    según TF-IDF.
    """
    idx = list(corpus_personajes.keys()).index(nombre)
    fila = tfidf_pers[idx].toarray().flatten()
    indices = fila.argsort()[-N:][::-1]
    return [(feature_names_pers[i], fila[i]) for i in indices]

# =================================================================
# Ejemplo: palabras características de 'noe'
# =================================================================

carac_noe = palabras_caracteristicas_personaje("noe", N=20)

print(" Palabras más características de NOÉ:\n")
for palabra, score in carac_noe:
    print(f"{palabra}: {score:.4f}")

"""###Calcular el TTR global del corpus"""

# =================================================================
# TTR (Type/Token Ratio) global del corpus
# =================================================================

# Aplanar todas las palabras útiles
tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

# Tipos (palabras únicas)
tipos = set(tokens)

ttr = len(tipos) / len(tokens)

print(" Número total de tokens:", len(tokens))
print(" Número total de tipos (vocabulario único):", len(tipos))
print(f" TTR (Type/Token Ratio) global: {ttr:.4f}")

# =================================================================
# TTR por libro
# =================================================================

import pandas as pd

ttr_por_libro = []

for libro, grupo in df_norm.groupby('nrsva_book_index'):
    tokens_libro = [p for lista in grupo['tokens_no_stop'] for p in lista]
    tipos_libro = set(tokens_libro)
    ttr_val = len(tipos_libro) / len(tokens_libro)
    ttr_por_libro.append((libro, ttr_val))

df_ttr_libro = pd.DataFrame(ttr_por_libro, columns=["Libro (NRSVA)", "TTR"])
display(df_ttr_libro)

# =================================================================
# Gráfico: TTR por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.plot(df_ttr_libro["Libro (NRSVA)"], df_ttr_libro["TTR"], marker='o', linestyle='-', color='darkblue')

plt.title("Índice de Riqueza Léxica (TTR) por Libro")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("TTR")

plt.xticks(rotation=45)
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# TTR usando lemas
# =================================================================

if 'lemmas' in df_norm.columns:
    lemas = [l for lista in df_norm['lemmas'] for l in lista]
    tipos_lemmas = set(lemas)
    ttr_lemmas = len(tipos_lemmas) / len(lemas)

    print(" TTR basado en lemas:", round(ttr_lemmas, 4))
else:
    print(" No existe la columna 'lemmas'. Ejecuta la lematización primero.")

"""### Aplanar tokens"""

# =================================================================
# Aplanar todas las palabras del corpus
# =================================================================

tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

print("✔ Tokens cargados:", len(tokens))

"""###Calcular frecuencias globales"""

# =================================================================
# Frecuencias globales de cada palabra
# =================================================================

from collections import Counter

frecuencias = Counter(tokens)

print(" Total de tipos únicos:", len(frecuencias))

# =================================================================
# Hapax Legomena
# =================================================================

hapax_legomena = [pal for pal, freq in frecuencias.items() if freq == 1]

print(" Total de Hapax Legomena:", len(hapax_legomena))
print("Ejemplos:", hapax_legomena[:20])

# =================================================================
# Hapax Dislegomena
# =================================================================

hapax_dislegomena = [pal for pal, freq in frecuencias.items() if freq == 2]

print(" Total de Hapax Dislegomena:", len(hapax_dislegomena))
print("Ejemplos:", hapax_dislegomena[:20])

# =================================================================
# Tabla resumen Hapax Legomena / Dislegomena
# =================================================================

import pandas as pd

df_hapax = pd.DataFrame({
    "Métrica": ["Total Tokens", "Tipos únicos", "Hapax Legomena", "Hapax Dislegomena"],
    "Valor": [len(tokens), len(frecuencias), len(hapax_legomena), len(hapax_dislegomena)]
})

display(df_hapax)

# =================================================================
# Porcentaje de hapax
# =================================================================

pct_hapax = len(hapax_legomena) / len(frecuencias) * 100
pct_dis = len(hapax_dislegomena) / len(frecuencias) * 100

print(f" Porcentaje Hapax Legomena: {pct_hapax:.2f}%")
print(f" Porcentaje Hapax Dislegomena: {pct_dis:.2f}%")

"""### Preparar tokens por libro"""

# =================================================================
# Preparar tokens por libro
# =================================================================

tokens_por_libro = {}

for libro, grupo in df_norm.groupby("nrsva_book_index"):
    tokens = [p for lista in grupo["tokens_no_stop"] for p in lista]
    tokens_por_libro[libro] = tokens

print(f" Libros procesados: {len(tokens_por_libro)}")

# =================================================================
# Diversidad léxica (TTR) por libro
# =================================================================

import pandas as pd

diversidad = []  # lista de filas

for libro, tokens in tokens_por_libro.items():
    tipos = set(tokens)
    ttr = len(tipos) / len(tokens)
    diversidad.append([libro, len(tokens), len(tipos), ttr])

df_lexico_libro = pd.DataFrame(diversidad, columns=[
    "Libro (NRSVA)", "Tokens Totales", "Tipos Únicos", "TTR"
]).sort_values("Libro (NRSVA)").reset_index(drop=True)

display(df_lexico_libro)

# =================================================================
# Gráfico: Diversidad léxica (TTR) por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.plot(df_lexico_libro["Libro (NRSVA)"], df_lexico_libro["TTR"],
         marker="o", linestyle="-", color="darkgreen")

plt.title("Índice de Diversidad Léxica (TTR) por Libro")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("TTR (Type/Token Ratio)")

plt.grid(True, linestyle="--", alpha=0.5)
plt.xticks(rotation=45)
plt.show()

# =================================================================
# Gráfico: Tipos y tokens por libro
# =================================================================

plt.figure(figsize=(14, 6))
plt.plot(df_lexico_libro["Libro (NRSVA)"], df_lexico_libro["Tokens Totales"],
         label="Tokens", color="steelblue")
plt.plot(df_lexico_libro["Libro (NRSVA)"], df_lexico_libro["Tipos Únicos"],
         label="Tipos", color="orange")

plt.title("Tokens vs Tipos Únicos por Libro")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("Cantidad")

plt.legend()
plt.grid(True, linestyle="--", alpha=0.5)
plt.xticks(rotation=45)
plt.show()

"""###Aplanar tokens y contar frecuencias"""

# =================================================================
# Aplanar tokens y calcular frecuencias
# =================================================================

from collections import Counter

tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]
frecuencias = Counter(tokens)

print(" Tokens totales:", len(tokens))
print(" Tipos únicos:", len(frecuencias))

"""### Detectar palabras raras (freq ≤ 3)"""

# =================================================================
# Palabras raras: frecuencia ≤ 3
# =================================================================

palabras_raras = [pal for pal, freq in frecuencias.items() if freq <= 3]

print("🔹 Palabras raras (freq ≤ 3):", len(palabras_raras))
print("Ejemplos:", palabras_raras[:20])

"""###Detectar palabras comunes (percentil 95)"""

# =================================================================
# Palabras comunes: percentil 95
# =================================================================

import numpy as np

valores_frec = np.array(list(frecuencias.values()))
umbral_95 = np.percentile(valores_frec, 95)

palabras_comunes = [pal for pal, freq in frecuencias.items() if freq >= umbral_95]

print(" Palabras comunes (percentil 95):", len(palabras_comunes))
print("Umbral de frecuencia (p95):", umbral_95)
print("Ejemplos:", palabras_comunes[:20])

"""###Resumen numérico"""

# =================================================================
# Resumen de palabras raras y comunes
# =================================================================

import pandas as pd

df_resumen = pd.DataFrame({
    "Categoría": ["Tokens totales", "Tipos únicos",
                  "Palabras raras (≤3)", "Palabras comunes (p95)"],
    "Valor": [len(tokens), len(frecuencias),
              len(palabras_raras), len(palabras_comunes)]
})

display(df_resumen)

"""###Gráfico: palabras raras vs comunes"""

# =================================================================
# Gráfico comparativo
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.bar(["Raras (≤3)", "Comunes (p95)"],
        [len(palabras_raras), len(palabras_comunes)],
        color=["purple", "darkorange"])

plt.title("Comparación de Palabras Raras vs Comunes")
plt.ylabel("Cantidad")
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""### Preparar tokens del corpus"""

# =================================================================
# Preparar tokens del corpus
# =================================================================

tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

print(" Tokens cargados:", len(tokens))

"""###Construir distribución de probabilidad"""

# =================================================================
# Frecuencias y probabilidades
# =================================================================

from collections import Counter
import numpy as np

frecuencias = Counter(tokens)
total_tokens = len(tokens)

# Probabilidades
probs = np.array([freq / total_tokens for freq in frecuencias.values()])

# =================================================================
# Entropía del texto
# =================================================================

entropia = -np.sum(probs * np.log2(probs))

print(f" Entropía total del texto (bits): {entropia:.4f}")

# =================================================================
# Entropía por libro
# =================================================================

import pandas as pd

entropia_libro = []

for libro, grupo in df_norm.groupby("nrsva_book_index"):
    tokens_l = [p for lista in grupo["tokens_no_stop"] for p in lista]
    frecs_l = Counter(tokens_l)
    total_l = len(tokens_l)
    probs_l = np.array([f / total_l for f in frecs_l.values()])

    H_l = -np.sum(probs_l * np.log2(probs_l))
    entropia_libro.append([libro, H_l])

df_entropia = pd.DataFrame(entropia_libro, columns=["Libro (NRSVA)", "Entropía"])
display(df_entropia)

# =================================================================
# Gráfico: Entropía por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.plot(df_entropia["Libro (NRSVA)"], df_entropia["Entropía"],
         marker="o", linestyle="-", color="darkred")

plt.title("Entropía Léxica por Libro")
plt.xlabel("Libro (Índice NRSVA)")
plt.ylabel("Entropía (bits)")

plt.grid(True, linestyle="--", alpha=0.5)
plt.xticks(rotation=45)
plt.show()

"""### Asegurar columna word_count (si no existe)"""

# =================================================================
# Crear word_count si aún no existe
# =================================================================

if 'word_count' not in df_norm.columns:
    df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

print(" Columna word_count creada / verificada.")

"""### Verificar / crear word_count"""

# =================================================================
# Crear word_count si aún no existe
# =================================================================

if 'word_count' not in df_norm.columns:
    df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

print("✔ word_count creado/verificado.")
df_norm['word_count'].head()

# =================================================================
# Histograma de longitudes de versículos (en palabras)
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.hist(df_norm['word_count'], bins=40, color='steelblue', alpha=0.8)

plt.title("Distribución de Longitudes de Versículos")
plt.xlabel("Número de palabras por versículo")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Contar palabras más frecuentes
# =================================================================

from collections import Counter

# Aplanar todos los tokens
tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

# Contar frecuencias
frecuencias = Counter(tokens)

# Seleccionar las N más frecuentes
N = 30
top_palabras = frecuencias.most_common(N)

top_palabras[:10]

# =================================================================
# Convertir top N palabras a DataFrame
# =================================================================

import pandas as pd

df_top = pd.DataFrame(top_palabras, columns=["Palabra", "Frecuencia"])
df_top

# =================================================================
# Gráfico de barras de las palabras más frecuentes
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.bar(df_top["Palabra"], df_top["Frecuencia"], color="purple", alpha=0.8)

plt.title(f"Top {N} Palabras Más Frecuentes")
plt.xlabel("Palabra")
plt.ylabel("Frecuencia")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.show()

# =================================================================
# Seleccionar top N palabras más frecuentes del corpus
# =================================================================

from collections import Counter

# Aplanar tokens
tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

# Calcular frecuencias
frecuencias = Counter(tokens)

# Elegir top N palabras
N = 30
top_words = [pal for pal, freq in frecuencias.most_common(N)]

print("✔ Palabras seleccionadas:", top_words[:10], "...")

"""### Contar frecuencias por capítulo"""

# =================================================================
# Construir matriz libro-capítulo vs palabra
# =================================================================

import pandas as pd

# Crear DataFrame vacío
df_heat = pd.DataFrame(columns=["Libro", "Capítulo"] + top_words)

# Agrupar por libro y capítulo
grupos = df_norm.groupby(["nrsva_book_index", "nrsva_chapter"])

rows = []

for (libro, cap), grupo in grupos:
    tokens_cap = [p for lista in grupo['tokens_no_stop'] for p in lista]
    frecs_cap = Counter(tokens_cap)

    fila = {"Libro": libro, "Capítulo": cap}
    fila.update({w: frecs_cap.get(w, 0) for w in top_words})

    rows.append(fila)

df_heat = pd.DataFrame(rows)

print("✔ Matriz creada con forma:", df_heat.shape)
df_heat.head()

# =================================================================
# Crear índice capítulo absoluto (libro.capítulo)
# =================================================================

df_heat["LibroCap"] = df_heat["Libro"].astype(str) + "." + df_heat["Capítulo"].astype(str)
df_heat = df_heat.sort_values(["Libro", "Capítulo"]).reset_index(drop=True)

df_heat.index = df_heat["LibroCap"]
df_heat_num = df_heat[top_words]

print("✔ Índice LibroCap generado.")
df_heat_num.head()

# =================================================================
# Instalar librería WordCloud
# =================================================================

!pip install wordcloud

# =================================================================
# Construir texto para WordCloud
# =================================================================

# Aplanar todos los tokens limpios
tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

# Unir todo como texto
texto_wordcloud = " ".join(tokens)

print("✔ Texto para WordCloud construido.")

"""#WordCloud

El WordCloud representa las palabras más frecuentes de todo el corpus bíblico (después de limpiar y normalizar el texto).

El tamaño de cada palabra indica su frecuencia relativa: cuanto más grande, más veces aparece.

Los colores solo ayudan a diferenciar visualmente las palabras; no codifican una métrica adicional.

# Palabras dominantes y su significado general

Las palabras más grandes son, entre otras:
“dios”, “señor”, “pueblo”, “tierra”, “hijo”, “padre”, “hombre”, “casa”, “día”, “corazón”, “Israel”.

Esto sugiere que el corpus está fuertemente organizado en torno a:

* Relación divina: “dios”, “señor”.

* Relación comunitaria: “pueblo”, “Israel”, “ciudad”, “casa”.

* Relación familiar y humana: “hijo”, “padre”, “hombre”, “hermano”, “mujer”.

* Dimensión existencial y espiritual: “corazón”, “pecado”, “verdad”, “obra”.

* Espacio y tiempo: “tierra”, “día”, “lugar”.

En conjunto, el WordCloud confirma que el discurso bíblico combina de forma constante Dios, comunidad, familia, territorio y experiencia humana, lo que coincide con la estructura temática central del texto.
"""

# =================================================================
# Generar WordCloud
# =================================================================

from wordcloud import WordCloud
import matplotlib.pyplot as plt

wc = WordCloud(
    width=1600,
    height=900,
    background_color="white",
    max_words=300,
    colormap="viridis"
).generate(texto_wordcloud)

plt.figure(figsize=(16, 9))
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.title("WordCloud - Palabras Más Frecuentes (Corpus Completo)", fontsize=18)
plt.show()

# =================================================================
# WordCloud por libro (versión corregida y robusta)
# =================================================================

def wordcloud_por_libro(libro_id):
    # Filtrar el libro
    grupo = df_norm[df_norm["nrsva_book_index"] == libro_id]

    # Validar si existe ese libro
    if grupo.empty:
        print(f" No existe ningún libro con índice {libro_id}.")
        print(" Usa df_norm['nrsva_book_index'].unique() para ver los índices disponibles.")
        return

    # Extraer tokens
    tokens_libro = [p for lista in grupo['tokens_no_stop'] for p in lista]

    # Validar si el libro tiene palabras
    if len(tokens_libro) == 0:
        print(f"⚠ El libro con índice {libro_id} no tiene palabras procesables.")
        return

    # Crear texto
    texto = " ".join(tokens_libro)

    # Generar WordCloud
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    wc = WordCloud(
        width=1600,
        height=900,
        background_color="white",
        colormap="plasma",
        max_words=200
    ).generate(texto)

    plt.figure(figsize=(16, 9))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"WordCloud - Libro {libro_id}", fontsize=18)
    plt.show()

# =================================================================
# Ver los índices reales de libros
# =================================================================

df_norm['nrsva_book_index'].unique()

# =================================================================
# Construir y contar bigramas a partir de tokens_no_stop
# =================================================================

from collections import Counter

def generar_bigramas(lista_tokens):
    """
    Recibe una lista de tokens y devuelve una lista de bigramas como tuplas (w1, w2)
    """
    return list(zip(lista_tokens[:-1], lista_tokens[1:]))

# Aplanar todos los tokens del corpus
tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

# Generar todos los bigramas
bigrams = generar_bigramas(tokens)

# Contar frecuencias de bigramas
bigram_freq = Counter(bigrams)

print(" Total de bigramas distintos:", len(bigram_freq))
list(bigram_freq.items())[:10]

# =================================================================
# Seleccionar TOP K bigramas más frecuentes
# =================================================================

K = 50  # puedes ajustar este valor

top_bigrams = bigram_freq.most_common(K)

print(f" Seleccionados los {K} bigramas más frecuentes:")
for par, freq in top_bigrams[:10]:
    print(f"{par} -> {freq}")

# =================================================================
# Construir grafo de co-ocurrencia de bigramas
# =================================================================

!pip install networkx

import networkx as nx

G = nx.Graph()

# Añadir nodos y aristas con peso
for (w1, w2), freq in top_bigrams:
    G.add_node(w1)
    G.add_node(w2)
    G.add_edge(w1, w2, weight=freq)

print(" Nodos en el grafo:", G.number_of_nodes())
print(" Aristas en el grafo:", G.number_of_edges())

"""1. Selección de bigramas y construcción del grafo

Se identificaron los 50 bigramas más frecuentes del texto normalizado (tokens sin stopwords).
Cada bigrama representa dos palabras que aparecen juntas de manera consecutiva dentro de un versículo.
Estas relaciones se transformaron en un grafo donde:

* cada nodo es una palabra,

* cada arista representa una co-ocurrencia fuerte,

* el grosor de la arista indica la cantidad de veces que ese par aparece en el texto.

Este tipo de grafo permite visualizar vínculos léxicos locales, es decir, asociaciones semánticas que emergen directamente del flujo narrativo.


2. Organización del grafo

El grafo se representa en una distribución por fuerzas (force layout), generando agrupamientos naturales:

* palabras con más conexiones aparecen hacia el centro,

* palabras marginales forman pequeñas islas en los bordes,

* los clusters representan temas recurrentes del texto.

3. Interpretación del grafo de bigramas
* A. Núcleo central del grafo — Temas dominantes

El centro del grafo agrupa palabras estrechamente relacionadas, como:

* * pueblo, israel, hijos, hijas, padre,

* * jehová/señor/dios (dependiendo de la versión),

* * cristo, jesús, siervo,

* * palabra, vino, entonces.

Este núcleo revela que:

* * la narrativa bíblica presenta una fuerte estructura en torno a relaciones familiares,

* * comunidades identitarias (pueblo–israel),

* * vínculos religiosos (señor–siervo, cristo–jesús),

* * secuencias de acción (vino la palabra, entonces dijo).

Estas asociaciones muestran el andamiaje narrativo y teológico fundamental del texto.

*  B. Clusters temáticos claramente separados

El grafo muestra varios grupos aislados que representan temas específicos:

1. Grupo “hombre–hijo”

Un cluster pequeño pero fuerte, que muestra relaciones familiares y antropológicas.

2. Grupo “tiempo–año–día”

Indica asociaciones temporales frecuentes, típicas de narraciones estructuradas cronológicamente.

3. Grupo “siete–días”

Conexión que sugiere:

* * referencias simbólicas (número 7),

* * ciclos litúrgicos o rituales,

* * textos de carácter legal o ceremonial.

4. Grupo “tabernáculo–testimonio”

Relacionado con textos del Pentateuco, especialmente Éxodo.

5. Grupo “egipto–congregación–tierra”

Asociado a temas de:

* * éxodo,

* * peregrinación,

* * identidad tribal del pueblo de Israel.

6. Grupo “rey–judá–babilonia”

Claramente vinculado a narraciones históricas y proféticas:

* * monarquía israelita,

* * deportación,

* * conflictos geopolíticos.



* C. Palabras periféricas — Ideas cristalizadas en contextos específicos

En los bordes del grafo aparecen pares con poca conexión al resto de conceptos, como:

* “pues–ahora”,

* “cosas–estas”,

* “manera–gran”,

* “espíritu–santo” (tema teológico específico),

* “días–siete”,

* “hombre–ninguno”.

Estas islas demuestran que ciertos conceptos aparecen solo en contextos particulares, reflejando:

* estilo narrativo diverso,

* uso temático muy localizado,

* presencia de secciones doctrinales, proféticas o legales.


4. Lectura global del grafo de co-ocurrencia

El grafo revela que:

* El texto tiene una estructura semántica altamente modular, con clusters que representan diferentes secciones bíblicas (históricas, proféticas, narrativas, litúrgicas).
* Las palabras relacionadas con identidad del pueblo (Israel) y relación divina (Dios/Señor) forman el núcleo más fuerte.
* Los clusters periféricos representan temas específicos, frecuentemente aislados: rituales, genealogías, secuencias temporales, leyes o eventos históricos.
* El grafo confirma que el lenguaje bíblico combina:

* * narración,

* * teología,

* * historia,

* * ritualidad,

* * simbolismo.

5. Conclusión

El grafo de bigramas expone la estructura semántica profunda del texto:

* identifica los centros temáticos dominantes,

* separa los conjuntos lexicológicos que corresponden a diferentes géneros y secciones de la Biblia,

* y permite visualizar cómo las palabras se relacionan directamente dentro de la secuencia narrativa.

Es una herramienta fundamental para estudios de:

* análisis temático,

* estilometría,

* lingüística bíblica,

* modelado semántico automatizado,

* o extracción de tópicos.
"""

# =================================================================
# Graficar el grafo de bigramas
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 10))

# Posiciones de los nodos
pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)

# Pesos de las aristas (para grosor)
weights = [G[u][v]['weight'] for u, v in G.edges()]

# Dibujar nodos
nx.draw_networkx_nodes(G, pos, node_size=400, node_color='lightblue')

# Dibujar aristas (más grosor = más frecuencia)
nx.draw_networkx_edges(
    G, pos,
    width=[w / max(weights) * 5 for w in weights],  # normalizar grosor
    alpha=0.6
)

# Etiquetas de los nodos (palabras)
nx.draw_networkx_labels(G, pos, font_size=10)

plt.title(f"Grafo de Co-ocurrencia de Palabras (Top {K} Bigramas)")
plt.axis("off")
plt.show()

# =================================================================
# Grafo de bigramas filtrado por una palabra clave
# =================================================================

palabra_clave = "dios"

# Filtrar solo bigramas donde aparece la palabra clave
top_bigrams_filtrados = [
    ((w1, w2), freq)
    for (w1, w2), freq in top_bigrams
    if w1 == palabra_clave or w2 == palabra_clave
]

G_key = nx.Graph()

for (w1, w2), freq in top_bigrams_filtrados:
    G_key.add_node(w1)
    G_key.add_node(w2)
    G_key.add_edge(w1, w2, weight=freq)

plt.figure(figsize=(10, 7))

pos = nx.spring_layout(G_key, k=0.7, iterations=50, seed=42)
weights = [G_key[u][v]['weight'] for u, v in G_key.edges()]

nx.draw_networkx_nodes(G_key, pos, node_size=600, node_color='lightgreen')
nx.draw_networkx_edges(
    G_key, pos,
    width=[w / max(weights) * 6 for w in weights],
    alpha=0.7
)
nx.draw_networkx_labels(G_key, pos, font_size=11)

plt.title(f"Grafo de Bigramas alrededor de '{palabra_clave}'")
plt.axis("off")
plt.show()

# =================================================================
# Frecuencia de una palabra a lo largo del texto (por capítulo)
# =================================================================

import pandas as pd

def tendencia_palabra(palabra):
    palabra = palabra.lower()

    frecuencias = []
    for (libro, cap), grupo in df_norm.groupby(["nrsva_book_index", "nrsva_chapter"]):
        tokens = [p.lower() for lista in grupo["tokens_no_stop"] for p in lista]
        freq = tokens.count(palabra)
        frecuencias.append([f"{libro}.{cap}", freq])

    df_tend = pd.DataFrame(frecuencias, columns=["LibroCap", "Frecuencia"])
    return df_tend

# =================================================================
# Graficar tendencia de una palabra
# =================================================================

import matplotlib.pyplot as plt

palabra = "dios"
df_tend = tendencia_palabra(palabra)

plt.figure(figsize=(16,4))
plt.plot(df_tend["LibroCap"], df_tend["Frecuencia"], color="purple")

plt.title(f"Tendencia temporal de la palabra: {palabra}")
plt.xticks([], [])  # demasiados capítulos para mostrar etiquetas
plt.ylabel("Frecuencia por capítulo")
plt.grid(True, linestyle="--", alpha=0.4)
plt.show()

# =================================================================
# Correlación entre palabras frecuentes
# =================================================================

from collections import Counter
import numpy as np

# Top 50 palabras para análisis
top_words = [w for w, _ in Counter(tokens).most_common(50)]

# Matriz capítulos vs palabras
data = []

for (libro, cap), grupo in df_norm.groupby(["nrsva_book_index", "nrsva_chapter"]):
    tokens_c = [p for lista in grupo['tokens_no_stop'] for p in lista]
    frecs = Counter(tokens_c)
    data.append([frecs.get(w, 0) for w in top_words])

import pandas as pd
df_mat = pd.DataFrame(data, columns=top_words)

corr = df_mat.corr()

"""1. Selección de palabras frecuentes y construcción de la matriz de correlaciones

Se seleccionaron las palabras con mayor frecuencia absoluta en todo el corpus después del proceso de normalización (tokenización, limpieza y eliminación de stopwords).
Para cada palabra se construyó un vector binario o de conteo por versículo, permitiendo medir cómo varía su presencia a lo largo del texto.

A partir de estos vectores se calculó una matriz de correlaciones, donde cada celda representa el grado en que dos palabras tienden a aparecer juntas (correlación positiva) o de manera inversa (correlación negativa).

2. Generación del heatmap

La matriz resultante se visualiza mediante un mapa de calor:

* Las palabras se ubican tanto en filas como en columnas.

* La diagonal principal muestra correlación perfecta (=1), ya que cada palabra está correlacionada consigo misma.

* Los colores rojizos indican correlaciones positivas más altas.

* Los tonos azulados indican correlaciones negativas o muy bajas.

* Los tonos suaves intermedios representan correlaciones débiles o casi nulas.

Este tipo de visualización permite identificar asociaciones semánticas y patrones de coocurrencia en el texto bíblico.

3. Interpretación del heatmap de correlación
* Predominio de correlaciones débiles

La gran mayoría de las celdas muestran colores en rangos rosados claros, lo que indica que la mayor parte de las palabras frecuentes tienen correlaciones bajas entre sí.
Esto es esperable en textos amplios y variados, donde el vocabulario se distribuye en múltiples contextos distintos.

* Palabras con correlaciones moderadas

Aun cuando no existen correlaciones extremadamente fuertes (más allá de la diagonal), se observan algunos tonos rojizos dispersos. Estos sugieren patrones de coocurrencia entre ciertos términos, por ejemplo:

* * “dijo” con nombres propios (sugerencia de diálogos o narrativas directas).

* * “pueblo”, “israel”, “judá” con correlaciones entre sí (términos relacionados a identidad colectiva).

* * “dios”, “señor” con palabras asociadas al discurso religioso.

Estas correlaciones indican vínculos temáticos claros dentro de la estructura narrativa bíblica.

* Palabras con comportamientos más aislados

Algunas palabras muestran correlación casi nula con la mayoría del resto, reflejando usos muy específicos o contextuales:

* * nombres propios muy particulares,

* * términos relacionados a eventos concretos,

* * palabras descriptivas con distribución dispersa.

Su representación en tonos muy claros o azulados confirma una presencia menos alineada con el vocabulario dominante.

* Lectura semántica general del heatmap

El patrón global evidencia que:

* El texto posee alta diversidad semántica,

* La correlación entre palabras no se concentra en grupos específicos fuertes (como ocurriría en textos muy técnicos),

* Y los conceptos clave mantienen relaciones lógicas entre sí, pero no extremadamente rígidas.

Esto es característico de un texto narrativo y teológico extenso, como la Biblia, donde los temas se articulan de forma amplia y no siempre predecible.

4. Conclusión del análisis

El heatmap de correlación revela:

* Coocurrencias temáticas moderadas (ej. Dios–señor, pueblo–israel).

* Gran dispersión temática, con predominio de correlaciones bajas.

* Estructura narrativa compleja, en la que el uso del lenguaje varía de forma amplia según el libro, capítulo y contexto literario.

Este tipo de análisis es útil para estudios de semántica, estilometría bíblica, análisis temático automatizado y modelado lingüístico avanzado.
"""

# =================================================================
# Heatmap de correlación entre palabras
# =================================================================

import seaborn as sns
plt.figure(figsize=(14,12))
sns.heatmap(corr, cmap="coolwarm", center=0)
plt.title("Correlación entre palabras frecuentes")
plt.show()

# =================================================================
# Longitud promedio del versículo por libro
# =================================================================

df_style = df_norm.groupby("nrsva_book_index")["word_count"].mean().reset_index()
df_style.columns = ["Libro", "LongitudMedia"]

plt.figure(figsize=(12,5))
plt.plot(df_style["Libro"], df_style["LongitudMedia"], marker="o")
plt.title("Complejidad narrativa por libro")
plt.ylabel("Palabras por versículo")
plt.grid(True)
plt.show()

# =================================================================
# Frecuencias de palabras cuando aparece un personaje
# =================================================================

def contexto_personaje(nombre, top=20):
    nombre = nombre.lower()
    mask = df_norm['tokens_no_stop'].apply(lambda toks: nombre in [p.lower() for p in toks])
    tokens_p = [p for lista in df_norm[mask]['tokens_no_stop'] for p in lista]
    frecs = Counter(tokens_p).most_common(top)
    return frecs

# =================================================================
# Frecuencias por libro para un set de palabras
# =================================================================

from collections import Counter
import pandas as pd

# Palabras a comparar (puedes cambiarlas)
palabras = ["dios", "hombre", "tierra", "día", "rey"]

# Construir tabla Libro × Palabras
rows = []

for libro, grupo in df_norm.groupby("nrsva_book_index"):
    tokens = [p.lower() for lista in grupo["tokens_no_stop"] for p in lista]
    frec = Counter(tokens)
    rows.append([libro] + [frec.get(w, 0) for w in palabras])

df_radar = pd.DataFrame(rows, columns=["Libro"] + palabras)

# Escoger algunos libros para comparar (ejemplo: primeros 5)
df_radar_sel = df_radar.head(5)
df_radar_sel

# =================================================================
# Función: Gráfico RADAR
# =================================================================

import numpy as np
import matplotlib.pyplot as plt

def plot_radar(df, fila, etiquetas, titulo="Radar Chart"):
    valores = df.loc[fila, etiquetas].values
    categorias = etiquetas
    N = len(categorias)

    # Cerrar el círculo
    valores = np.concatenate((valores, [valores[0]]))
    angulos = np.linspace(0, 2*np.pi, N, endpoint=False)
    angulos = np.concatenate((angulos, [angulos[0]]))

    plt.figure(figsize=(8, 8))
    plt.polar(angulos, valores, marker='o')

    plt.fill(angulos, valores, alpha=0.25)
    plt.xticks(angulos[:-1], categorias)
    plt.title(titulo, size=14)
    plt.show()

"""## Radar

1. Selección de palabras clave y cálculo de frecuencias por libro
2. Filtrado del libro específico y extracción de sus conteos
3. Construcción del gráfico radar con las palabras como ejes
4. Comparación visual de la relevancia relativa de cada término en el libro

El gráfico radar muestra la frecuencia relativa de cinco palabras clave en un libro bíblico concreto (identificado como Libro 01O):
“dios”, “hombre”, “tierra”, “día” y “rey”. Cada eje del radar representa una de estas palabras y el radio indica cuántas veces aparece en ese libro tras el preprocesamiento.

1. “tierra” como término dominante

El eje correspondiente a “tierra” es el que alcanza el valor más alto del radar (alrededor de 350 ocurrencias).
Esto sugiere que en este libro:

* la referencia al mundo creado, al espacio físico y a la realidad terrenal tiene un papel central;

* hay una fuerte carga narrativa o teológica asociada a la creación, el territorio, la fertilidad o la promesa de la tierra.

En términos de contenido, “tierra” actúa como eje temático principal.

2. “dios”: fuerte presencia, pero secundaria frente a “tierra”

La palabra “dios” presenta también una frecuencia muy alta (en torno a 220–240), situándose como segundo eje más relevante:

* indica una presencia constante de referencias directas a Dios,

* confirma el carácter claramente teológico del libro,

* pero muestra que, en este caso, la narrativa sobre el mundo (“tierra”) supera ligeramente la referencia directa a la divinidad en número de menciones.

Esto es coherente con un libro donde se habla mucho de la acción de Dios en la tierra.

3. “día” y “hombre”: términos relevantes, pero de menor intensidad

Las palabras “día” y “hombre” aparecen con una frecuencia intermedia (aprox. 50–70 apariciones cada una):

* “día” sugiere una narrativa marcada por el tiempo, secuencias, eventos o ciclos (creación, relatos cronológicos, etc.).

* “hombre” apunta a la presencia de personajes humanos, su relación con Dios, la tierra y los acontecimientos narrados.

Ambas palabras funcionan como soportes narrativos: no dominan el discurso, pero están presentes de forma constante.

4. “rey”: término claramente menos frecuente

La palabra “rey” se sitúa en el nivel más bajo del radar (en torno a 40 ocurrencias o menos):

* esto indica que la temática monárquica o institucional no es central en este libro,

* podría tratarse de un libro previo a la instauración de la monarquía o centrado en otros temas (creación, patriarcas, leyes, etc.).

En términos de análisis, “rey” aparece como concepto marginal en la estructura semántica del libro.

5. Lectura global del radar para el Libro 01O

El patrón del radar sugiere que el libro:

* está fuertemente orientado a la relación Dios–tierra,

* mantiene una presencia significativa de tiempo (“día”) y ser humano (“hombre”),

* y apenas desarrolla la figura del “rey”, lo que orienta el foco más hacia creación, promesa, alianzas o relatos fundacionales que hacia estructuras políticas consolidadas.

En síntesis: es un libro donde el énfasis parece estar en la acción de Dios sobre la tierra y en la experiencia humana dentro de esa realidad, más que en instituciones de poder.
"""

# =================================================================
# Graficar radar para un libro específico
# =================================================================

libro_id = df_radar_sel.iloc[0]["Libro"]  # primer libro de la tabla
fila = df_radar_sel[df_radar_sel["Libro"] == libro_id]

plot_radar(df=fila,
           fila=fila.index[0],
           etiquetas=palabras,
           titulo=f"Radar: Libro {libro_id}")

# =================================================================
# Métricas lingüísticas por libro
# =================================================================

import numpy as np
from collections import Counter

datos = []

for libro, grupo in df_norm.groupby("nrsva_book_index"):

    # Tokens del libro
    tokens = [p for lista in grupo["tokens_no_stop"] for p in lista]

    # TTR
    tipos = set(tokens)
    ttr = len(tipos) / len(tokens)

    # Entropía
    frec = Counter(tokens)
    probs = np.array([v / len(tokens) for v in frec.values()])
    H = -np.sum(probs * np.log2(probs))

    # Longitud promedio de versículo
    prom_long = grupo['word_count'].mean()

    datos.append([libro, ttr, H, prom_long])

df_metricas = pd.DataFrame(datos, columns=["Libro", "TTR", "Entropía", "Longitud"])
df_metricas.head()

# =================================================================
# Ley de Zipf: construir tabla rango vs frecuencia
# =================================================================

from collections import Counter
import pandas as pd

# Aplanar todos los tokens limpios
tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

# Contar frecuencias
frecuencias = Counter(tokens)

# Ordenar por frecuencia (descendente)
palabras_ordenadas = frecuencias.most_common()

# Construir DataFrame: rango, palabra, frecuencia
zipf_data = pd.DataFrame(
    [(rango + 1, palabra, freq) for rango, (palabra, freq) in enumerate(palabras_ordenadas)],
    columns=["Rango", "Palabra", "Frecuencia"]
)

display(zipf_data.head(20))
print("✔ Tabla Zipf construida. Total de palabras distintas:", len(zipf_data))

"""# Ley de Zipf

1. Obtención del vocabulario y sus frecuencias absolutas
2. Ordenamiento de palabras por frecuencia (ranking descendente)
3. Construcción de la tabla rango–frecuencia
4. Aplicación de escala logarítmica doble (log–log)
5. Generación del gráfico Ley de Zipf para evaluar distribución léxica

Este gráfico representa la relación entre el rango de cada palabra (posición en una lista ordenada por frecuencia) y la frecuencia con la que aparece en el corpus, utilizando una escala logarítmica en ambos ejes. Esta visualización permite verificar si el texto cumple la Ley de Zipf, uno de los principios estadísticos más importantes de la lingüística cuantitativa.

El resultado muestra con claridad que el texto bíblico sí cumple Zipf.

1. El tramo inicial muestra palabras extremadamente frecuentes

En la zona izquierda del gráfico (rango 1–10):

* Las palabras más frecuentes tienen miles de apariciones.

* El descenso entre la palabra más frecuente y la segunda/tercera es muy pronunciado.

* Este patrón es típico de cualquier lenguaje natural, donde pocas palabras cumplen funciones claves.

Estas palabras suelen ser:

* verbos esenciales,

* sustantivos dominantes temáticos,

* conectores semánticos recurrentes.

2. La sección media mantiene una caída estable y casi lineal

Desde el rango ≈10 hasta ≈1.000, se observa:

* una pendiente estable,

* sin saltos abruptos ni irregularidades,

* lo que indica un comportamiento natural y bien distribuido del vocabulario.

Este tramo es fundamental:

* representa el vocabulario activo principal del corpus,

* incluye palabras frecuentes pero no dominantes,

* mantiene la regularidad estadística que caracteriza la Ley de Zipf.

3. La cola derecha muestra miles de palabras raras o de una sola aparición

A partir del rango ≈1.000 y hasta más de 10.000:

* la frecuencia cae drásticamente,

* muchas palabras aparecen solo una vez (hapax legomena),

* se aprecia una cola larga típica de corpus extensos.

Estas palabras suelen ser:

* nombres propios,

* términos teológicos específicos,

* variantes morfológicas menos comunes,

* palabras asociadas a contextos únicos dentro del texto.

La existencia de esta cola confirma la diversidad semántica y temática del corpus.

4. La linealidad en escala log–log confirma el cumplimiento de Zipf

Visualmente, el gráfico muestra:

* una línea recta inclinada en la mayor parte de la distribución,

* una forma característica de los modelos Zipfianos,

* ausencia de irregularidades que indicarían procesamiento defectuoso del texto.

Esto demuestra que:

* el texto conserva propiedades estadísticas naturales

* incluso después de limpieza, normalización y eliminación de stopwords.

5. ¿Qué implica que el texto cumpla Zipf?

Cumplir la Ley de Zipf significa:

* que el lenguaje del texto es natural, consistente y no aleatorio,

* que se puede modelar mediante técnicas estándar de NLP,

* que el vocabulario está organizado de manera auto-regulada,

* que su estructura es comparable a la de cualquier corpus lingüístico bien formado.

Desde un punto de vista técnico:

* permite estimar vocabulario óptimo para modelos,

* facilita compresión y tokenización,

* valida la calidad del procesamiento previo.
"""

# =================================================================
# Gráfico Ley de Zipf (log-log)
# =================================================================

import matplotlib.pyplot as plt
import numpy as np

ranks = zipf_data["Rango"].values
freqs = zipf_data["Frecuencia"].values

plt.figure(figsize=(8, 6))
plt.loglog(ranks, freqs, marker='.', linestyle='none')

plt.title("Ley de Zipf - Texto bíblico (tokens_no_stop)")
plt.xlabel("Rango de la palabra (log)")
plt.ylabel("Frecuencia de la palabra (log)")
plt.grid(True, which="both", linestyle='--', alpha=0.5)
plt.show()

"""1. Objetivo del análisis

Este análisis busca comprobar si la distribución del vocabulario del texto sigue la Ley de Zipf, que describe cómo las palabras más frecuentes tienden a repetirse con mucha mayor frecuencia que las menos comunes. Para ello, se aplica un ajuste lineal a los datos transformados a escala logarítmica.

2. Procedimiento realizado

* Se seleccionan las palabras más frecuentes (las primeras 500) para obtener una representación clara del comportamiento zipfiano.

* Se calcula el rango de cada palabra y su frecuencia.

* Ambos valores se transforman a escala logarítmica.

* Se ajusta una línea recta sobre los datos transformados, lo que permite medir el grado de comportamiento zipfiano del texto.

3. Gráfico generado

El gráfico muestra:

* Puntos azules: datos reales del texto, expresados en escala log-log (rango vs frecuencia).

* Línea azul: ajuste lineal aplicado a esos datos.

* Título: muestra la pendiente obtenida como medida del comportamiento zipfiano.

4. Interpretación del gráfico

* Los puntos siguen una tendencia claramente lineal en la gráfica log-log, lo que confirma que el texto cumple la Ley de Zipf.

* Las palabras más frecuentes están muy por encima del resto, mientras que la frecuencia disminuye de manera constante a medida que aumenta el rango.

* La línea ajustada representa adecuadamente esta tendencia, indicando una alta regularidad estadística típica del lenguaje natural.

* Esto significa que:

* * el texto tiene una estructura lingüística bien definida,

* * las transformaciones de limpieza y normalización fueron correctas,

y el comportamiento del vocabulario sigue patrones universales del idioma.

5. Conclusión

El análisis confirma que, incluso después del procesamiento de texto (tokenización, normalización y eliminación de stopwords), el vocabulario del texto mantiene una distribución característica del lenguaje natural, ajustándose firmemente al comportamiento esperado de la Ley de Zipf.
"""

# =================================================================
# Ajuste lineal en escala log-log (opcional)
# =================================================================

# Usamos solo las primeras N palabras (las más frecuentes)
N = 500  # puedes ajustar este valor
ranks_N = ranks[:N]
freqs_N = freqs[:N]

# Pasar a log10
log_r = np.log10(ranks_N)
log_f = np.log10(freqs_N)

# Ajuste lineal: log_f ≈ a + b * log_r
coef = np.polyfit(log_r, log_f, 1)
a, b = coef

print(f"Recta ajustada en log10: log10(freq) ≈ {a:.3f} + {b:.3f} * log10(rango)")

# Graficar puntos + recta
plt.figure(figsize=(8, 6))
plt.scatter(log_r, log_f, s=10, alpha=0.5, label="Datos (log10)")
plt.plot(log_r, a + b * log_r, linewidth=2, label="Ajuste lineal")

plt.title(f"Ley de Zipf (log10) — pendiente ≈ {b:.3f}")
plt.xlabel("log10(Rango)")
plt.ylabel("log10(Frecuencia)")
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend()
plt.show()