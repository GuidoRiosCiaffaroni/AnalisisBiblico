# -*- coding: utf-8 -*-
"""Control2_04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18vATf2LogM0XUK_EmD-3esvE0Ode0l2U

# Control 2

#### Guido Rios Ciaffaroni
#### Eduardo Opazo Diaz

https://www.biblesupersearch.com/unbound-downloads/
"""

!wget -O spanish_sagradas_escrituras_1569.txt \
https://raw.githubusercontent.com/GuidoRiosCiaffaroni/AnalisisBiblico/refs/heads/main/Libros/spanish_sagradas_escrituras_1569_utf8_mapped_to_NRSVA.txt

# =================================================================
# 1. SETUP E IMPORTACI√ìN DE DATOS
# =================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Descargar recursos de NLTK (solo necesario la primera vez)
try:
    nltk.download('stopwords')
    nltk.download('punkt')
except:
    pass  # Ya descargado

# Ruta del archivo subido al entorno (prove√≠do por ChatGPT)
FILE_PATH = '/content/spanish_sagradas_escrituras_1569.txt'

# Columnas del formato BCVS (The Unbound Bible)
COLUMNS = [
    'nrsva_book_index', 'nrsva_chapter', 'nrsva_verse',
    'orig_book_index', 'orig_chapter', 'orig_verse',
    'orig_subverse', 'order_by', 'text'
]

# =================================================================
# 2. CARGA DEL ARCHIVO
# =================================================================
df = pd.read_csv(
    FILE_PATH,
    sep='\t',
    names=COLUMNS,
    comment='#',
    encoding='utf-8',
    skipinitialspace=True
)

# =================================================================
# 3. LIMPIEZA INICIAL DEL TEXTO
# =================================================================

# Eliminar etiquetas <I> ... </I> y el s√≠mbolo ¬∂
df['text_clean'] = df['text'].astype(str)
df['text_clean'] = df['text_clean'].apply(lambda x: re.sub(r'<I>.*?</I>', '', x))
df['text_clean'] = df['text_clean'].apply(lambda x: x.replace('¬∂', ' ').strip())

# Reemplazar cadenas vac√≠as por NaN y eliminar filas vac√≠as
df['text_clean'] = df['text_clean'].replace('', np.nan)
df = df.dropna(subset=['text_clean']).reset_index(drop=True)

# =================================================================
# 4. LIMPIEZA PROFUNDA DEL TEXTO (para NLP)
# =================================================================

def limpiar_texto(t):
    t = t.lower()  # Min√∫sculas
    t = re.sub(r'[^a-z√°√©√≠√≥√∫√±√º ]', ' ', t)  # Solo letras
    t = re.sub(r'\s+', ' ', t)  # Un solo espacio
    return t.strip()

df['text_clean_deep'] = df['text_clean'].apply(limpiar_texto)

print(" DataFrame cargado y limpiado correctamente")

print("\nPrimeras 5 filas del DataFrame limpio:")
print(df[['nrsva_book_index', 'nrsva_chapter', 'nrsva_verse', 'text_clean_deep']].head())

print("\nInformaci√≥n del DataFrame:")
print(df.info())

# (BLOQUE PREVIO ‚Äì YA EJECUTADO)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

FILE_PATH = '/content/spanish_sagradas_escrituras_1569.txt'

COLUMNS = [
    'nrsva_book_index', 'nrsva_chapter', 'nrsva_verse',
    'orig_book_index', 'orig_chapter', 'orig_verse',
    'orig_subverse', 'order_by', 'text'
]

df = pd.read_csv(
    FILE_PATH,
    sep='\t',
    names=COLUMNS,
    comment='#',
    encoding='utf-8',
    skipinitialspace=True
)

print("DataFrame original cargado:", df.shape)

"""# Manejo de faltantes y duplicados"""

# =================================================================
# 1. MANEJO DE FALTANTES Y DUPLICADOS
# =================================================================

# Copia de trabajo para no modificar el original
df_clean = df.copy()

# 1.1 Revisi√≥n de valores faltantes
print("Valores faltantes por columna antes de limpiar:")
print(df_clean.isna().sum())

# 1.2 Eliminar filas donde 'text' est√© vac√≠o o sea NaN
df_clean['text'] = df_clean['text'].astype(str)
df_clean['text'] = df_clean['text'].replace(['', ' ', 'nan'], np.nan)
df_clean = df_clean.dropna(subset=['text'])

# 1.3 Eliminaci√≥n de filas duplicadas exactas en todas las columnas
df_clean = df_clean.drop_duplicates()

# (Opcional) Eliminaci√≥n de duplicados solo a nivel de texto
# df_clean = df_clean.drop_duplicates(subset=['text'])

df_clean = df_clean.reset_index(drop=True)

print("\n Despu√©s de limpiar faltantes y duplicados:")
print("Shape:", df_clean.shape)
print("Valores faltantes por columna:")
print(df_clean.isna().sum())

"""# BLOQUE 2 ‚Äì Eliminaci√≥n de ruido (etiquetas, s√≠mbolos, puntuaci√≥n, n√∫meros‚Ä¶)"""

# =================================================================
# 2. ELIMINACI√ìN DE RUIDO
# =================================================================

import re

# Partimos de df_clean generado en el bloque anterior
df_noise = df_clean.copy()

def eliminar_ruido(texto):
    # Aseguramos que sea string
    t = str(texto)

    # 2.1 Eliminar etiquetas HTML tipo <I>...</I> y similares
    t = re.sub(r'<.*?>', ' ', t)

    # 2.2 Eliminar s√≠mbolo ¬∂
    t = t.replace('¬∂', ' ')

    # 2.3 Eliminar n√∫meros
    t = re.sub(r'\d+', ' ', t)

    # 2.4 Eliminar signos de puntuaci√≥n y caracteres no alfab√©ticos
    t = re.sub(r"[^a-zA-Z√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë√º√ú\s]", " ", t)

    # 2.5 Reemplazar m√∫ltiples espacios por uno solo
    t = re.sub(r'\s+', ' ', t)

    # 2.6 Quitar espacios al inicio y al final
    t = t.strip()

    return t

# Nueva columna con texto sin ruido
df_noise['text_no_noise'] = df_noise['text'].apply(eliminar_ruido)

print(" Ruido eliminado. Ejemplo de antes / despu√©s:\n")
print("ANTES :", df_noise['text'].iloc[0])
print("DESPU√âS:", df_noise['text_no_noise'].iloc[0])

"""# Normalizaci√≥n (minusculizaci√≥n, stopwords, tokenizaci√≥n, etc.)"""

# =================================================================
# 3. NORMALIZACI√ìN DEL TEXTO (CORREGIDO y VALIDADO)
# =================================================================

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Descargar recursos necesarios
try:
    nltk.download('stopwords')
    nltk.download('punkt')
    nltk.download('punkt_tab')  # NECESARIO EN GOOGLE COLAB
except:
    pass

df_norm = df_noise.copy()

# 3.1 Convertir a min√∫sculas
df_norm['text_lower'] = df_norm['text_no_noise'].str.lower()

# 3.2 Tokenizaci√≥n
df_norm['tokens'] = df_norm['text_lower'].apply(word_tokenize)

# 3.3 Stopwords en espa√±ol
stopwords_es = set(stopwords.words('spanish'))

def quitar_stopwords(tokens):
    return [tok for tok in tokens if tok not in stopwords_es and len(tok) > 1]

df_norm['tokens_no_stop'] = df_norm['tokens'].apply(quitar_stopwords)

# 3.4 Reconstruir texto normalizado
df_norm['text_normalized'] = df_norm['tokens_no_stop'].apply(lambda toks: ' '.join(toks))

print(" Normalizaci√≥n completada correctamente.")

# Mostrar ejemplo
print("\nEjemplo:\n")
print("Original :", df_norm['text_no_noise'].iloc[0])
print("Tokens   :", df_norm['tokens'].iloc[0][:20])
print("Sin stop :", df_norm['tokens_no_stop'].iloc[0][:20])
print("Texto normalizado :", df_norm['text_normalized'].iloc[0][:200])

"""# Estadistica Basica

## N√∫mero total de vers√≠culos
"""

# =================================================================
# C√°lculo del n√∫mero total de vers√≠culos
# =================================================================

total_versiculos = len(df_norm)

print(" N√∫mero total de vers√≠culos en la base de datos:", total_versiculos)

# =================================================================
# Gr√°fico de n√∫mero de vers√≠culos por libro
# =================================================================

import matplotlib.pyplot as plt

# Contar vers√≠culos por libro usando el √≠ndice de libro original
versiculos_por_libro = df_norm.groupby('nrsva_book_index').size()

# Crear el gr√°fico
plt.figure(figsize=(14, 6))
versiculos_por_libro.plot(kind='bar')

plt.title('N√∫mero de Vers√≠culos por Libro')
plt.xlabel('Libro (√çndice NRSVA)')
plt.ylabel('Cantidad de Vers√≠culos')

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### N√∫mero total de palabras"""

# =================================================================
# C√°lculo del n√∫mero total de palabras
# =================================================================

# Asegurarse de que la columna existe
if 'tokens_no_stop' not in df_norm.columns:
    raise ValueError("‚ö† La columna 'tokens_no_stop' no existe. Aseg√∫rate de haber ejecutado la normalizaci√≥n.")

# Contar palabras
total_palabras = df_norm['tokens_no_stop'].apply(len).sum()

print(" N√∫mero total de palabras (limpias y sin stopwords):", total_palabras)

# =================================================================
# Gr√°fico de n√∫mero de palabras por vers√≠culo
# =================================================================

import matplotlib.pyplot as plt

# Crear una columna con el conteo de palabras por vers√≠culo
df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

plt.figure(figsize=(14, 5))
plt.plot(df_norm['word_count'], alpha=0.7)

plt.title("N√∫mero de Palabras por Vers√≠culo (Texto Normalizado)")
plt.xlabel("Vers√≠culo (ordenado por 'order_by')")
plt.ylabel("Cantidad de Palabras")

plt.grid(True, linestyle='--', alpha=0.5)

plt.show()

# =================================================================
# Promedio de palabras por libro
# =================================================================

promedio_palabras_libro = df_norm.groupby('nrsva_book_index')['word_count'].mean()

plt.figure(figsize=(14, 6))
promedio_palabras_libro.plot(kind='bar')

plt.title("Promedio de Palabras por Vers√≠culo en Cada Libro")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("Promedio de Palabras")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### N√∫mero total de palabras √∫nicas ("vocabulario")"""

# =================================================================
# C√°lculo del n√∫mero de palabras √∫nicas (vocabulario)
# =================================================================

# Aplanar lista de listas
todas_las_palabras = [pal for lista in df_norm['tokens_no_stop'] for pal in lista]

# Convertir a conjunto (elimina duplicados)
vocabulario = set(todas_las_palabras)

num_palabras_unicas = len(vocabulario)

print(" N√∫mero total de palabras √∫nicas en el texto:", num_palabras_unicas)

# =================================================================
# Tama√±o del vocabulario por libro
# =================================================================

import matplotlib.pyplot as plt

# Funci√≥n para obtener vocabulario de un libro
def vocabulario_por_libro(df):
    resultados = {}
    for libro, grupo in df.groupby('nrsva_book_index'):
        palabras = [pal for lista in grupo['tokens_no_stop'] for pal in lista]
        resultados[libro] = len(set(palabras))
    return resultados

vocab_libros = vocabulario_por_libro(df_norm)

# Convertir a Series para facilitar el gr√°fico
import pandas as pd
vocab_series = pd.Series(vocab_libros).sort_index()

# Gr√°fico
plt.figure(figsize=(14, 6))
vocab_series.plot(kind='bar')

plt.title("Tama√±o del Vocabulario por Libro")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("N√∫mero de palabras √∫nicas")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Longitud promedio del texto por vers√≠culo"""

# =================================================================
# Longitud promedio del texto por vers√≠culo
# =================================================================

# Crear columna de longitud por vers√≠culo
df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

# Longitud promedio
longitud_promedio = df_norm['word_count'].mean()

print(" Longitud promedio del texto por vers√≠culo:", longitud_promedio)

# =================================================================
# Gr√°fico: Longitud promedio por libro
# =================================================================

import matplotlib.pyplot as plt

# Calcular promedio por libro
promedio_por_libro = df_norm.groupby('nrsva_book_index')['word_count'].mean()

plt.figure(figsize=(14, 6))
promedio_por_libro.plot(kind='bar')

plt.title("Longitud Promedio del Texto por Vers√≠culo en Cada Libro")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("N√∫mero Promedio de Palabras por Vers√≠culo")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

# =================================================================
# Distribuci√≥n general de longitudes de vers√≠culos
# =================================================================

plt.figure(figsize=(12, 5))
plt.hist(df_norm['word_count'], bins=40, alpha=0.7)

plt.title("Distribuci√≥n de Longitudes de Vers√≠culos")
plt.xlabel("N√∫mero de Palabras por Vers√≠culo")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""### Calcular longitud m√°xima y m√≠nima del vers√≠culo"""

# =================================================================
# Longitud m√°xima y m√≠nima del vers√≠culo
# =================================================================

# Asegurar columna de conteo de palabras
df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

longitud_maxima = df_norm['word_count'].max()
longitud_minima = df_norm['word_count'].min()

# Vers√≠culos espec√≠ficos (opcional)
versiculo_max = df_norm[df_norm['word_count'] == longitud_maxima].iloc[0]
versiculo_min = df_norm[df_norm['word_count'] == longitud_minima].iloc[0]

print(" Longitud m√°xima de un vers√≠culo:", longitud_maxima, "palabras")
print(" Vers√≠culo m√°s largo:", versiculo_max['text_no_noise'])
print()
print(" Longitud m√≠nima de un vers√≠culo:", longitud_minima, "palabras")
print(" Vers√≠culo m√°s corto:", versiculo_min['text_no_noise'])

# =================================================================
# Gr√°fico: Longitud m√°xima, m√≠nima y promedio
# =================================================================

import matplotlib.pyplot as plt

longitud_promedio = df_norm['word_count'].mean()

metricas = {
    "M√≠nima": longitud_minima,
    "Promedio": longitud_promedio,
    "M√°xima": longitud_maxima
}

plt.figure(figsize=(8, 5))
plt.bar(metricas.keys(), metricas.values(), color=['green', 'blue', 'red'])

plt.title("Longitud M√≠nima, Promedio y M√°xima de los Vers√≠culos")
plt.ylabel("N√∫mero de Palabras")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Distribuci√≥n de longitudes de vers√≠culos
# =================================================================

plt.figure(figsize=(12, 5))
plt.hist(df_norm['word_count'], bins=40, alpha=0.7, color='purple')

plt.title("Distribuci√≥n de Longitudes de Vers√≠culos")
plt.xlabel("N√∫mero de Palabras")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""### C√°lculo de la distribuci√≥n de longitudes"""

# =================================================================
# Distribuci√≥n descriptiva de longitudes de vers√≠culos
# =================================================================

# Asegurar que existe la columna de conteo
df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

descripcion = df_norm['word_count'].describe()

print(" Distribuci√≥n descriptiva de longitudes:")
print(descripcion)

# =================================================================
# Histograma de la distribuci√≥n de longitudes
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.hist(df_norm['word_count'], bins=40, alpha=0.7, color='blue')

plt.title("Distribuci√≥n de Longitudes de Vers√≠culos")
plt.xlabel("N√∫mero de Palabras por Vers√≠culo")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Boxplot de longitudes de vers√≠culos
# =================================================================

plt.figure(figsize=(8, 4))
plt.boxplot(df_norm['word_count'], vert=False)

plt.title("Boxplot de Longitudes de Vers√≠culos")
plt.xlabel("N√∫mero de Palabras")

plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# KDE (curva suavizada de la distribuci√≥n)
# =================================================================

import seaborn as sns

plt.figure(figsize=(12, 5))
sns.kdeplot(df_norm['word_count'], fill=True)

plt.title("Distribuci√≥n KDE de Longitudes de Vers√≠culos")
plt.xlabel("N√∫mero de Palabras")
plt.ylabel("Densidad")
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

"""### Calcular m√©tricas estad√≠sticas de longitud"""

# =================================================================
# M√©tricas estad√≠sticas: media, mediana, moda, varianza,
# desviaci√≥n est√°ndar de la longitud de vers√≠culos
# =================================================================

import numpy as np
from scipy import stats

# Asegurar columna de conteo
df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

# Media
media = df_norm['word_count'].mean()

# Mediana
mediana = df_norm['word_count'].median()

# Moda (puede devolver varias; tomamos la primera)
moda = stats.mode(df_norm['word_count'], keepdims=True)[0][0]

# Varianza
varianza = df_norm['word_count'].var()

# Desviaci√≥n est√°ndar
desviacion = df_norm['word_count'].std()

print(" M√âTRICAS ESTAD√çSTICAS DE LONGITUD DE VERS√çCULOS")
print("-----------------------------------------------")
print(f"Media: {media:.2f} palabras")
print(f"Mediana: {mediana:.2f} palabras")
print(f"Moda: {moda} palabras")
print(f"Varianza: {varianza:.2f}")
print(f"Desviaci√≥n Est√°ndar: {desviacion:.2f}")

# =================================================================
# Gr√°fico comparativo de m√©tricas estad√≠sticas
# =================================================================

import matplotlib.pyplot as plt

metricas = {
    "Media": media,
    "Mediana": mediana,
    "Moda": moda,
    "Varianza": varianza,
    "Desviaci√≥n Est√°ndar": desviacion
}

plt.figure(figsize=(10, 5))
plt.bar(metricas.keys(), metricas.values(), color=['blue', 'green', 'orange', 'purple', 'red'])

plt.title("M√©tricas Estad√≠sticas de Longitud de Vers√≠culos")
plt.ylabel("Valor")
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

# =================================================================
# Tabla resumen de m√©tricas estad√≠sticas
# =================================================================

import pandas as pd

tabla = pd.DataFrame({
    "M√©trica": ["Media", "Mediana", "Moda", "Varianza", "Desviaci√≥n Est√°ndar"],
    "Valor": [media, mediana, moda, varianza, desviacion]
})

display(tabla)

"""### C√°lculo de la cantidad de letras promedio por palabra"""

# =================================================================
# Cantidad de letras promedio por palabra
# =================================================================

# Aplanar lista de palabras
todas_las_palabras = [palabra for lista in df_norm['tokens_no_stop'] for palabra in lista]

# Calcular longitud de cada palabra
longitudes = [len(palabra) for palabra in todas_las_palabras]

# Promedio de letras
promedio_letras = sum(longitudes) / len(longitudes)

print(" Cantidad de letras promedio por palabra:", round(promedio_letras, 2))

# =================================================================
# Histograma de longitudes de palabras
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.hist(longitudes, bins=15, alpha=0.7, color='teal')

plt.title("Distribuci√≥n de Longitudes de Palabras")
plt.xlabel("N√∫mero de Letras")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Boxplot de longitudes de palabras
# =================================================================

plt.figure(figsize=(8, 4))
plt.boxplot(longitudes, vert=False)

plt.title("Boxplot de Longitudes de Palabras")
plt.xlabel("N√∫mero de Letras")
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.show()

"""### Calcular porcentaje de stopwords vs. palabras √∫tiles"""

# =================================================================
# Porcentaje de stopwords vs. palabras √∫tiles
# =================================================================

from nltk.corpus import stopwords
stopwords_es = set(stopwords.words('spanish'))

# Aplanar listas
todas_las_palabras = [pal for lista in df_norm['tokens'] for pal in lista]

# Identificar stopwords
stopwords_en_texto = [pal for pal in todas_las_palabras if pal.lower() in stopwords_es]

# Palabras √∫tiles (no stopwords)
palabras_utiles = [pal for pal in todas_las_palabras if pal.lower() not in stopwords_es]

# Totales
total = len(todas_las_palabras)
total_stop = len(stopwords_en_texto)
total_utiles = len(palabras_utiles)

# Porcentajes
porcentaje_stop = (total_stop / total) * 100
porcentaje_utiles = (total_utiles / total) * 100

print(" PORCENTAJE DE STOPWORDS VS. PALABRAS √öTILES")
print("------------------------------------------------")
print(f"Total de palabras: {total}")
print(f"Stopwords: {total_stop} ({porcentaje_stop:.2f}%)")
print(f"Palabras √∫tiles: {total_utiles} ({porcentaje_utiles:.2f}%)")

# =================================================================
# Gr√°fico: Stopwords vs palabras √∫tiles (Pie Chart)
# =================================================================

import matplotlib.pyplot as plt

labels = ['Stopwords', 'Palabras √∫tiles']
sizes = [porcentaje_stop, porcentaje_utiles]
colors = ['red', 'green']

plt.figure(figsize=(8, 8))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)
plt.title("Porcentaje de Stopwords vs. Palabras √ötiles")
plt.axis('equal')  # Mantener forma circular
plt.show()

# =================================================================
# Gr√°fico de barras: Stopwords vs palabras √∫tiles
# =================================================================

plt.figure(figsize=(8, 5))
plt.bar(['Stopwords', 'Palabras √∫tiles'], [total_stop, total_utiles], color=['red', 'green'])

plt.title("Cantidad de Stopwords vs. Palabras √ötiles")
plt.ylabel("N√∫mero de Palabras")
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### N√∫mero de libros distintos"""

# =================================================================
# N√∫mero de libros distintos
# =================================================================

# Contar cu√°ntos libros diferentes hay en el √≠ndice NRSVA
num_libros_distintos = df_norm['nrsva_book_index'].nunique()

print(" N√∫mero de libros distintos en el corpus:", num_libros_distintos)

# =================================================================
# Listado de libros distintos
# =================================================================

libros_unicos = df_norm['nrsva_book_index'].unique()
libros_unicos.sort()

print(" Libros presentes (√≠ndices NRSVA):")
print(libros_unicos)

# =================================================================
# Gr√°fico: N√∫mero de vers√≠culos por libro
# =================================================================

import matplotlib.pyplot as plt

versiculos_por_libro = df_norm.groupby('nrsva_book_index').size()

plt.figure(figsize=(14, 6))
versiculos_por_libro.plot(kind='bar')

plt.title("Cantidad de Vers√≠culos por Libro")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("N√∫mero de Vers√≠culos")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### N√∫mero de cap√≠tulos por libro"""

# =================================================================
# N√∫mero de cap√≠tulos por libro
# =================================================================

# Contar cap√≠tulos √∫nicos dentro de cada libro
capitulos_por_libro = df_norm.groupby('nrsva_book_index')['nrsva_chapter'].nunique()

print("üìò N√∫mero de cap√≠tulos por libro:")
print(capitulos_por_libro)

# =================================================================
# Gr√°fico: N√∫mero de cap√≠tulos por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
capitulos_por_libro.plot(kind='bar', color='skyblue')

plt.title("N√∫mero de Cap√≠tulos por Libro")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("Cantidad de Cap√≠tulos")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

# =================================================================
# Tabla ordenada de cap√≠tulos por libro
# =================================================================

import pandas as pd

tabla_capitulos = capitulos_por_libro.sort_index().reset_index()
tabla_capitulos.columns = ['Libro (NRSVA)', 'Cap√≠tulos']

display(tabla_capitulos)

"""### Calcular las Top N palabras m√°s frecuentes"""

# =================================================================
# Top N palabras m√°s frecuentes
# =================================================================

from collections import Counter

N = 50  # Cambia este valor para obtener m√°s o menos palabras

# Aplanar lista de palabras √∫tiles
todas_las_palabras_utiles = [pal for lista in df_norm['tokens_no_stop'] for pal in lista]

# Calcular frecuencias
frecuencias = Counter(todas_las_palabras_utiles)

top_n = frecuencias.most_common(N)

print(f"üîù Top {N} palabras m√°s frecuentes:")
for palabra, freq in top_n:
    print(f"{palabra}: {freq}")

# =================================================================
# Tabla con las Top N palabras m√°s frecuentes
# =================================================================

import pandas as pd

df_top_n = pd.DataFrame(top_n, columns=['Palabra', 'Frecuencia'])
display(df_top_n)

# =================================================================
# Gr√°fico de barras: Top N palabras m√°s frecuentes
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.bar(df_top_n['Palabra'], df_top_n['Frecuencia'], color='teal')

plt.title(f"Top {N} Palabras M√°s Frecuentes (Limpias y Sin Stopwords)")
plt.xlabel("Palabra")
plt.ylabel("Frecuencia")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Frecuencia de palabras por libro (tabla completa)"""

# =================================================================
# Ver los √≠ndices de libros disponibles en df_norm
# =================================================================

libros_disponibles = sorted(df_norm['nrsva_book_index'].unique())

print(" Libros disponibles (√≠ndices NRSVA):")
print(libros_disponibles)

# =================================================================
# Frecuencia de palabras por libro
# =================================================================

from collections import Counter

# Diccionario: libro ‚Üí frecuencia de palabras
frecuencias_por_libro = {}

for libro, grupo in df_norm.groupby('nrsva_book_index'):
    palabras = [pal for lista in grupo['tokens_no_stop'] for pal in lista]
    frecuencias_por_libro[libro] = Counter(palabras)

print(" Frecuencias de palabras por libro generadas correctamente.")

# =================================================================
# Frecuencia de palabras por libro (seguro)
# =================================================================

from collections import Counter

frecuencias_por_libro = {}

for libro, grupo in df_norm.groupby('nrsva_book_index'):
    palabras = [pal for lista in grupo['tokens_no_stop'] for pal in lista]
    frecuencias_por_libro[libro] = Counter(palabras)

print(" Frecuencias por libro generadas correctamente.")
print(" Libros que tienen datos:", list(frecuencias_por_libro.keys()))

# =================================================================
# Selecci√≥n segura de un libro para mostrar Top N palabras
# =================================================================

N = 20
libro_id = libros_disponibles[0]   # Por ejemplo, elegir el primero autom√°ticamente

print(f" Usando el libro con √≠ndice NRSVA: {libro_id}")

top_n_libro = frecuencias_por_libro[libro_id].most_common(N)

print(f"\n Top {N} palabras del libro {libro_id}:")
for palabra, freq in top_n_libro:
    print(f"{palabra}: {freq}")

# =================================================================
# Tabla con las Top N palabras del libro elegido
# =================================================================

import pandas as pd

df_top_n_libro = pd.DataFrame(top_n_libro, columns=["Palabra", "Frecuencia"])
display(df_top_n_libro)

# =================================================================
# Gr√°fico Top N palabras del libro elegido
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.bar(df_top_n_libro['Palabra'], df_top_n_libro['Frecuencia'], color='darkcyan')

plt.title(f"Top {N} Palabras M√°s Frecuentes ‚Äî Libro NRSVA {libro_id}")
plt.xlabel("Palabra")
plt.ylabel("Frecuencia")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Frecuencia de palabras por cap√≠tulo"""

# =================================================================
# Frecuencia de palabras por cap√≠tulo
# =================================================================

from collections import Counter

frecuencias_por_capitulo = {}

for (libro, cap), grupo in df_norm.groupby(['nrsva_book_index', 'nrsva_chapter']):
    palabras = [pal for lista in grupo['tokens_no_stop'] for pal in lista]
    frecuencias_por_capitulo[(libro, cap)] = Counter(palabras)

print(" Frecuencias generadas correctamente para cada cap√≠tulo.")
print("Ejemplo de claves disponibles (libro, cap√≠tulo):")
print(list(frecuencias_por_capitulo.keys())[:10])  # Mostrar solo primeros 10

# =================================================================
# Ver cap√≠tulos disponibles
# =================================================================

capitulos_disponibles = list(frecuencias_por_capitulo.keys())
capitulos_disponibles[:10]

# =================================================================
# Top N palabras de un cap√≠tulo
# =================================================================

N = 20

# Seleccionar un cap√≠tulo usando un √≠ndice v√°lido
libro_id = capitulos_disponibles[0][0]      # Ej: primer libro disponible
capitulo_id = capitulos_disponibles[0][1]   # Ej: primer cap√≠tulo disponible

print(f" Analizando Libro {libro_id}, Cap√≠tulo {capitulo_id}")

top_n_capitulo = frecuencias_por_capitulo[(libro_id, capitulo_id)].most_common(N)

print(f" Top {N} palabras del libro {libro_id}, cap√≠tulo {capitulo_id}:")
for palabra, freq in top_n_capitulo:
    print(f"{palabra}: {freq}")

# =================================================================
# DataFrame de las Top N palabras del cap√≠tulo
# =================================================================

import pandas as pd

df_top_n_cap = pd.DataFrame(top_n_capitulo, columns=["Palabra", "Frecuencia"])
display(df_top_n_cap)

# =================================================================
# Gr√°fico Top N palabras del cap√≠tulo
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.bar(df_top_n_cap['Palabra'], df_top_n_cap['Frecuencia'], color='darkolivegreen')

plt.title(f"Top {N} Palabras ‚Äî Libro {libro_id}, Cap√≠tulo {capitulo_id}")
plt.xlabel("Palabra")
plt.ylabel("Frecuencia")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Histograma de frecuencia de palabras (Top N)"""

# =================================================================
# Histograma de frecuencia de palabras (Top N)
# =================================================================

from collections import Counter
import matplotlib.pyplot as plt

N = 50  # Cambia para mostrar m√°s o menos palabras

# Aplanar palabras
palabras = [p for lista in df_norm['tokens_no_stop'] for p in lista]

# Frecuencias
frecuencias = Counter(palabras)
top_n = frecuencias.most_common(N)

# Solo los valores de frecuencia
valores = [freq for palabra, freq in top_n]

plt.figure(figsize=(12, 6))
plt.hist(valores, bins=10, color='steelblue', alpha=0.8)

plt.title(f"Histograma de Frecuencia ‚Äî Top {N} Palabras")
plt.xlabel("Frecuencia")
plt.ylabel("Cantidad de Palabras")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Histograma de longitud de palabras
# =================================================================

# Longitud de cada palabra
longitudes_palabras = [len(p) for p in palabras]

plt.figure(figsize=(12, 5))
plt.hist(longitudes_palabras, bins=15, color='teal', alpha=0.8)

plt.title("Histograma de Longitud de Palabras")
plt.xlabel("N√∫mero de Letras por Palabra")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Histograma de longitud de vers√≠culos
# =================================================================

# Asegurar columna de conteo
df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

plt.figure(figsize=(12, 5))
plt.hist(df_norm['word_count'], bins=40, color='purple', alpha=0.8)

plt.title("Histograma de Longitud de Vers√≠culos")
plt.xlabel("N√∫mero de Palabras por Vers√≠culo")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""# Definir palabras clave y contar sus frecuencias en todo el corpus"""

# =================================================================
# Frecuencia total de palabras clave en todo el corpus
# =================================================================

palabras_clave = ["dios", "hombre", "tierra"]  # Puedes agregar m√°s

# Todas las palabras del corpus
todas = [p.lower() for lista in df_norm['tokens_no_stop'] for p in lista]

frecuencia_global = {p: todas.count(p) for p in palabras_clave}

print(" Frecuencia global de palabras clave:")
for palabra, freq in frecuencia_global.items():
    print(f"{palabra}: {freq}")

# =================================================================
# Gr√°fico de frecuencia global de palabras clave
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.bar(frecuencia_global.keys(), frecuencia_global.values(), color=['purple', 'green', 'orange'])

plt.title("Frecuencia Global de Palabras Clave")
plt.xlabel("Palabra")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Frecuencia de palabras clave por libro
# =================================================================

frecuencia_por_libro = {}

for libro, grupo in df_norm.groupby('nrsva_book_index'):
    palabras = [p.lower() for lista in grupo['tokens_no_stop'] for p in lista]
    frecuencia_por_libro[libro] = {clave: palabras.count(clave) for clave in palabras_clave}

print(" Frecuencias por libro generadas.")
frecuencia_por_libro

# =================================================================
# Crear DataFrame: filas = libros, columnas = palabras clave
# =================================================================

import pandas as pd

df_frec_libro = pd.DataFrame.from_dict(frecuencia_por_libro, orient='index')
df_frec_libro.index.name = "Libro (NRSVA)"

display(df_frec_libro)

# =================================================================
# Gr√°fico comparativo por libro
# =================================================================

plt.figure(figsize=(14, 7))
df_frec_libro.plot(kind='bar', figsize=(14, 7))

plt.title("Frecuencia de Palabras Clave por Libro")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("Frecuencia")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Top N palabras con stopwords"""

# =================================================================
# Top N palabras CON stopwords
# =================================================================

from collections import Counter

N = 20  # Ajusta la cantidad de palabras a listar

# Aplanar todas las palabras (incluye stopwords)
palabras_con_stop = [p.lower() for lista in df_norm['tokens'] for p in lista]

frecuencias_con_stop = Counter(palabras_con_stop)
top_n_con_stop = frecuencias_con_stop.most_common(N)

print(f" Top {N} palabras CON stopwords:")
for palabra, freq in top_n_con_stop:
    print(f"{palabra}: {freq}")

# =================================================================
# Top N palabras SIN stopwords
# =================================================================

# Aplanar palabras √∫tiles (sin stopwords)
palabras_sin_stop = [p.lower() for lista in df_norm['tokens_no_stop'] for p in lista]

frecuencias_sin_stop = Counter(palabras_sin_stop)
top_n_sin_stop = frecuencias_sin_stop.most_common(N)

print(f"\n Top {N} palabras SIN stopwords:")
for palabra, freq in top_n_sin_stop:
    print(f"{palabra}: {freq}")

# =================================================================
# Comparaci√≥n en DataFrame
# =================================================================

import pandas as pd

df_con = pd.DataFrame(top_n_con_stop, columns=["Palabra_con", "Frecuencia_con"])
df_sin = pd.DataFrame(top_n_sin_stop, columns=["Palabra_sin", "Frecuencia_sin"])

display(df_con)
display(df_sin)

# =================================================================
# Gr√°ficos comparativos lado a lado
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(16, 6))

# --- Gr√°fico 1: CON stopwords
plt.subplot(1, 2, 1)
plt.bar(df_con['Palabra_con'], df_con['Frecuencia_con'], color='crimson')
plt.title(f"Top {N} Palabras CON Stopwords")
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

# --- Gr√°fico 2: SIN stopwords
plt.subplot(1, 2, 2)
plt.bar(df_sin['Palabra_sin'], df_sin['Frecuencia_sin'], color='teal')
plt.title(f"Top {N} Palabras SIN Stopwords")
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.tight_layout()
plt.show()

"""### Calcular palabras totales por libro"""

# =================================================================
# Palabras totales por libro
# =================================================================

from collections import Counter

# Diccionario: libro ‚Üí total de palabras
palabras_por_libro = {}

for libro, grupo in df_norm.groupby('nrsva_book_index'):
    palabras = [p for lista in grupo['tokens_no_stop'] for p in lista]
    palabras_por_libro[libro] = len(palabras)

print(" Palabras totales por libro calculadas correctamente.\n")
print(palabras_por_libro)

# =================================================================
# Tabla: palabras totales por libro
# =================================================================

import pandas as pd

df_palabras_por_libro = pd.DataFrame(
    list(palabras_por_libro.items()),
    columns=["Libro (NRSVA)", "Palabras Totales"]
).sort_values("Libro (NRSVA)").reset_index(drop=True)

display(df_palabras_por_libro)

# =================================================================
# Gr√°fico: Palabras totales por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.bar(df_palabras_por_libro["Libro (NRSVA)"],
        df_palabras_por_libro["Palabras Totales"],
        color='slateblue')

plt.title("Palabras Totales por Libro")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("Cantidad de Palabras")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""### Calcular palabras totales por cap√≠tulo"""

# =================================================================
# Palabras totales por cap√≠tulo
# =================================================================

palabras_por_capitulo = {}

for (libro, cap), grupo in df_norm.groupby(['nrsva_book_index', 'nrsva_chapter']):
    palabras = [p for lista in grupo['tokens_no_stop'] for p in lista]
    palabras_por_capitulo[(libro, cap)] = len(palabras)

print(" Palabras totales por cap√≠tulo calculadas correctamente.")
print("Ejemplo de los primeros elementos:")
list(palabras_por_capitulo.items())[:10]

# =================================================================
# DataFrame: Palabras totales por cap√≠tulo
# =================================================================

import pandas as pd

df_palabras_capitulo = pd.DataFrame(
    [(libro, cap, total) for (libro, cap), total in palabras_por_capitulo.items()],
    columns=["Libro (NRSVA)", "Cap√≠tulo", "Palabras Totales"]
).sort_values(["Libro (NRSVA)", "Cap√≠tulo"]).reset_index(drop=True)

display(df_palabras_capitulo)

# =================================================================
# Gr√°fico: Palabras totales por cap√≠tulo
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 7))
plt.plot(df_palabras_capitulo["Palabras Totales"], marker='o', linestyle='-', alpha=0.7)

plt.title("Palabras Totales por Cap√≠tulo (Ordenado por Libro y Cap√≠tulo)")
plt.xlabel("Cap√≠tulos (secuenciales)")
plt.ylabel("Total de Palabras")

plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

"""### Calcular promedio de palabras por vers√≠culo"""

# =================================================================
# Promedio de palabras por vers√≠culo
# =================================================================

# Crear la columna word_count (si no existe)
df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

# Calcular el promedio
promedio_palabras_versiculo = df_norm['word_count'].mean()

print(" Promedio de palabras por vers√≠culo:", round(promedio_palabras_versiculo, 2))

# =================================================================
# Histograma del n√∫mero de palabras por vers√≠culo
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.hist(df_norm['word_count'], bins=40, color='mediumseagreen', alpha=0.8)

plt.title("Distribuci√≥n del N√∫mero de Palabras por Vers√≠culo")
plt.xlabel("N√∫mero de Palabras")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Boxplot del n√∫mero de palabras por vers√≠culo
# =================================================================

plt.figure(figsize=(8, 4))
plt.boxplot(df_norm['word_count'], vert=False)

plt.title("Boxplot: Palabras por Vers√≠culo")
plt.xlabel("N√∫mero de Palabras")
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.show()

"""### Asegurar columna word_count"""

# =================================================================
# Asegurar columna de conteo de palabras
# =================================================================

df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

"""### Obtener el vers√≠culo m√°s largo por libro"""

# =================================================================
# Vers√≠culo m√°s largo por libro
# =================================================================

versiculo_mas_largo_por_libro = (
    df_norm.loc[df_norm.groupby('nrsva_book_index')['word_count'].idxmax()]
    .sort_values('nrsva_book_index')
    .reset_index(drop=True)
)

display(versiculo_mas_largo_por_libro)

# =================================================================
# Versi√≥n resumida del vers√≠culo m√°s largo por libro
# =================================================================

resumen = versiculo_mas_largo_por_libro[
    ['nrsva_book_index', 'nrsva_chapter', 'nrsva_verse', 'word_count']
]

display(resumen)

# =================================================================
# Gr√°fico: longitud del vers√≠culo m√°s largo por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.bar(resumen['nrsva_book_index'], resumen['word_count'], color='darkorange')

plt.title("Vers√≠culo M√°s Largo por Libro (NRSVA)")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("N√∫mero de Palabras")

plt.xticks(resumen['nrsva_book_index'], rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Asegurar columna word_count"""

# =================================================================
# Asegurar columna de conteo de palabras
# =================================================================

df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

"""### Obtener el vers√≠culo m√°s corto por libro"""

# =================================================================
# Vers√≠culo m√°s corto por libro
# =================================================================

versiculo_mas_corto_por_libro = (
    df_norm.loc[df_norm.groupby('nrsva_book_index')['word_count'].idxmin()]
    .sort_values('nrsva_book_index')
    .reset_index(drop=True)
)

display(versiculo_mas_corto_por_libro)

# =================================================================
# Resumen del vers√≠culo m√°s corto por libro
# =================================================================

resumen_corto = versiculo_mas_corto_por_libro[
    ['nrsva_book_index', 'nrsva_chapter', 'nrsva_verse', 'word_count']
]

display(resumen_corto)

# =================================================================
# Gr√°fico: Vers√≠culo m√°s corto por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.bar(resumen_corto['nrsva_book_index'], resumen_corto['word_count'], color='forestgreen')

plt.title("Vers√≠culo M√°s Corto por Libro (NRSVA)")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("N√∫mero de Palabras")

plt.xticks(resumen_corto['nrsva_book_index'], rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""### Calcular palabras totales por libro"""

# =================================================================
# Palabras totales por libro
# =================================================================

palabras_por_libro = {}

for libro, grupo in df_norm.groupby('nrsva_book_index'):
    palabras = [p for lista in grupo['tokens_no_stop'] for p in lista]
    palabras_por_libro[libro] = len(palabras)

print(" Palabras totales calculadas correctamente.")

# =================================================================
# Libro m√°s extenso y m√°s breve
# =================================================================

# Convertir a DataFrame
import pandas as pd

df_palabras_por_libro = pd.DataFrame(
    list(palabras_por_libro.items()),
    columns=["Libro (NRSVA)", "Palabras Totales"]
)

# Ordenar de mayor a menor
df_ordenado = df_palabras_por_libro.sort_values("Palabras Totales", ascending=False)

libro_mas_extenso = df_ordenado.iloc[0]
libro_mas_breve = df_ordenado.iloc[-1]

print(" Libro m√°s extenso:")
print(libro_mas_extenso)

print("\n Libro m√°s breve:")
print(libro_mas_breve)

# =================================================================
# Gr√°fico: Palabras totales por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.bar(df_ordenado["Libro (NRSVA)"], df_ordenado["Palabras Totales"], color='royalblue')

plt.title("Palabras Totales por Libro ‚Äî Identificaci√≥n de Libros Extensos y Breves")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("Palabras Totales")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""### Asegurar columna word_count"""

# =================================================================
# Asegurar columna de conteo de palabras por vers√≠culo
# =================================================================

df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

print("‚úî Columna word_count creada correctamente.")

# =================================================================
# Densidad promedio de palabras por vers√≠culo
# =================================================================

densidad_promedio = df_norm['word_count'].mean()

print(" Densidad promedio de palabras por vers√≠culo:", round(densidad_promedio, 2))

# =================================================================
# Estad√≠sticas descriptivas de la densidad
# =================================================================

print(df_norm['word_count'].describe())

# =================================================================
# Histograma de palabras por vers√≠culo
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.hist(df_norm['word_count'], bins=40, color='steelblue', alpha=0.8)

plt.title("Distribuci√≥n de Palabras por Vers√≠culo")
plt.xlabel("Palabras por Vers√≠culo")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Boxplot de palabras por vers√≠culo
# =================================================================

plt.figure(figsize=(10, 4))
plt.boxplot(df_norm['word_count'], vert=False, notch=True)

plt.title("Boxplot ‚Äî Palabras por Vers√≠culo")
plt.xlabel("N√∫mero de Palabras")
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.show()

"""### Asegurar columna de longitud de vers√≠culo"""

# =================================================================
# Asegurar columna de longitud de vers√≠culos
# =================================================================

df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)
print("‚úî Columna word_count asegurada.")

"""### C√°lculo de longitud promedio por libro"""

# =================================================================
# Longitud promedio de vers√≠culos por libro
# =================================================================

import pandas as pd

longitud_promedio_libro = (
    df_norm.groupby('nrsva_book_index')['word_count']
    .mean()
    .reset_index(name='Promedio_Palabras')
    .sort_values('nrsva_book_index')
)

display(longitud_promedio_libro)

# =================================================================
# Boxplot ‚Äî Distribuci√≥n de longitud de vers√≠culos por libro
# =================================================================

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(16, 7))
sns.boxplot(
    data=df_norm,
    x='nrsva_book_index',
    y='word_count',
    showfliers=False,     # para evitar ruido visual por outliers
    palette='viridis'
)

plt.title("Distribuci√≥n de Longitud de Vers√≠culos por Libro (sin outliers)")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("Palabras por Vers√≠culo")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

# =================================================================
# Gr√°fico de viol√≠n ‚Äî Longitud por libro
# =================================================================

plt.figure(figsize=(16, 7))
sns.violinplot(
    data=df_norm,
    x='nrsva_book_index',
    y='word_count',
    inner='quartile',
    palette='magma'
)

plt.title("Viol√≠n Plot: Distribuci√≥n de Longitud de Vers√≠culos por Libro")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("Palabras por Vers√≠culo")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

# =================================================================
# Estad√≠sticas descriptivas de longitud por libro
# =================================================================

estadisticas_libro = (
    df_norm.groupby('nrsva_book_index')['word_count']
    .describe()
    .round(2)
)

display(estadisticas_libro)

"""### Preparar tokens para N-grams"""

# =================================================================
# Preparar tokens para N-grams
# =================================================================

# Obtener todas las palabras √∫tiles
tokens = [palabra for lista in df_norm['tokens_no_stop'] for palabra in lista]

print(f"‚úî Total de palabras procesadas: {len(tokens)}")

"""### Funci√≥n gen√©rica para generar N-grams"""

# =================================================================
# Funci√≥n para generar N-grams
# =================================================================

from collections import Counter

def generar_ngrams(tokens, n):
    ngrams = zip(*[tokens[i:] for i in range(n)])
    return Counter([" ".join(ngram) for ngram in ngrams])

"""### Bigramas (2-grams)"""

# =================================================================
# Bigramas
# =================================================================

bigrams = generar_ngrams(tokens, 2)
top_bigrams = bigrams.most_common(20)

print(" Top 20 BIGRAMAS:")
for bg, freq in top_bigrams:
    print(f"{bg}: {freq}")

"""### BLOQUE 4 ‚Äî Trigramas (3-grams)"""

# =================================================================
# Trigramas
# =================================================================

trigrams = generar_ngrams(tokens, 3)
top_trigrams = trigrams.most_common(20)

print(" Top 20 TRIGRAMAS:")
for tg, freq in top_trigrams:
    print(f"{tg}: {freq}")

"""###BLOQUE 5 ‚Äî 4-grams"""

# =================================================================
# 4-grams
# =================================================================

fourgrams = generar_ngrams(tokens, 4)
top_fourgrams = fourgrams.most_common(20)

print(" Top 20 4-GRAMS:")
for fg, freq in top_fourgrams:
    print(f"{fg}: {freq}")

"""###Convertir resultados a DataFrames"""

# =================================================================
# Convertir N-grams a DataFrames
# =================================================================

import pandas as pd

df_bigrams = pd.DataFrame(top_bigrams, columns=["Bigram", "Frecuencia"])
df_trigrams = pd.DataFrame(top_trigrams, columns=["Trigram", "Frecuencia"])
df_fourgrams = pd.DataFrame(top_fourgrams, columns=["4-gram", "Frecuencia"])

display(df_bigrams)
display(df_trigrams)
display(df_fourgrams)

"""###Instalar spaCy y el modelo en espa√±ol"""

# =================================================================
# Instalaci√≥n de spaCy y modelo de espa√±ol
# =================================================================

!pip install -U spacy

# Descargar modelo peque√±o de espa√±ol
!python -m spacy download es_core_news_sm

"""### Cargar spaCy y el modelo de espa√±ol"""

# =================================================================
# Carga de spaCy y modelo de espa√±ol
# =================================================================

import spacy

# Cargar el modelo de espa√±ol
nlp = spacy.load("es_core_news_sm")

print(" Modelo de spaCy en espa√±ol cargado correctamente.")

"""### Funci√≥n de lematizaci√≥n para una lista de tokens"""

# =================================================================
# Funci√≥n para lematizar una lista de tokens
# =================================================================

def lematizar_lista(tokens):
    """
    Recibe una lista de tokens (palabras) y devuelve una lista de lemas.
    """
    # Unir tokens en un solo texto
    texto = " ".join(tokens)

    # Procesar con spaCy
    doc = nlp(texto)

    # Extraer lemas (evitando espacios y signos)
    lemas = [token.lemma_ for token in doc if token.is_alpha]

    return lemas

"""### Aplicar lematizaci√≥n al DataFrame"""

# =================================================================
# Aplicar lematizaci√≥n a cada vers√≠culo
# =================================================================

# Verificaci√≥n b√°sica
if 'tokens_no_stop' not in df_norm.columns:
    raise ValueError(" La columna 'tokens_no_stop' no existe. Aseg√∫rate de haber ejecutado la normalizaci√≥n antes.")

# Aplicar a todo el DataFrame (puede tardar un poco)
df_norm['lemmas'] = df_norm['tokens_no_stop'].apply(lematizar_lista)

# Reconstruir el texto lematizado
df_norm['text_lemmatized'] = df_norm['lemmas'].apply(lambda lst: " ".join(lst))

print(" Lematizaci√≥n completada.")
print("\nEjemplo de la primera fila:\n")
print("Tokens sin stopwords :", df_norm['tokens_no_stop'].iloc[0][:20])
print("Lemas                 :", df_norm['lemmas'].iloc[0][:20])
print("Texto lematizado      :", df_norm['text_lemmatized'].iloc[0][:200])

"""### Top N lemas m√°s frecuentes"""

# =================================================================
# Top N lemas m√°s frecuentes
# =================================================================

from collections import Counter

N = 30

todos_los_lemas = [lemma for lista in df_norm['lemmas'] for lemma in lista]
frecuencias_lemas = Counter(todos_los_lemas)
top_lemas = frecuencias_lemas.most_common(N)

print(f" Top {N} lemas m√°s frecuentes:")
for lem, freq in top_lemas:
    print(f"{lem}: {freq}")

"""#### Asegurar instalaci√≥n y carga del modelo spaCy en espa√±ol"""

# =================================================================
# Instalaci√≥n de spaCy y modelo de espa√±ol
# =================================================================

!pip install -U spacy
!python -m spacy download es_core_news_sm

"""### Cargar spaCy con POS-tagging"""

# =================================================================
# Cargar spaCy con modelo de espa√±ol
# =================================================================

import spacy
nlp = spacy.load("es_core_news_sm")

print(" Modelo spaCy cargado con POS-tagging.")

"""### Funci√≥n para etiquetar POS en una lista de tokens"""

# =================================================================
# Funci√≥n que aplica POS-tagging a una lista de tokens
# =================================================================

def pos_tags(tokens):
    texto = " ".join(tokens)
    doc = nlp(texto)
    return [(token.text, token.lemma_, token.pos_) for token in doc if token.is_alpha]

"""###Aplicar POS-tagging a cada vers√≠culo"""

# =================================================================
# Aplicar POS-tagging a cada vers√≠culo
# =================================================================

df_norm['pos_tags'] = df_norm['tokens_no_stop'].apply(pos_tags)

print("‚úî POS-tagging aplicado correctamente.")
print(df_norm['pos_tags'].iloc[0][:20])  # Mostrar ejemplo

"""### Conteo de verbos, sustantivos y adjetivos"""

# =================================================================
# Conteo global de verbos, sustantivos y adjetivos
# =================================================================

from collections import Counter

contador_pos = Counter()

for lista in df_norm['pos_tags']:
    for _, _, pos in lista:
        contador_pos[pos] += 1

verbos = contador_pos['VERB']
sustantivos = contador_pos['NOUN'] + contador_pos['PROPN']   # Sustantivo com√∫n + propio
adjetivos = contador_pos['ADJ']

print(" CONTEO GRAMATICAL GLOBAL")
print("-----------------------------")
print(f"Verbos (VERB):        {verbos}")
print(f"Sustantivos (NOUN+PROPN): {sustantivos}")
print(f"Adjetivos (ADJ):      {adjetivos}")

""" #### Tabla comparativa"""

# =================================================================
# Tabla con los conteos gramaticales
# =================================================================

import pandas as pd

df_pos_global = pd.DataFrame({
    "Categor√≠a gramatical": ["Verbos", "Sustantivos", "Adjetivos"],
    "Cantidad": [verbos, sustantivos, adjetivos]
})

display(df_pos_global)

"""###Gr√°fico comparativo"""

# =================================================================
# Gr√°fico de barras comparativo POS
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.bar(["Verbos", "Sustantivos", "Adjetivos"],
        [verbos, sustantivos, adjetivos],
        color=['blue', 'green', 'purple'])

plt.title("Distribuci√≥n de Categor√≠as Gramaticales")
plt.ylabel("Cantidad")
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""### Vocabulario Unico"""

# =================================================================
# Vocabulario √∫nico a partir de tokens_no_stop
# =================================================================

# Aplanar todas las palabras √∫tiles
todas_las_palabras = [pal for lista in df_norm['tokens_no_stop'] for pal in lista]

# Construir el vocabulario √∫nico
vocabulario_unico = sorted(set(todas_las_palabras))

print(" Tama√±o del vocabulario √∫nico:", len(vocabulario_unico))
print("\nEjemplo de las primeras 50 palabras del vocabulario:")
print(vocabulario_unico[:50])

# =================================================================
# Diccionarios word2id e id2word
# =================================================================

word2id = {palabra: idx for idx, palabra in enumerate(vocabulario_unico)}
id2word = {idx: palabra for palabra, idx in word2id.items()}

print("‚úî Diccionarios de vocabulario construidos.")
print("Ejemplo:")
for i in range(10):
    print(i, "->", id2word[i])

# =================================================================
# Vocabulario con frecuencia
# =================================================================

from collections import Counter
import pandas as pd

frecuencias_vocab = Counter(todas_las_palabras)

df_vocabulario = (
    pd.DataFrame(
        [(pal, freq, word2id[pal]) for pal, freq in frecuencias_vocab.items()],
        columns=["Palabra", "Frecuencia", "ID"]
    )
    .sort_values("Frecuencia", ascending=False)
    .reset_index(drop=True)
)

display(df_vocabulario.head(30))
print("\n‚úî Vocabulario con frecuencia construido. Filas totales:", len(df_vocabulario))

# =================================================================
# Guardar vocabulario a CSV
# =================================================================

df_vocabulario.to_csv("vocabulario_unico_con_frecuencia.csv", index=False, encoding="utf-8")
print(" Archivo 'vocabulario_unico_con_frecuencia.csv' guardado en el entorno de trabajo.")

# =================================================================
# Vocabulario √∫nico basado en lemas (opcional)
# =================================================================

todas_los_lemas = [lem for lista in df_norm['lemmas'] for lem in lista]
vocabulario_lemas = sorted(set(todas_los_lemas))

print(" Tama√±o del vocabulario √∫nico (lemas):", len(vocabulario_lemas))
print(vocabulario_lemas[:50])

"""###Construir texto limpio por libro"""

# =================================================================
# Construir corpus por libro
# =================================================================

import pandas as pd

# Convertir tokens sin stopwords a texto
df_norm['text_no_stop'] = df_norm['tokens_no_stop'].apply(lambda x: " ".join(x))

# Agrupar textos por libro
corpus_por_libro = (
    df_norm.groupby('nrsva_book_index')['text_no_stop']
    .apply(lambda textos: " ".join(textos))
)

print(" Total de libros procesados:", len(corpus_por_libro))
corpus_por_libro.head()

"""###Vectorizar usando TF-IDF"""

# =================================================================
# Vectorizaci√≥n TF-IDF por libro
# =================================================================

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    lowercase=True,
    analyzer='word',
    token_pattern=r'\b[a-z√°√©√≠√≥√∫√±]+\b',
    min_df=5,      # ignora palabras que aparecen menos de 5 veces
)

tfidf_matrix = vectorizer.fit_transform(corpus_por_libro)

print(" Matriz TF-IDF creada")
print("Dimensiones:", tfidf_matrix.shape)

"""###Funci√≥n para obtener Top N palabras caracter√≠sticas por libro"""

# =================================================================
# Funci√≥n: Top N palabras caracter√≠sticas (keyness)
# =================================================================

import numpy as np

feature_names = vectorizer.get_feature_names_out()

def palabras_caracteristicas(libro_id, N=20):
    """
    Obtiene las N palabras con mayor puntuaci√≥n TF-IDF para un libro.
    """
    idx = list(corpus_por_libro.index).index(libro_id)
    fila = tfidf_matrix[idx].toarray().flatten()
    top_indices = fila.argsort()[-N:][::-1]

    return [(feature_names[i], fila[i]) for i in top_indices]

# =================================================================
# Ejemplo: palabras caracter√≠sticas del primer libro del corpus
# =================================================================

primer_libro = corpus_por_libro.index[0]

print(f" Palabras m√°s caracter√≠sticas para el libro {primer_libro}:\n")

for palabra, score in palabras_caracteristicas(primer_libro, N=20):
    print(f"{palabra}: {score:.4f}")

# =================================================================
# Tabla de palabras caracter√≠sticas para un libro
# =================================================================

libro_id = primer_libro  # c√°mbialo por cualquier √≠ndice NRSVA

data = palabras_caracteristicas(libro_id, N=30)
df_keyness = pd.DataFrame(data, columns=["Palabra", "TF-IDF"])

display(df_keyness)

# =================================================================
# Gr√°fico TF-IDF para palabras m√°s distintivas
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.bar(df_keyness["Palabra"], df_keyness["TF-IDF"], color='darkred')

plt.title(f"Palabras m√°s caracter√≠sticas del libro {libro_id}")
plt.xlabel("Palabra")
plt.ylabel("TF-IDF")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""###Preprocesamiento: asegurar columnas"""

# =================================================================
# Asegurar columnas necesarias
# =================================================================

df_norm['text_no_stop'] = df_norm['tokens_no_stop'].apply(lambda x: " ".join(x))

print(" Columnas aseguradas: tokens_no_stop, text_no_stop")

# =================================================================
# Filtrar vers√≠culos donde aparece un personaje
# =================================================================

def versiculos_personaje(df, nombre):
    """
    Devuelve todos los vers√≠culos donde aparece el 'nombre' como palabra exacta.
    """
    nombre = nombre.lower()
    mask = df['tokens_no_stop'].apply(lambda lista: nombre in [p.lower() for p in lista])
    return df[mask]

# =================================================================
# Construir corpus por personaje
# =================================================================

def corpus_de_personaje(df, nombre):
    df_p = versiculos_personaje(df, nombre)
    texto = " ".join(df_p['text_no_stop'])
    return texto

# =================================================================
# Lista de personajes (puedes agregar los que quieras)
# =================================================================

personajes = ["dios", "adan", "noe", "abraham", "jacob"]

# =================================================================
# Corpus por personaje
# =================================================================

corpus_personajes = {p: corpus_de_personaje(df_norm, p) for p in personajes}

print(" Corpus por personaje creado.")

# =================================================================
# Matriz TF-IDF para personajes
# =================================================================

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer_pers = TfidfVectorizer(
    lowercase=True,
    analyzer="word",
    token_pattern=r'\b[a-z√°√©√≠√≥√∫√±]+\b',
    min_df=2
)

tfidf_pers = vectorizer_pers.fit_transform(corpus_personajes.values())
feature_names_pers = vectorizer_pers.get_feature_names_out()

print(" Matriz TF-IDF de personajes creada.")
print(tfidf_pers.shape)

# =================================================================
# Palabras clave por personaje
# =================================================================

import numpy as np
import pandas as pd

def palabras_caracteristicas_personaje(nombre, N=20):
    """
    Devuelve las N palabras m√°s caracter√≠sticas del personaje 'nombre'
    seg√∫n TF-IDF.
    """
    idx = list(corpus_personajes.keys()).index(nombre)
    fila = tfidf_pers[idx].toarray().flatten()
    indices = fila.argsort()[-N:][::-1]
    return [(feature_names_pers[i], fila[i]) for i in indices]

# =================================================================
# Ejemplo: palabras caracter√≠sticas de 'noe'
# =================================================================

carac_noe = palabras_caracteristicas_personaje("noe", N=20)

print(" Palabras m√°s caracter√≠sticas de NO√â:\n")
for palabra, score in carac_noe:
    print(f"{palabra}: {score:.4f}")

"""###Calcular el TTR global del corpus"""

# =================================================================
# TTR (Type/Token Ratio) global del corpus
# =================================================================

# Aplanar todas las palabras √∫tiles
tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

# Tipos (palabras √∫nicas)
tipos = set(tokens)

ttr = len(tipos) / len(tokens)

print(" N√∫mero total de tokens:", len(tokens))
print(" N√∫mero total de tipos (vocabulario √∫nico):", len(tipos))
print(f" TTR (Type/Token Ratio) global: {ttr:.4f}")

# =================================================================
# TTR por libro
# =================================================================

import pandas as pd

ttr_por_libro = []

for libro, grupo in df_norm.groupby('nrsva_book_index'):
    tokens_libro = [p for lista in grupo['tokens_no_stop'] for p in lista]
    tipos_libro = set(tokens_libro)
    ttr_val = len(tipos_libro) / len(tokens_libro)
    ttr_por_libro.append((libro, ttr_val))

df_ttr_libro = pd.DataFrame(ttr_por_libro, columns=["Libro (NRSVA)", "TTR"])
display(df_ttr_libro)

# =================================================================
# Gr√°fico: TTR por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.plot(df_ttr_libro["Libro (NRSVA)"], df_ttr_libro["TTR"], marker='o', linestyle='-', color='darkblue')

plt.title("√çndice de Riqueza L√©xica (TTR) por Libro")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("TTR")

plt.xticks(rotation=45)
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# TTR usando lemas
# =================================================================

if 'lemmas' in df_norm.columns:
    lemas = [l for lista in df_norm['lemmas'] for l in lista]
    tipos_lemmas = set(lemas)
    ttr_lemmas = len(tipos_lemmas) / len(lemas)

    print(" TTR basado en lemas:", round(ttr_lemmas, 4))
else:
    print(" No existe la columna 'lemmas'. Ejecuta la lematizaci√≥n primero.")

"""### Aplanar tokens"""

# =================================================================
# Aplanar todas las palabras del corpus
# =================================================================

tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

print("‚úî Tokens cargados:", len(tokens))

"""###Calcular frecuencias globales"""

# =================================================================
# Frecuencias globales de cada palabra
# =================================================================

from collections import Counter

frecuencias = Counter(tokens)

print(" Total de tipos √∫nicos:", len(frecuencias))

# =================================================================
# Hapax Legomena
# =================================================================

hapax_legomena = [pal for pal, freq in frecuencias.items() if freq == 1]

print(" Total de Hapax Legomena:", len(hapax_legomena))
print("Ejemplos:", hapax_legomena[:20])

# =================================================================
# Hapax Dislegomena
# =================================================================

hapax_dislegomena = [pal for pal, freq in frecuencias.items() if freq == 2]

print(" Total de Hapax Dislegomena:", len(hapax_dislegomena))
print("Ejemplos:", hapax_dislegomena[:20])

# =================================================================
# Tabla resumen Hapax Legomena / Dislegomena
# =================================================================

import pandas as pd

df_hapax = pd.DataFrame({
    "M√©trica": ["Total Tokens", "Tipos √∫nicos", "Hapax Legomena", "Hapax Dislegomena"],
    "Valor": [len(tokens), len(frecuencias), len(hapax_legomena), len(hapax_dislegomena)]
})

display(df_hapax)

# =================================================================
# Porcentaje de hapax
# =================================================================

pct_hapax = len(hapax_legomena) / len(frecuencias) * 100
pct_dis = len(hapax_dislegomena) / len(frecuencias) * 100

print(f" Porcentaje Hapax Legomena: {pct_hapax:.2f}%")
print(f" Porcentaje Hapax Dislegomena: {pct_dis:.2f}%")

"""### Preparar tokens por libro"""

# =================================================================
# Preparar tokens por libro
# =================================================================

tokens_por_libro = {}

for libro, grupo in df_norm.groupby("nrsva_book_index"):
    tokens = [p for lista in grupo["tokens_no_stop"] for p in lista]
    tokens_por_libro[libro] = tokens

print(f" Libros procesados: {len(tokens_por_libro)}")

# =================================================================
# Diversidad l√©xica (TTR) por libro
# =================================================================

import pandas as pd

diversidad = []  # lista de filas

for libro, tokens in tokens_por_libro.items():
    tipos = set(tokens)
    ttr = len(tipos) / len(tokens)
    diversidad.append([libro, len(tokens), len(tipos), ttr])

df_lexico_libro = pd.DataFrame(diversidad, columns=[
    "Libro (NRSVA)", "Tokens Totales", "Tipos √önicos", "TTR"
]).sort_values("Libro (NRSVA)").reset_index(drop=True)

display(df_lexico_libro)

# =================================================================
# Gr√°fico: Diversidad l√©xica (TTR) por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.plot(df_lexico_libro["Libro (NRSVA)"], df_lexico_libro["TTR"],
         marker="o", linestyle="-", color="darkgreen")

plt.title("√çndice de Diversidad L√©xica (TTR) por Libro")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("TTR (Type/Token Ratio)")

plt.grid(True, linestyle="--", alpha=0.5)
plt.xticks(rotation=45)
plt.show()

# =================================================================
# Gr√°fico: Tipos y tokens por libro
# =================================================================

plt.figure(figsize=(14, 6))
plt.plot(df_lexico_libro["Libro (NRSVA)"], df_lexico_libro["Tokens Totales"],
         label="Tokens", color="steelblue")
plt.plot(df_lexico_libro["Libro (NRSVA)"], df_lexico_libro["Tipos √önicos"],
         label="Tipos", color="orange")

plt.title("Tokens vs Tipos √önicos por Libro")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("Cantidad")

plt.legend()
plt.grid(True, linestyle="--", alpha=0.5)
plt.xticks(rotation=45)
plt.show()

"""###Aplanar tokens y contar frecuencias"""

# =================================================================
# Aplanar tokens y calcular frecuencias
# =================================================================

from collections import Counter

tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]
frecuencias = Counter(tokens)

print(" Tokens totales:", len(tokens))
print(" Tipos √∫nicos:", len(frecuencias))

"""### Detectar palabras raras (freq ‚â§ 3)"""

# =================================================================
# Palabras raras: frecuencia ‚â§ 3
# =================================================================

palabras_raras = [pal for pal, freq in frecuencias.items() if freq <= 3]

print("üîπ Palabras raras (freq ‚â§ 3):", len(palabras_raras))
print("Ejemplos:", palabras_raras[:20])

"""###Detectar palabras comunes (percentil 95)"""

# =================================================================
# Palabras comunes: percentil 95
# =================================================================

import numpy as np

valores_frec = np.array(list(frecuencias.values()))
umbral_95 = np.percentile(valores_frec, 95)

palabras_comunes = [pal for pal, freq in frecuencias.items() if freq >= umbral_95]

print(" Palabras comunes (percentil 95):", len(palabras_comunes))
print("Umbral de frecuencia (p95):", umbral_95)
print("Ejemplos:", palabras_comunes[:20])

"""###Resumen num√©rico"""

# =================================================================
# Resumen de palabras raras y comunes
# =================================================================

import pandas as pd

df_resumen = pd.DataFrame({
    "Categor√≠a": ["Tokens totales", "Tipos √∫nicos",
                  "Palabras raras (‚â§3)", "Palabras comunes (p95)"],
    "Valor": [len(tokens), len(frecuencias),
              len(palabras_raras), len(palabras_comunes)]
})

display(df_resumen)

"""###Gr√°fico: palabras raras vs comunes"""

# =================================================================
# Gr√°fico comparativo
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.bar(["Raras (‚â§3)", "Comunes (p95)"],
        [len(palabras_raras), len(palabras_comunes)],
        color=["purple", "darkorange"])

plt.title("Comparaci√≥n de Palabras Raras vs Comunes")
plt.ylabel("Cantidad")
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""### Preparar tokens del corpus"""

# =================================================================
# Preparar tokens del corpus
# =================================================================

tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

print(" Tokens cargados:", len(tokens))

"""###Construir distribuci√≥n de probabilidad"""

# =================================================================
# Frecuencias y probabilidades
# =================================================================

from collections import Counter
import numpy as np

frecuencias = Counter(tokens)
total_tokens = len(tokens)

# Probabilidades
probs = np.array([freq / total_tokens for freq in frecuencias.values()])

# =================================================================
# Entrop√≠a del texto
# =================================================================

entropia = -np.sum(probs * np.log2(probs))

print(f" Entrop√≠a total del texto (bits): {entropia:.4f}")

# =================================================================
# Entrop√≠a por libro
# =================================================================

import pandas as pd

entropia_libro = []

for libro, grupo in df_norm.groupby("nrsva_book_index"):
    tokens_l = [p for lista in grupo["tokens_no_stop"] for p in lista]
    frecs_l = Counter(tokens_l)
    total_l = len(tokens_l)
    probs_l = np.array([f / total_l for f in frecs_l.values()])

    H_l = -np.sum(probs_l * np.log2(probs_l))
    entropia_libro.append([libro, H_l])

df_entropia = pd.DataFrame(entropia_libro, columns=["Libro (NRSVA)", "Entrop√≠a"])
display(df_entropia)

# =================================================================
# Gr√°fico: Entrop√≠a por libro
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.plot(df_entropia["Libro (NRSVA)"], df_entropia["Entrop√≠a"],
         marker="o", linestyle="-", color="darkred")

plt.title("Entrop√≠a L√©xica por Libro")
plt.xlabel("Libro (√çndice NRSVA)")
plt.ylabel("Entrop√≠a (bits)")

plt.grid(True, linestyle="--", alpha=0.5)
plt.xticks(rotation=45)
plt.show()

"""### Asegurar columna word_count (si no existe)"""

# =================================================================
# Crear word_count si a√∫n no existe
# =================================================================

if 'word_count' not in df_norm.columns:
    df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

print(" Columna word_count creada / verificada.")

"""### Verificar / crear word_count"""

# =================================================================
# Crear word_count si a√∫n no existe
# =================================================================

if 'word_count' not in df_norm.columns:
    df_norm['word_count'] = df_norm['tokens_no_stop'].apply(len)

print("‚úî word_count creado/verificado.")
df_norm['word_count'].head()

# =================================================================
# Histograma de longitudes de vers√≠culos (en palabras)
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.hist(df_norm['word_count'], bins=40, color='steelblue', alpha=0.8)

plt.title("Distribuci√≥n de Longitudes de Vers√≠culos")
plt.xlabel("N√∫mero de palabras por vers√≠culo")
plt.ylabel("Frecuencia")

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Contar palabras m√°s frecuentes
# =================================================================

from collections import Counter

# Aplanar todos los tokens
tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

# Contar frecuencias
frecuencias = Counter(tokens)

# Seleccionar las N m√°s frecuentes
N = 30
top_palabras = frecuencias.most_common(N)

top_palabras[:10]

# =================================================================
# Convertir top N palabras a DataFrame
# =================================================================

import pandas as pd

df_top = pd.DataFrame(top_palabras, columns=["Palabra", "Frecuencia"])
df_top

# =================================================================
# Gr√°fico de barras de las palabras m√°s frecuentes
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.bar(df_top["Palabra"], df_top["Frecuencia"], color="purple", alpha=0.8)

plt.title(f"Top {N} Palabras M√°s Frecuentes")
plt.xlabel("Palabra")
plt.ylabel("Frecuencia")

plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.show()

# =================================================================
# Seleccionar top N palabras m√°s frecuentes del corpus
# =================================================================

from collections import Counter

# Aplanar tokens
tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

# Calcular frecuencias
frecuencias = Counter(tokens)

# Elegir top N palabras
N = 30
top_words = [pal for pal, freq in frecuencias.most_common(N)]

print("‚úî Palabras seleccionadas:", top_words[:10], "...")

"""### Contar frecuencias por cap√≠tulo"""

# =================================================================
# Construir matriz libro-cap√≠tulo vs palabra
# =================================================================

import pandas as pd

# Crear DataFrame vac√≠o
df_heat = pd.DataFrame(columns=["Libro", "Cap√≠tulo"] + top_words)

# Agrupar por libro y cap√≠tulo
grupos = df_norm.groupby(["nrsva_book_index", "nrsva_chapter"])

rows = []

for (libro, cap), grupo in grupos:
    tokens_cap = [p for lista in grupo['tokens_no_stop'] for p in lista]
    frecs_cap = Counter(tokens_cap)

    fila = {"Libro": libro, "Cap√≠tulo": cap}
    fila.update({w: frecs_cap.get(w, 0) for w in top_words})

    rows.append(fila)

df_heat = pd.DataFrame(rows)

print("‚úî Matriz creada con forma:", df_heat.shape)
df_heat.head()

# =================================================================
# Crear √≠ndice cap√≠tulo absoluto (libro.cap√≠tulo)
# =================================================================

df_heat["LibroCap"] = df_heat["Libro"].astype(str) + "." + df_heat["Cap√≠tulo"].astype(str)
df_heat = df_heat.sort_values(["Libro", "Cap√≠tulo"]).reset_index(drop=True)

df_heat.index = df_heat["LibroCap"]
df_heat_num = df_heat[top_words]

print("‚úî √çndice LibroCap generado.")
df_heat_num.head()

# =================================================================
# Instalar librer√≠a WordCloud
# =================================================================

!pip install wordcloud

# =================================================================
# Construir texto para WordCloud
# =================================================================

# Aplanar todos los tokens limpios
tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

# Unir todo como texto
texto_wordcloud = " ".join(tokens)

print("‚úî Texto para WordCloud construido.")

# =================================================================
# Generar WordCloud
# =================================================================

from wordcloud import WordCloud
import matplotlib.pyplot as plt

wc = WordCloud(
    width=1600,
    height=900,
    background_color="white",
    max_words=300,
    colormap="viridis"
).generate(texto_wordcloud)

plt.figure(figsize=(16, 9))
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.title("WordCloud - Palabras M√°s Frecuentes (Corpus Completo)", fontsize=18)
plt.show()

# =================================================================
# WordCloud por libro (versi√≥n corregida y robusta)
# =================================================================

def wordcloud_por_libro(libro_id):
    # Filtrar el libro
    grupo = df_norm[df_norm["nrsva_book_index"] == libro_id]

    # Validar si existe ese libro
    if grupo.empty:
        print(f" No existe ning√∫n libro con √≠ndice {libro_id}.")
        print(" Usa df_norm['nrsva_book_index'].unique() para ver los √≠ndices disponibles.")
        return

    # Extraer tokens
    tokens_libro = [p for lista in grupo['tokens_no_stop'] for p in lista]

    # Validar si el libro tiene palabras
    if len(tokens_libro) == 0:
        print(f"‚ö† El libro con √≠ndice {libro_id} no tiene palabras procesables.")
        return

    # Crear texto
    texto = " ".join(tokens_libro)

    # Generar WordCloud
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    wc = WordCloud(
        width=1600,
        height=900,
        background_color="white",
        colormap="plasma",
        max_words=200
    ).generate(texto)

    plt.figure(figsize=(16, 9))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"WordCloud - Libro {libro_id}", fontsize=18)
    plt.show()

# =================================================================
# Ver los √≠ndices reales de libros
# =================================================================

df_norm['nrsva_book_index'].unique()

# =================================================================
# Construir y contar bigramas a partir de tokens_no_stop
# =================================================================

from collections import Counter

def generar_bigramas(lista_tokens):
    """
    Recibe una lista de tokens y devuelve una lista de bigramas como tuplas (w1, w2)
    """
    return list(zip(lista_tokens[:-1], lista_tokens[1:]))

# Aplanar todos los tokens del corpus
tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

# Generar todos los bigramas
bigrams = generar_bigramas(tokens)

# Contar frecuencias de bigramas
bigram_freq = Counter(bigrams)

print(" Total de bigramas distintos:", len(bigram_freq))
list(bigram_freq.items())[:10]

# =================================================================
# Seleccionar TOP K bigramas m√°s frecuentes
# =================================================================

K = 50  # puedes ajustar este valor

top_bigrams = bigram_freq.most_common(K)

print(f" Seleccionados los {K} bigramas m√°s frecuentes:")
for par, freq in top_bigrams[:10]:
    print(f"{par} -> {freq}")

# =================================================================
# Construir grafo de co-ocurrencia de bigramas
# =================================================================

!pip install networkx

import networkx as nx

G = nx.Graph()

# A√±adir nodos y aristas con peso
for (w1, w2), freq in top_bigrams:
    G.add_node(w1)
    G.add_node(w2)
    G.add_edge(w1, w2, weight=freq)

print(" Nodos en el grafo:", G.number_of_nodes())
print(" Aristas en el grafo:", G.number_of_edges())

# =================================================================
# Graficar el grafo de bigramas
# =================================================================

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 10))

# Posiciones de los nodos
pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)

# Pesos de las aristas (para grosor)
weights = [G[u][v]['weight'] for u, v in G.edges()]

# Dibujar nodos
nx.draw_networkx_nodes(G, pos, node_size=400, node_color='lightblue')

# Dibujar aristas (m√°s grosor = m√°s frecuencia)
nx.draw_networkx_edges(
    G, pos,
    width=[w / max(weights) * 5 for w in weights],  # normalizar grosor
    alpha=0.6
)

# Etiquetas de los nodos (palabras)
nx.draw_networkx_labels(G, pos, font_size=10)

plt.title(f"Grafo de Co-ocurrencia de Palabras (Top {K} Bigramas)")
plt.axis("off")
plt.show()

# =================================================================
# Grafo de bigramas filtrado por una palabra clave
# =================================================================

palabra_clave = "dios"

# Filtrar solo bigramas donde aparece la palabra clave
top_bigrams_filtrados = [
    ((w1, w2), freq)
    for (w1, w2), freq in top_bigrams
    if w1 == palabra_clave or w2 == palabra_clave
]

G_key = nx.Graph()

for (w1, w2), freq in top_bigrams_filtrados:
    G_key.add_node(w1)
    G_key.add_node(w2)
    G_key.add_edge(w1, w2, weight=freq)

plt.figure(figsize=(10, 7))

pos = nx.spring_layout(G_key, k=0.7, iterations=50, seed=42)
weights = [G_key[u][v]['weight'] for u, v in G_key.edges()]

nx.draw_networkx_nodes(G_key, pos, node_size=600, node_color='lightgreen')
nx.draw_networkx_edges(
    G_key, pos,
    width=[w / max(weights) * 6 for w in weights],
    alpha=0.7
)
nx.draw_networkx_labels(G_key, pos, font_size=11)

plt.title(f"Grafo de Bigramas alrededor de '{palabra_clave}'")
plt.axis("off")
plt.show()

# =================================================================
# Frecuencia de una palabra a lo largo del texto (por cap√≠tulo)
# =================================================================

import pandas as pd

def tendencia_palabra(palabra):
    palabra = palabra.lower()

    frecuencias = []
    for (libro, cap), grupo in df_norm.groupby(["nrsva_book_index", "nrsva_chapter"]):
        tokens = [p.lower() for lista in grupo["tokens_no_stop"] for p in lista]
        freq = tokens.count(palabra)
        frecuencias.append([f"{libro}.{cap}", freq])

    df_tend = pd.DataFrame(frecuencias, columns=["LibroCap", "Frecuencia"])
    return df_tend

# =================================================================
# Graficar tendencia de una palabra
# =================================================================

import matplotlib.pyplot as plt

palabra = "dios"
df_tend = tendencia_palabra(palabra)

plt.figure(figsize=(16,4))
plt.plot(df_tend["LibroCap"], df_tend["Frecuencia"], color="purple")

plt.title(f"Tendencia temporal de la palabra: {palabra}")
plt.xticks([], [])  # demasiados cap√≠tulos para mostrar etiquetas
plt.ylabel("Frecuencia por cap√≠tulo")
plt.grid(True, linestyle="--", alpha=0.4)
plt.show()

# =================================================================
# Correlaci√≥n entre palabras frecuentes
# =================================================================

from collections import Counter
import numpy as np

# Top 50 palabras para an√°lisis
top_words = [w for w, _ in Counter(tokens).most_common(50)]

# Matriz cap√≠tulos vs palabras
data = []

for (libro, cap), grupo in df_norm.groupby(["nrsva_book_index", "nrsva_chapter"]):
    tokens_c = [p for lista in grupo['tokens_no_stop'] for p in lista]
    frecs = Counter(tokens_c)
    data.append([frecs.get(w, 0) for w in top_words])

import pandas as pd
df_mat = pd.DataFrame(data, columns=top_words)

corr = df_mat.corr()

# =================================================================
# Heatmap de correlaci√≥n entre palabras
# =================================================================

import seaborn as sns
plt.figure(figsize=(14,12))
sns.heatmap(corr, cmap="coolwarm", center=0)
plt.title("Correlaci√≥n entre palabras frecuentes")
plt.show()

# =================================================================
# Longitud promedio del vers√≠culo por libro
# =================================================================

df_style = df_norm.groupby("nrsva_book_index")["word_count"].mean().reset_index()
df_style.columns = ["Libro", "LongitudMedia"]

plt.figure(figsize=(12,5))
plt.plot(df_style["Libro"], df_style["LongitudMedia"], marker="o")
plt.title("Complejidad narrativa por libro")
plt.ylabel("Palabras por vers√≠culo")
plt.grid(True)
plt.show()

# =================================================================
# Frecuencias de palabras cuando aparece un personaje
# =================================================================

def contexto_personaje(nombre, top=20):
    nombre = nombre.lower()
    mask = df_norm['tokens_no_stop'].apply(lambda toks: nombre in [p.lower() for p in toks])
    tokens_p = [p for lista in df_norm[mask]['tokens_no_stop'] for p in lista]
    frecs = Counter(tokens_p).most_common(top)
    return frecs

# =================================================================
# Frecuencias por libro para un set de palabras
# =================================================================

from collections import Counter
import pandas as pd

# Palabras a comparar (puedes cambiarlas)
palabras = ["dios", "hombre", "tierra", "d√≠a", "rey"]

# Construir tabla Libro √ó Palabras
rows = []

for libro, grupo in df_norm.groupby("nrsva_book_index"):
    tokens = [p.lower() for lista in grupo["tokens_no_stop"] for p in lista]
    frec = Counter(tokens)
    rows.append([libro] + [frec.get(w, 0) for w in palabras])

df_radar = pd.DataFrame(rows, columns=["Libro"] + palabras)

# Escoger algunos libros para comparar (ejemplo: primeros 5)
df_radar_sel = df_radar.head(5)
df_radar_sel

# =================================================================
# Funci√≥n: Gr√°fico RADAR
# =================================================================

import numpy as np
import matplotlib.pyplot as plt

def plot_radar(df, fila, etiquetas, titulo="Radar Chart"):
    valores = df.loc[fila, etiquetas].values
    categorias = etiquetas
    N = len(categorias)

    # Cerrar el c√≠rculo
    valores = np.concatenate((valores, [valores[0]]))
    angulos = np.linspace(0, 2*np.pi, N, endpoint=False)
    angulos = np.concatenate((angulos, [angulos[0]]))

    plt.figure(figsize=(8, 8))
    plt.polar(angulos, valores, marker='o')

    plt.fill(angulos, valores, alpha=0.25)
    plt.xticks(angulos[:-1], categorias)
    plt.title(titulo, size=14)
    plt.show()

# =================================================================
# Graficar radar para un libro espec√≠fico
# =================================================================

libro_id = df_radar_sel.iloc[0]["Libro"]  # primer libro de la tabla
fila = df_radar_sel[df_radar_sel["Libro"] == libro_id]

plot_radar(df=fila,
           fila=fila.index[0],
           etiquetas=palabras,
           titulo=f"Radar: Libro {libro_id}")

# =================================================================
# M√©tricas ling√º√≠sticas por libro
# =================================================================

import numpy as np
from collections import Counter

datos = []

for libro, grupo in df_norm.groupby("nrsva_book_index"):

    # Tokens del libro
    tokens = [p for lista in grupo["tokens_no_stop"] for p in lista]

    # TTR
    tipos = set(tokens)
    ttr = len(tipos) / len(tokens)

    # Entrop√≠a
    frec = Counter(tokens)
    probs = np.array([v / len(tokens) for v in frec.values()])
    H = -np.sum(probs * np.log2(probs))

    # Longitud promedio de vers√≠culo
    prom_long = grupo['word_count'].mean()

    datos.append([libro, ttr, H, prom_long])

df_metricas = pd.DataFrame(datos, columns=["Libro", "TTR", "Entrop√≠a", "Longitud"])
df_metricas.head()

# =================================================================
# Ley de Zipf: construir tabla rango vs frecuencia
# =================================================================

from collections import Counter
import pandas as pd

# Aplanar todos los tokens limpios
tokens = [p for lista in df_norm['tokens_no_stop'] for p in lista]

# Contar frecuencias
frecuencias = Counter(tokens)

# Ordenar por frecuencia (descendente)
palabras_ordenadas = frecuencias.most_common()

# Construir DataFrame: rango, palabra, frecuencia
zipf_data = pd.DataFrame(
    [(rango + 1, palabra, freq) for rango, (palabra, freq) in enumerate(palabras_ordenadas)],
    columns=["Rango", "Palabra", "Frecuencia"]
)

display(zipf_data.head(20))
print("‚úî Tabla Zipf construida. Total de palabras distintas:", len(zipf_data))

# =================================================================
# Gr√°fico Ley de Zipf (log-log)
# =================================================================

import matplotlib.pyplot as plt
import numpy as np

ranks = zipf_data["Rango"].values
freqs = zipf_data["Frecuencia"].values

plt.figure(figsize=(8, 6))
plt.loglog(ranks, freqs, marker='.', linestyle='none')

plt.title("Ley de Zipf - Texto b√≠blico (tokens_no_stop)")
plt.xlabel("Rango de la palabra (log)")
plt.ylabel("Frecuencia de la palabra (log)")
plt.grid(True, which="both", linestyle='--', alpha=0.5)
plt.show()

# =================================================================
# Ajuste lineal en escala log-log (opcional)
# =================================================================

# Usamos solo las primeras N palabras (las m√°s frecuentes)
N = 500  # puedes ajustar este valor
ranks_N = ranks[:N]
freqs_N = freqs[:N]

# Pasar a log10
log_r = np.log10(ranks_N)
log_f = np.log10(freqs_N)

# Ajuste lineal: log_f ‚âà a + b * log_r
coef = np.polyfit(log_r, log_f, 1)
a, b = coef

print(f"Recta ajustada en log10: log10(freq) ‚âà {a:.3f} + {b:.3f} * log10(rango)")

# Graficar puntos + recta
plt.figure(figsize=(8, 6))
plt.scatter(log_r, log_f, s=10, alpha=0.5, label="Datos (log10)")
plt.plot(log_r, a + b * log_r, linewidth=2, label="Ajuste lineal")

plt.title(f"Ley de Zipf (log10) ‚Äî pendiente ‚âà {b:.3f}")
plt.xlabel("log10(Rango)")
plt.ylabel("log10(Frecuencia)")
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend()
plt.show()